{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoML_PCs.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PaCxQYCA9GMU",
        "hK5qjZlW9lVs",
        "FwlsvGoDrgqy",
        "suyGXvdRGxrv",
        "4yQWX7QTB7Ce",
        "FgNFL-HsCE5F",
        "Uel3kCnHG88v",
        "meKxSv8JIf6o",
        "9N2JZ3-XIpkG",
        "9_rDfUP5I7Ki",
        "zUl7jFBCybn9",
        "HQfhsuf3tCCK",
        "cXhEUYWdi1CR",
        "4746jxTL6E4A",
        "l8b30PZF6PsY",
        "Dryt48K_616G",
        "moLrGBBi7AaY",
        "NDLUCiNi7T-G",
        "KWd32Qtz7irF",
        "oveqB1fDhyX3"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viniciuswv/AutoML---Classification-of-Electrical-Charges-in-Smart-Grid/blob/main/AutoML_PCs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xzq_sjVd7llk"
      },
      "source": [
        "![Logo PPGEN.png](data:image/png;base64,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)\n",
        "\n",
        "_____\n",
        "**Programa de Pós-graduação em Energia - PPGEN   |   Universidade Federal do Espírito Santo - UFES**\n",
        "_____\n",
        "**As part of the dissertation:** \"*Automated adjustment of hyperparameters in convolutional neural network to identify similar electrical loads in smart grid.*\"\n",
        "_____\n",
        "**Developers:**\n",
        "Vinicius Wittig Vianna (PPGEN) - viniciuswv@gmail.com;\n",
        "Wanderley Cardoso Celeste (DCEL/PPGEN) - wanderley.celeste@ufes.br;\n",
        "Helder Roberto de Oliveira (DEE/PPGEE) - helder.rocha@ufes.br;\n",
        "Leonardo Jose Silvestre (DCEL) - leonardo.silvestre@ufes.br\n",
        "_____\n",
        "This code was developed as a dissertation project by (VIANNA, 2021), for the implementation and investigation of an two stages based optimization for automatically configuring a hyperparameters set of interest in the convolutional neural network (CNN) originally developed in (FIRMES, 2020), whose objective is to classify 16 possible combinations of two types of highly similar electrical charges (lamps and PCs) on a test bench.\n",
        "\n",
        "\n",
        "**Abstract:**\n",
        "\n",
        "*  The first step is to apply the Hill Climb(down) optimization, in order to investigate how far it is possible to reduce the data (organized in \"cases\") while maintaining the network's **test accuracy** greater than or equal to 95%.\n",
        "\n",
        "*  The second step is to apply the Bayesian optimization, in order to investigate whether it's possible to make the network **valid accuracy** close to 100%, by automatically selecting a good set of hyperparameters.\n",
        "\n",
        "*  Was necessary to adjust some network learning parameters to improve the robustness and stability during training and validation, otherwise, the Hillclimb optimizer could be stuck in a local minimum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaCxQYCA9GMU"
      },
      "source": [
        "# 1.Libraries and frameworks import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiYqNlQtQ9ds"
      },
      "source": [
        "Import of the necessary tools and verification of the graphics card provided by Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uvnL9fA8ukk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d37937d4-a7de-467c-fbaa-b4b3105a60b8"
      },
      "source": [
        "# Canceling miscellaneous cautions:\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        " \n",
        "# General modeling in the dataset:\n",
        "!pip install pandas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import random as python_random\n",
        "import scipy.io\n",
        "import time\n",
        "import datetime\n",
        "from numpy import array\n",
        "from decimal import Decimal\n",
        "from numpy import savetxt\n",
        " \n",
        "# Data pre-processing methods:\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        " \n",
        "# Conversion of vectors into a binary class matrix:\n",
        "from keras.utils import to_categorical\n",
        " \n",
        "# Indexing of samples for training, validation and test:\n",
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "# Neural network architecture (TensorFlow and Keras):\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.keras import backend\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.models import load_model\n",
        "from tensorflow.python.keras.layers import *\n",
        "from tensorflow.python.keras.callbacks import *\n",
        "from tensorflow.python.keras.optimizers import *\n",
        "\n",
        "# Scikit-optimize (2nd stage of optimization):\n",
        "!pip install scikit-optimize\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "from skopt.utils import use_named_args\n",
        "import skopt.plots\n",
        "from skopt import callbacks\n",
        "from skopt.callbacks import CheckpointSaver\n",
        "from skopt.callbacks import TimerCallback\n",
        " \n",
        "# Confusion matrix:\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "import itertools\n",
        " \n",
        "# Plotting tools:\n",
        "!pip install matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.ticker as ticker\n",
        "from keras.utils.vis_utils import plot_model\n",
        " \n",
        "# TensorDash (to check metrics by \"TensorDash\" Android app):\n",
        "!pip install tensor-dash\n",
        "from tensordash.tensordash import Tensordash"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Collecting scikit-optimize\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/03/be33e89f55866065a02e515c5b319304a801a9f1027a9b311a9b1d1f8dc7/scikit_optimize-0.8.1-py2.py3-none-any.whl (101kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.19.5)\n",
            "Collecting pyaml>=16.9\n",
            "  Downloading https://files.pythonhosted.org/packages/15/c4/1310a054d33abc318426a956e7d6df0df76a6ddfa9c66f6310274fb75d42/pyaml-20.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.22.2.post1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-20.4.0 scikit-optimize-0.8.1\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Collecting tensor-dash\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/a7/d6714fea01209936e23bdd1db91c61c94a8f6ee718fede28afbff63241d4/tensor_dash-1.8.1-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from tensor-dash) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->tensor-dash) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->tensor-dash) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->tensor-dash) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->tensor-dash) (3.0.4)\n",
            "Installing collected packages: tensor-dash\n",
            "Successfully installed tensor-dash-1.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSZ-f9TZu4jo"
      },
      "source": [
        "**Verification of the graphics card (GPU):**\r\n",
        "*   If hardware acelerator option is \"on\", then it's expected a Tesla T4, P100 or V100 models (V100>P100>T4)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kn9UYoeZ9wUX",
        "outputId": "1ef37dee-044a-449a-e0fc-2a54139052b6"
      },
      "source": [
        "# Verificando o modelo da GPU utilizada:\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 6170649969142828958, name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 15692777408\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 6217520858222415651\n",
              " physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hK5qjZlW9lVs"
      },
      "source": [
        "# 2.Dataset import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqublYYYXNcb"
      },
      "source": [
        "* The dataset is imported directly from Google Drive, just mount the virtual drive and establish the path of the root folder.\n",
        "\n",
        "* Already loaded, the Pandas dataframe object (tabular data) is converted to Numpy array (two-dimensional matrix)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzsZuqfn9XiA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85b33c8d-0d54-45dc-9ba3-f0bd9e7b7b43"
      },
      "source": [
        "# Módulo de importação de dados a partir do Google Drive:\n",
        "from google.colab import drive\n",
        "# Montando o Google Drive na máquina virtual do Colaboratory:\n",
        "drive.mount('/content/drive')\n",
        " \n",
        "# Importação do dataset original de corrente dos computadores:\n",
        "path = '/content/drive/My Drive/MESTRADO - UFES/Bancos de dados/Banco de dados B computadores/'\n",
        "\n",
        "path = path + 'current.csv'\n",
        "\n",
        "allData  = pd.read_csv(path, delimiter=';')\n",
        " \n",
        "# Transformação do dataframe em array:\n",
        "data0 = np.asarray(allData)\n",
        "\n",
        "# Adequação do tamanho total da base de dados:\n",
        "data = data0[0:999600, :]\n",
        " \n",
        "# Verificação do shape requerido:\n",
        "print('\\nThe matrix shape is:',data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "The matrix shape is: (999600, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwlsvGoDrgqy"
      },
      "source": [
        "# 3.First stage architecture (subset, model , train/test and optimizer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2vNcB00c7Ng"
      },
      "source": [
        "**In this section, the functions responsible for the subset, train and optimization of CNN are implemented:**\n",
        "\n",
        "\n",
        "* 3.1 (subset) Function that makes the subset ordered according to the number of cases required;\n",
        "* 3.2 (create_model) Function that creates the model and saves its architecture and initial weights;\n",
        "* 3.3 (fit_model) Function that trains and validates the created model;\n",
        "* 3.4 (test_model) Function that tests the model created and trained;\n",
        "* 3.5 (hillclimb) Function that uses the Hill Climbing heuristic to minimize the cost (test accuracy)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suyGXvdRGxrv"
      },
      "source": [
        "## 3.1 Preprocessing and subset function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaWKvmU2ONsl"
      },
      "source": [
        "**Observations:**\r\n",
        "\r\n",
        "*   The data manipulation and the labels creation are the same as in (FIRMES, 2020).\r\n",
        "* It was designed two strategies (investigation purposes) for the dataset processing:\r\n",
        " \r\n",
        "\r\n",
        "1.   Preprocessing BEFORE the data subset;\r\n",
        "2.   Preprocessing AFTER the data subset.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yQWX7QTB7Ce"
      },
      "source": [
        "### 3.1.1 Preprocessing BEFORE subset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UE5t_nQIKAu",
        "outputId": "e8a1d998-c9d0-4643-e5f3-8665cfa450e8"
      },
      "source": [
        "# PRÉ-PROCESSAMENTO ANTES DO SUBSET:\n",
        "# Processamento da base de dados (Normalização do conjunto inteiro):\n",
        "# O detalhamento deste processamento está na dissertação.\n",
        " \n",
        "data_transp = data.transpose() # Transposição: (999600,16) para (16,999600)\n",
        "# (Cuidado com o transpose. Se rodar a célula mais de uma vez, ele volta para a\n",
        "#  forma original da matriz e então dá erro!)\n",
        "\n",
        "# Redução dos dados para exatamente 999600 linhas (Verificar FIRMES, 2020):\n",
        "data_adj = data_transp[:, 0:999600]\n",
        "\n",
        "print('The transposed shape is:', data_adj.shape)\n",
        " \n",
        "data3d = data_adj.reshape(16, 600, 1666) # Reshape matriz 2D para 3D.\n",
        " \n",
        "# Criação dos labels:\n",
        "allData = np.empty((0,1667))\n",
        "ones = np.ones(600)\n",
        "\n",
        "# Separa os dados, atribuindo os labels das classes:\n",
        "for i in range(0, 16):\n",
        "  labels = i * ones\n",
        "  dataClass = data3d[i,:,:]\n",
        "  dataClass = np.column_stack((dataClass,labels))\n",
        "  allData = np.vstack((allData,dataClass))\n",
        " \n",
        "X = allData[:,:-1] # Fatia: todas as linhas, todas as colunas (menos a última).\n",
        "Y = allData[:,-1] # Fatia: todas as linhas, só a última coluna.\n",
        "\n",
        "# Procedimento para normalização/padronização da base como um todo:\n",
        "# Serão investigados três tipos de pré-processamento:\n",
        "scaler = StandardScaler()\n",
        "minmax = MinMaxScaler()\n",
        "powert = PowerTransformer(method='yeo-johnson', standardize=True)\n",
        "\n",
        "# Teste dos pré-processadores:-------------------------------------------------\n",
        "#Xt = minmax.fit_transform(X) # Testando com o método MinMaxScaler...\n",
        "Xt = scaler.fit_transform(X) # Testando com o método StandardScaler...\n",
        "#Xt = powert.fit_transform(X)  # Testando com o método PowerTransformer...\n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "print('The preprocessed (Xt) data shape is:', Xt.shape)\n",
        "\n",
        "Xt  # Verificação breve dos dados pré-processados..."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The transposed shape is: (16, 999600)\n",
            "The preprocessed (Xt) data shape is: (9600, 1666)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.47371101, 1.47380066, 1.47363811, ..., 1.47366888, 1.47383405,\n",
              "        1.47364487],\n",
              "       [1.4738908 , 1.47380066, 1.47381792, ..., 1.47402841, 1.47365428,\n",
              "        1.47382466],\n",
              "       [1.4738908 , 1.47371076, 1.47390782, ..., 1.47375876, 1.47374416,\n",
              "        1.47373476],\n",
              "       ...,\n",
              "       [1.47784629, 1.47613811, 1.47750408, ..., 1.47726415, 1.47662054,\n",
              "        1.47589217],\n",
              "       [1.47712711, 1.47649771, 1.47615549, ..., 1.47717427, 1.47644077,\n",
              "        1.47715066],\n",
              "       [1.47766649, 1.47640781, 1.47696465, ..., 1.47600581, 1.47769918,\n",
              "        1.4767012 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbwA7-LW1oHK"
      },
      "source": [
        "Below it's presented the function that **subsets** data by cases quantity:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqtfapuTycvo"
      },
      "source": [
        "# Função que faz o subset balanceado no dataset e nos labels criados:\n",
        "\n",
        "def subset(X, Y, n_cases):\n",
        "  '''\n",
        "  X: dataset pré-processado.\n",
        "  Y: labels criados.\n",
        "  n_cases: nº de casos que se deseja manter para cada classe.\n",
        "  '''\n",
        "  \n",
        "  Xnew = [] # Armazena o subset da base de dados.\n",
        "  Ynew = [] # Armazena o subset dos rótulos criados.\n",
        "  \n",
        "  # De cada classe (600 linhas), reduz conforme a quantidade de CASOS requerida.\n",
        "  for i in range(0, 9600, 600):\n",
        "    for j in X[i:i+n_cases]:\n",
        "      Xnew.append(j) # Adiciona os dados na nova lista.\n",
        "  \n",
        "  Xsubset = np.asarray(Xnew) # Transforma a lista em array, novamente.\n",
        "\n",
        "  # De cada classe (600 linhas), reduz conforme a quantidade de LABELS requerida.\n",
        "  for i in range(0, 9600, 600):\n",
        "    for j in Y[i:i+n_cases]:\n",
        "      Ynew.append(j) # Adiciona os dados na nova lista.\n",
        "  \n",
        "  Ysubset = np.asarray(Ynew) # Transforma a lista em array, novamente.\n",
        "\n",
        "  # Converte Y (labels) para categórico:\n",
        "  #(Isto é, converte o vetor de classe (inteiros) em matriz de classe binária)\n",
        "  Ysubset = to_categorical(Ysubset)\n",
        " \n",
        "  # Separação das amostras de treinamento, validação e teste:\n",
        "  # Indexação de 20% para TESTE:\n",
        "  X_train_val, X_test, Y_train_val, Y_test = train_test_split(Xsubset,\n",
        "                                                              Ysubset,\n",
        "                                                              test_size=0.20,\n",
        "                                                              random_state=42)\n",
        "  \n",
        "  # Do TREINAMENTO, indexa 20% para VALIDAÇÃO:\n",
        "  X_train, X_val, Y_train, Y_val = train_test_split(X_train_val,\n",
        "                                                    Y_train_val,\n",
        "                                                    test_size=0.20,\n",
        "                                                    random_state=42)\n",
        " \n",
        "  X_train = X_train.astype('float32')\n",
        "  X_val = X_val.astype('float32')\n",
        " \n",
        "  # Reshape das dimensões de X_train, X_val e X_test para se adequar à CNN:\n",
        "  X_train = X_train.reshape(X_train.shape[0],X_train.shape[1],1)\n",
        "  X_val = X_val.reshape(X_val.shape[0],X_val.shape[1],1)\n",
        "  X_test = X_test.reshape(X_test.shape[0], X_test.shape[1],1)\n",
        " \n",
        "  return X_train, Y_train, X_val, Y_val, X_test, Y_test, n_cases\n",
        "\n",
        "\n",
        "# Caso queira testar a função:\n",
        "#X_train, Y_train, X_val, Y_val, X_test, Y_test, n_cases = subset(Xt, Y, 600)\n",
        "#print(X_train.shape, Y_train.shape, X_val.shape, Y_val.shape, X_test.shape, Y_test.shape, n_cases)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQgdpRsJxaNO"
      },
      "source": [
        "**Don't need to run this next cell!**\r\n",
        "\r\n",
        "Was conduced an **experiment** solicited by Wanderley. The pre-processing method here was not applied directly in the whole dataset, but in \"case-by-case\" method. Was expected this method could handle better with the data, but this hipothesis couldn't be proved (low accuracy was identified).\r\n",
        "\r\n",
        "**Obs.:** This piece of code was left here in case of any experiments with different types of preprocessing in future works (and it took a lot of time to develop too :D)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMu1MpXLvB2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "063b5af3-d409-46b6-c61b-a2b1ff349785"
      },
      "source": [
        "# Processamento da base de dados (Normalização de caso em caso):\n",
        "# O detalhamento deste processamento está na dissertação.\n",
        " \n",
        "data_transp2 = data.transpose() # Transposição: (999600,16) para (16,999600)\n",
        "\n",
        "data_adj2 = data_transp2[:, 0:999600] # Reduzindo o dataset para exatamente 999600 amostras.\n",
        "\n",
        "print('The transposed shape is:', data_adj2.shape)\n",
        "\n",
        "data3d2 = data_adj2.reshape(16, 600, 1666) # Reshape matriz 2D para 3D.\n",
        "# (Cuidado com o transpose. Se rodar a célula mais de uma vez, ele volta para a\n",
        "#  forma original da matriz e então dá erro!)\n",
        " \n",
        "# Criação dos labels:\n",
        "allData = np.empty((0,1667))\n",
        "ones = np.ones(600)\n",
        "\n",
        "# Separa os dados, atribuindo os rótulos das classes:\n",
        "for i in range(0, 16):\n",
        "  labels = i * ones\n",
        "  dataClass = data3d2[i,:,:]\n",
        "  dataClass = np.column_stack((dataClass,labels))\n",
        "  allData = np.vstack((allData,dataClass))\n",
        " \n",
        "X = allData[:,:-1] # Fatia: todas as linhas, todas as colunas (menos a última).\n",
        "Y = allData[:,-1] # Fatia: todas as linhas, só a última coluna.\n",
        "\n",
        "# Procedimento para normalização/padronização a cada caso (1666 amostras):\n",
        "# Serão investigados três tipos de pré-processamento:\n",
        "scaler = StandardScaler()\n",
        "minmax = MinMaxScaler()\n",
        "powertr = PowerTransformer()\n",
        "\n",
        "Xn = [] # Lista que armazena os casos normalizados.\n",
        "Xs = [] # Lista que armazena os casos padronizados.\n",
        "Xp = [] # Lista que armazena os casos padronizados.\n",
        "\n",
        "# Padronizando/normalizando de caso em caso (cada 1666 amostras):\n",
        "for i in range(0,9600):\n",
        "  Xi = X[i,:]\n",
        "  Xi = Xi.reshape((-1,1)) # Reshape p/ adequar a entrada à ferramenta.\n",
        "  Zminmax = minmax.fit_transform(Xi)\n",
        "  Xn.append(Zminmax) # Atualiza a respectiva lista.\n",
        "  Zscaler = scaler.fit_transform(Xi)\n",
        "  Xs.append(Zscaler) # Atualiza a respectiva lista.\n",
        "  Zpower = powertr.fit_transform(Xi)\n",
        "  Xp.append(Zpower) # Atualiza a respectiva lista.\n",
        "\n",
        "Xn = np.asarray(Xn).reshape((9600, 1666)) # Transforma a lista em array, novamente.\n",
        "Xs = np.asarray(Xs).reshape((9600, 1666)) # Transforma a lista em array, novamente.\n",
        "Xp = np.asarray(Xp).reshape((9600, 1666)) # Transforma a lista em array, novamente.\n",
        "\n",
        "print('The standardized (Xs) data shape is:', Xs.shape,\n",
        "      '\\nThe normalized (Xn) data shape is:', Xn.shape,\n",
        "      '\\nThe transformed (Xp) data shape is:', Xp.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The transposed shape is: (16, 999600)\n",
            "The standardized (Xs) data shape is: (9600, 1666) \n",
            "The normalized (Xn) data shape is: (9600, 1666) \n",
            "The transformed (Xp) data shape is: (9600, 1666)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6QWZIpd3wae"
      },
      "source": [
        "Below it's only a sketch of the \"balanced subset\" design, that was applied to the main function (above). Was left here for general purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCr890bEfKvY",
        "outputId": "7add67cf-4abf-4360-a019-ef024b758b5b"
      },
      "source": [
        "# Teste do raciocínio para redução balanceada de amostras e labels de cada classe:\n",
        "\n",
        "uns = np.ones(600)\n",
        "empty = np.empty(600)\n",
        "Yzn = []\n",
        "for i in range(0,16):\n",
        "  Yz = i*uns\n",
        "  Yzn.append(Yz)\n",
        "\n",
        "# Reshape dos labels criados:\n",
        "Yzn = np.asarray(Yzn).reshape((9600,1))\n",
        "\n",
        "Yzn_ = [] # Armazena o subset dos labels criados.\n",
        "\n",
        "for i in range(0, 9600, 600):\n",
        "    for j in Yzn[i:i+1]: # O último dígito é a qtde. de casos que deseja manter.\n",
        "      Yzn_.append(j) # Adiciona os dados na nova lista.\n",
        "  \n",
        "Yznsubset = np.asarray(Yzn_) # Transforma a lista em array, novamente.\n",
        "\n",
        "Yznsubset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.],\n",
              "       [ 1.],\n",
              "       [ 2.],\n",
              "       [ 3.],\n",
              "       [ 4.],\n",
              "       [ 5.],\n",
              "       [ 6.],\n",
              "       [ 7.],\n",
              "       [ 8.],\n",
              "       [ 9.],\n",
              "       [10.],\n",
              "       [11.],\n",
              "       [12.],\n",
              "       [13.],\n",
              "       [14.],\n",
              "       [15.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgNFL-HsCE5F"
      },
      "source": [
        "### 3.1.2 Preprocessing AFTER subset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRGh8yXjRoDF"
      },
      "source": [
        "# FUNÇÃO QUE FAZ O SUBSET E PREPARAÇÃO DO BANCO DE DADOS (EXATAMENTE COMO O VICTOR FEZ):\r\n",
        "\r\n",
        "def subsetB(cases_num):\r\n",
        "  '''\r\n",
        "  cases_num: qtde. de casos que se deseja manter;\r\n",
        "  Retorna os inputs de treinamento/validação/teste para a CNN;\r\n",
        "  Preparação dos dados exatamente como realizado em FIRMES, 2020;\r\n",
        "  A rejeição dos casos é feita antes da preparação dos dados para a CNN\r\n",
        "  simulando uma situação real de coleta de poucas amostras/casos.\r\n",
        "  '''\r\n",
        "  \r\n",
        "  n_cases = cases_num  # Armazena a qtde. de casos utilizada para insights mais à frente.\r\n",
        " \r\n",
        "  data_transp = data.transpose()               # Transposição - (999600,16) para (16,999600)\r\n",
        "\r\n",
        "  data_sub = data_transp[:,:(1666 * cases_num)] # Fatiamento de colunas de acordo com a qtde. de casos requerida.\r\n",
        " \r\n",
        "  dataf = data_sub.reshape(16, cases_num, 1666) # Reshape matriz 2D para 3D (considerando casos totais)\r\n",
        " \r\n",
        "  # Criação dos labels:\r\n",
        "  allData = np.empty((0,1667))\r\n",
        "  ones = np.ones(cases_num)\r\n",
        "  # Separa os dados, atribuindo rótulo:\r\n",
        "  for i in range(0,16):\r\n",
        "    labels = i * ones\r\n",
        "    dataClass = dataf[i,:,:]\r\n",
        "    dataClass = np.column_stack((dataClass,labels))\r\n",
        "    allData = np.vstack((allData,dataClass))\r\n",
        " \r\n",
        "  X = allData[:,:-1] # Todas as linhas, todas as colunas (menos a última)\r\n",
        "  Y = allData[:,-1] # Todas as linhas, \r\n",
        " \r\n",
        "  # Normalização dos dados:\r\n",
        "  #(Transforma os dados para média zero e desvio padrão igual a um)\r\n",
        "  scaler = StandardScaler()\r\n",
        "  X = scaler.fit_transform(X)\r\n",
        " \r\n",
        "  # Converte Y para categórico:\r\n",
        "  #(Converte o vetor de classe (inteiros) em matriz de classe binária)\r\n",
        "  Y = to_categorical(Y)\r\n",
        " \r\n",
        "  # Separação das amostras de treinamento, validação e teste:\r\n",
        "  # Indexação de 20% para TESTE:\r\n",
        "  X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, test_size=0.20, random_state=42)\r\n",
        "  # Do TREINAMENTO, indexa 20% para VALIDAÇÃO:\r\n",
        "  X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size=0.20, random_state=42)\r\n",
        " \r\n",
        "  X_train = X_train.astype('float32')\r\n",
        "  X_val = X_val.astype('float32')\r\n",
        " \r\n",
        "  # Reshape das dimensões de X_train, X_val e X_test para se adequar à CNN:\r\n",
        "  X_train = X_train.reshape(X_train.shape[0],X_train.shape[1],1)\r\n",
        "  X_val = X_val.reshape(X_val.shape[0],X_val.shape[1],1)\r\n",
        "  X_test = X_test.reshape(X_test.shape[0], X_test.shape[1],1)\r\n",
        " \r\n",
        "  return X_train, Y_train, X_val, Y_val, X_test, Y_test, n_cases\r\n",
        " \r\n",
        "# Caso queira testar a função:\r\n",
        "#X_train, Y_train, X_val, Y_val, X_test, Y_test, n_cases = subsetB(600)\r\n",
        "#print(X_train.shape, Y_train.shape, X_val.shape, Y_val.shape, X_test.shape, Y_test.shape, n_cases)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uel3kCnHG88v"
      },
      "source": [
        "## 3.2 Model function (create_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uZLBINdTTdw"
      },
      "source": [
        "* It was necessary to implement the strategy of save the initially created model and upload it again, each round. Otherwise, the network continues to train its weights, causing the accuracy to be maintained even with the reduction of cases.\n",
        "\n",
        "\n",
        "* Therefore, the model created here (with all its information) is stored in its specific folder in Google Drive and, subsequently, the optimizer loads this same standard model each round."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujhpgi3Nyr0K"
      },
      "source": [
        "# Parâmetros para o Callbacks:\r\n",
        "\r\n",
        "# Determinando o caminho onde os parâmetros calculados serão salvos:\r\n",
        "checkpoint_filepath = '/content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current'\r\n",
        "# Salvando o modelo após cada época:\r\n",
        "checkpoint = ModelCheckpoint(filepath = checkpoint_filepath,\r\n",
        "                             monitor = 'val_accuracy', verbose = 1,\r\n",
        "                             mode = 'max', save_best_only = True)\r\n",
        "\r\n",
        "# EarlyStopping (O modelo pára o treinamento caso não perceba melhoria):\r\n",
        "earlystopping = EarlyStopping(monitor=\"val_accuracy\", min_delta=0, patience=100,\r\n",
        "                              verbose=1, mode=\"auto\", baseline=None,\r\n",
        "                              restore_best_weights=True)\r\n",
        "\r\n",
        "# TensorDash (acompanhamento das métricas do modelo pelo app Android):\r\n",
        "histories = Tensordash(ModelName = 'AutoML', email = 'viniciuswv@gmail.com',\r\n",
        "                       password = 'admin1')\r\n",
        "\r\n",
        "# Batch size (número de exemplos de treinamento usados em uma iteração/época):\r\n",
        "batch_size = 32\r\n",
        "\r\n",
        "# Épocas (quantidade de ciclos de treinamento da rede neural):\r\n",
        "epochs = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsTiA-whzJQS"
      },
      "source": [
        "# FUNÇÃO QUE CRIA A CNN:\n",
        "def create_model():\n",
        "  'Retorna o objeto (modelo), que estará sujeito à fitness function.'\n",
        "  'CNN de arquitetura nº 3 (FIRMES, 2020).'\n",
        "\n",
        "  # Criação das camadas com apoio do recurso 'keras.sequential':\n",
        "  model = Sequential() # Empilha linearmente as camadas da rede, conforme abaixo:\n",
        "  # Camada de alimentação do dataset:\n",
        "  model.add(InputLayer(input_shape=(1666,1))) # O tensor de entrada tem o shape (1666, 1).\n",
        " \n",
        "  # 1ª camada convolucional:\n",
        "  model.add(Conv1D(32, kernel_size=4, strides=1, activation='relu', padding='same'))\n",
        "  # 1ª camada MaxPooling:\n",
        "  model.add(MaxPooling1D(pool_size=3, strides=None, padding='valid'))\n",
        "  # 1ª camada de normalização do batch:\n",
        "  model.add(BatchNormalization())\n",
        "  \n",
        "  # 2ª camada convolucional:\n",
        "  model.add(Conv1D(64, kernel_size=4, strides=1, activation='relu', padding='same'))\n",
        "  # 2ª camada MaxPooling:\n",
        "  model.add(MaxPooling1D(pool_size=3, strides=None, padding='valid'))\n",
        "  # 2ª camada de normalização do batch:\n",
        "  model.add(BatchNormalization())\n",
        " \n",
        "  # 3ª camada convolucional:\n",
        "  model.add(Conv1D(128, kernel_size=4, strides=1, activation='relu', padding='same'))\n",
        "  # 3ª camada de MaxPooling:\n",
        "  model.add(MaxPooling1D(pool_size=3, strides=None, padding='valid'))\n",
        "  # 3ª camada de normalização do batch:\n",
        "  model.add(BatchNormalization())\n",
        "  \n",
        "  # 4ª camada convolucional:\n",
        "  model.add(Conv1D(256, kernel_size=4, strides=1, activation='relu', padding='same'))\n",
        "  # 4ª camada de MaxPooling:\n",
        "  model.add(MaxPooling1D(pool_size=3, strides=None, padding='valid'))\n",
        "  # 4ª camada de normalização do batch:\n",
        "  model.add(BatchNormalization())\n",
        "  \n",
        "  # Camada de achatamento (flatten):\n",
        "  model.add(Flatten())\n",
        "\n",
        "  # 1ª camada densa (fully connected):\n",
        "  model.add(Dense(512, activation='relu'))\n",
        "  # 2ª camada densa (fully connected):\n",
        "  model.add(Dense(256, activation='relu'))\n",
        "  # 3ª camada densa (fully connected):\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  # 4ª camada densa (fully connected):\n",
        "  model.add(Dense(16, activation='softmax'))\n",
        "\n",
        "  \n",
        "  # Otimizador da rede:\n",
        "  optimizer = tf.keras.optimizers.SGD(learning_rate= 0.01, name=\"SGD\")\n",
        "\n",
        "\n",
        "  # Compilação do modelo neural: \n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer= optimizer,\n",
        "                metrics=['accuracy'])\n",
        " \n",
        "  return model\n",
        " \n",
        "# Criação do modelo e verificação de seu sumário:\n",
        "model = create_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjhJVmYz0ohu"
      },
      "source": [
        "# (Computadores) Determinando o caminho para salvamento do modelo:\n",
        "save_path = '/content/drive/My Drive/MESTRADO - UFES/Bancos de dados/Banco de dados B computadores/standard_cnn/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQdbR40i0vdu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "703a3ebd-663b-42d5-88ae-2c052065fe2f"
      },
      "source": [
        "# Salvando o modelo e os pesos não treinados como referência para o looping:\n",
        "model.save(save_path,\n",
        "           overwrite = True,\n",
        "           save_format = 'initial_model_h5.h5') # ou 'tf'."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/MESTRADO - UFES/Bancos de dados/Banco de dados B computadores/standard_cnn/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj9cksoVO3pc"
      },
      "source": [
        "The summary is important to compare the structure and the quantity of parameters in the (FIRMES, 2020) architecture with the new optimized architecture:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVpt8LokOoXW",
        "outputId": "17a02672-de59-4664-a3d8-a38ebd64ccc1"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d (Conv1D)              (None, 1666, 32)          160       \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 555, 32)           0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 555, 32)           128       \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 555, 64)           8256      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 185, 64)           0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 185, 64)           256       \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 185, 128)          32896     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 61, 128)           0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 61, 128)           512       \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 61, 256)           131328    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 20, 256)           0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 20, 256)           1024      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 5120)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               2621952   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 16)                2064      \n",
            "=================================================================\n",
            "Total params: 2,962,800\n",
            "Trainable params: 2,961,840\n",
            "Non-trainable params: 960\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meKxSv8JIf6o"
      },
      "source": [
        "## 3.3 Training function (train / validation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWhjJTXVuw2V"
      },
      "source": [
        "# FUNÇÃO QUE TREINA O MODELO (train/validation):\n",
        "def fit_model(model, X_train, Y_train, X_val, Y_val, batch_size, epochs):\n",
        "  '''\n",
        "  Recebe o modelo, o dataset reduzido, o tamanho de batch e qtd. de épocas.\n",
        "  Retorna o modelo (treinado/validado), history e valor máx. de 'val_accuracy'.\n",
        "  '''\n",
        "  \n",
        "  # Reinicia a sessão do Keras, evitando sobrecarregamento de memória:\n",
        "  tf.keras.backend.clear_session()\n",
        "\n",
        "  start_time = time.time()         # Parâmetro para tempo de treinamento.\n",
        "  \n",
        "  # Método \".fit()\" do Keras:\n",
        "  history = model.fit(X_train,\n",
        "                      Y_train,\n",
        "                      batch_size = batch_size,\n",
        "                      epochs = epochs,\n",
        "                      verbose = 1,\n",
        "                      validation_data = (X_val, Y_val),\n",
        "                      callbacks = [histories,\n",
        "                                   checkpoint,\n",
        "                                   earlystopping])\n",
        "   \n",
        "  accuracy = history.history['val_accuracy']\n",
        "   \n",
        "  acc_max = max(accuracy)           # Armazena a maior val_accuracy da lista.\n",
        " \n",
        "  end_time = time.time()            # Parâmetro para tempo de treinamento.\n",
        " \n",
        "  # Visualização do tempo total de treinamento:\n",
        "  print(\"Training/validation time was: %g seconds\" % (end_time - start_time))\n",
        "\n",
        "  train_time = (end_time - start_time)\n",
        " \n",
        "  return model, history, acc_max, train_time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N2JZ3-XIpkG"
      },
      "source": [
        "## 3.4 Testing function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMZED4DBrx8x"
      },
      "source": [
        "# FUNÇÃO QUE TESTA O MODELO:\n",
        "def test_model(X_test, Y_test, model):\n",
        "  'Recebe as amostras de teste e labels (X, Y), assim como o novo modelo gerado.'\n",
        "  'Retorna a acurácia de teste (é o parâmetro para continuar iterando).'\n",
        "  \n",
        "  print('\\n',\n",
        "        '\\nEVALUATING THE MODEL:')\n",
        "  \n",
        "  start_time = time.time()\n",
        " \n",
        "  scores = model.evaluate(X_test, Y_test, verbose=1, return_dict=False)\n",
        " \n",
        "  end_time = time.time()\n",
        " \n",
        "  print(\"Evaluate time was %g seconds\" % (end_time - start_time))\n",
        " \n",
        "  test_accuracy = (scores[1])  # Armazena apenas a acurácia do teste.\n",
        "  \n",
        "  print(test_accuracy)\n",
        " \n",
        "  return test_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_rDfUP5I7Ki"
      },
      "source": [
        "## 3.5 Optimization function (Hill Climbdown)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWt71G_dgWJW"
      },
      "source": [
        "**This function consists of the following sub-functions:**\n",
        "\n",
        "1. Function that makes the subset;\n",
        "2. Command to load the standard CNN, with it's saved weights;\n",
        "3. Function that trains / validates CNN;\n",
        "4. Function that tests CNN;\n",
        "5. Fitness function for the iterative process of the Hill Climbdown heuristic.\n",
        "\n",
        "* As suggested by Professor Helder, we establish as fitness (cost, which will be minimized) the test accuracy itself!\n",
        "\n",
        "* The optimizer logic is based on the analysis of the neighboring solution, that is, at each round of case reduction, if the test accuracy is less than fitness (which starts at 1), fitness takes on this value and starts a new one round of case reduction and network training. This process is repeated until fitness (target accuracy) reaches 0.95 (95%).\n",
        "* The target accuracy was established at 95% because, according to the literature, in statistical tests a 95% confidence level can be admitted. Therefore, the idea is to take advantage of this \"gap\" to investigate how much we can reduce of the total dataset (FIRMES, 2020) due to the admitted statistical reliability.\n",
        "* **Important:** It can happen that the optimizer finds accuracy above 95% even with very low cases in the database. Accuracy alone cannot guarantee the robustness of the network under any circumstances. Therefore, a limit on the search space will be established, which means that the first stage will be able to reduce the data to up to 60 cases (10% of the total)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_1UYNC9tQI7"
      },
      "source": [
        "# FUNÇÃO DO OTIMIZADOR (DESCIDA DE ENCOSTA):\n",
        "def hillclimb():\n",
        "  'Reduz casos até que a acurácia de teste atinja um valor pré determinado.'\n",
        "  'Retorna o melhor modelo, históricos de fitness e redução de casos.'\n",
        " \n",
        "  acc_target = 0.95  # Qual a acurácia de teste alvo?\n",
        "  cases = 650        # Preencher com (600 + decrease step value utilizado).\n",
        "  fitness = 1        # Fitness inicia em 1 e irá descer.\n",
        "  \n",
        "  # Armazenamento das listas contendo o histórico da otimização:\n",
        "  fitness_hist = []\n",
        "  n_cases_hist = []\n",
        "  acc_val_hist = []\n",
        "  acc_test_hist = []\n",
        "  time_hist = []\n",
        " \n",
        "  # Lógica para percorrer o espaço de busca:\n",
        "  while fitness > acc_target:\n",
        "    cases = cases - 50  # Decrease step value (passo da redução de casos).\n",
        "\n",
        "    # Restrinja o espaço de busca para até 100 casos - verificar (VIANNA, 2021)\n",
        "    if cases == 0: # Como o passo é de 100 em 100...\n",
        "      break\n",
        "\n",
        "    # Faça o subset das amostras:\n",
        "    X_train_new, Y_train_new, X_val_new, Y_val_new, X_test_new, Y_test_new, n_cases = subsetB(cases)\n",
        " \n",
        "    # Carregue o modelo (CNN) padrão salvo:\n",
        "    model = load_model(save_path)\n",
        "\n",
        "    # Imprima algumas informações úteis durante o processo:\n",
        "    print('\\n---> ITERATING NOW WITH:',n_cases,'CASES !',\n",
        "          '(',\"%.2f\" % round((((n_cases)/600)*100),2),'% FROM TOTAL )',\n",
        "          '\\n----> AND THE HISTORY OF CASES DECREASE IS:',n_cases_hist,\n",
        "          '\\n',\n",
        "          '\\nTRAINING AND VALIDATING THE MODEL:',\n",
        "          '\\nIt will take a while ;D...')\n",
        "    \n",
        "    # Treine o modelo com as novas amostras:\n",
        "    model2, history, acc_max, train_time = fit_model(model, X_train_new, Y_train_new, X_val_new, Y_val_new, batch_size, epochs)\n",
        "\n",
        "    # Teste o modelo com as novas amostras:\n",
        "    test_accuracy = test_model(X_test_new, Y_test_new, model2)\n",
        "\n",
        "    # Imprima algumas informações úteis durante o processo:\n",
        "    print('\\nSUMMARY:',\n",
        "          '\\nWith',n_cases,'cases, the max val_accuracy was:',\n",
        "          \"%.4f\" % round((acc_max*100),4),'% and the test_accuracy was:',\n",
        "          \"%.4f\" % round((test_accuracy*100),4),'%.',\n",
        "          '\\nIt is a difference of',\n",
        "          \"%.4f\" % round(((acc_max*100)-(test_accuracy*100)),4),'%.')\n",
        "    \n",
        "    n_cases_hist.append(n_cases) # Armazene o histórico de redução de casos.\n",
        "    fitness_hist.append(fitness) # Armazene o histórico de fitness.\n",
        "    acc_val_hist.append(acc_max) # Armazene o histórico da acurácia de validação.\n",
        "    acc_test_hist.append(test_accuracy) # Armazene o histórico da acurácia de teste.\n",
        "    time_hist.append(train_time) # Armazene os tempos gastos nos treinamentos.\n",
        "\n",
        "    # Imprima o histórico parcial durante o processo:\n",
        "    # (Para caso houver algum problema, saber por onde começar)\n",
        "    print('Fitness history is:', fitness_hist,\n",
        "          '\\nCases history is:', n_cases_hist,\n",
        "          '\\nacc_val history is:', acc_val_hist,\n",
        "          '\\nacc_test history is:', acc_test_hist,\n",
        "          '\\nTimes history is:', time_hist)\n",
        "    \n",
        "    # Calcule a Fitness Function:\n",
        "    if test_accuracy < fitness:\n",
        "      fitness = test_accuracy # Fitness assume o valor da nova acurácia de teste.\n",
        "    \n",
        "    # Apague os modelos (pois suas informações podem persistir no estado geral):\n",
        "    del model, model2\n",
        "    # Reinicie a sessão do Keras, evitando sobrecarregamento de memória:\n",
        "    K.clear_session()\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.compat.v1.reset_default_graph()\n",
        "    \n",
        "  # Imprime a conclusão do processo de otimização:    \n",
        "  print('\\nHILL CLIMBING: For the configured step, was possible to reduce the dataset to',\n",
        "        n_cases_hist[-1],'cases, with test_accuracy of',acc_test_hist[-1])\n",
        " \n",
        "  return history, fitness_hist, n_cases_hist, acc_val_hist, acc_test_hist, time_hist, acc_target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUl7jFBCybn9"
      },
      "source": [
        "# 4.First stage simulation (cases number)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIOOS560BPMN"
      },
      "source": [
        "**When starting this cell, the \"Hill Climb function\" will automatically instantiate all the other functions, starting an iterative sequence in the form:**\n",
        "\n",
        "1. Subset a portion of the samples;\n",
        "2. Load the initial standard model (with respective weights);\n",
        "3. Train and validate;\n",
        "4. Test;\n",
        "5. Calculation of the fitness function (acc_test >= 95% ?);\n",
        "6. Restart the entire process, now with a smaller number of cases (and according to the adjusted step).\n",
        "\n",
        "*The results (graphs and insights) are presented in section 5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Lb9AU4ryYNP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a055ebf9-af62-4204-a4f9-990f64983f4e"
      },
      "source": [
        "# Execução da fitness function:\n",
        "history, fitness_hist, n_cases_hist, acc_val_hist, acc_test_hist, time_hist, acc_target = hillclimb()\n",
        " \n",
        "print('\\nThe fitness history is:',fitness_hist,\n",
        "      '\\nThe cases decrease history is:',n_cases_hist)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mA saída de streaming foi truncada nas últimas 5000 linhas.\u001b[0m\n",
            "Epoch 00337: val_accuracy did not improve from 0.98568\n",
            "Epoch 338/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0082 - accuracy: 0.9977 - val_loss: 0.0568 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00338: val_accuracy did not improve from 0.98568\n",
            "Epoch 339/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0065 - accuracy: 0.9977 - val_loss: 0.0620 - val_accuracy: 0.9785\n",
            "\n",
            "Epoch 00339: val_accuracy did not improve from 0.98568\n",
            "Epoch 340/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 0.0396 - val_accuracy: 0.9876\n",
            "\n",
            "Epoch 00340: val_accuracy improved from 0.98568 to 0.98763, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 341/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0119 - accuracy: 0.9971 - val_loss: 0.0918 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00341: val_accuracy did not improve from 0.98763\n",
            "Epoch 342/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0043 - accuracy: 0.9982 - val_loss: 0.0423 - val_accuracy: 0.9876\n",
            "\n",
            "Epoch 00342: val_accuracy did not improve from 0.98763\n",
            "Epoch 343/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0147 - accuracy: 0.9967 - val_loss: 0.1866 - val_accuracy: 0.9388\n",
            "\n",
            "Epoch 00343: val_accuracy did not improve from 0.98763\n",
            "Epoch 344/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0068 - accuracy: 0.9974 - val_loss: 0.0637 - val_accuracy: 0.9811\n",
            "\n",
            "Epoch 00344: val_accuracy did not improve from 0.98763\n",
            "Epoch 345/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0072 - accuracy: 0.9977 - val_loss: 0.1195 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00345: val_accuracy did not improve from 0.98763\n",
            "Epoch 346/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0040 - accuracy: 0.9985 - val_loss: 0.0494 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00346: val_accuracy did not improve from 0.98763\n",
            "Epoch 347/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0039 - accuracy: 0.9989 - val_loss: 0.0556 - val_accuracy: 0.9792\n",
            "\n",
            "Epoch 00347: val_accuracy did not improve from 0.98763\n",
            "Epoch 348/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0039 - accuracy: 0.9985 - val_loss: 0.0572 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00348: val_accuracy did not improve from 0.98763\n",
            "Epoch 349/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0477 - val_accuracy: 0.9850\n",
            "\n",
            "Epoch 00349: val_accuracy did not improve from 0.98763\n",
            "Epoch 350/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0029 - accuracy: 0.9990 - val_loss: 0.0411 - val_accuracy: 0.9876\n",
            "\n",
            "Epoch 00350: val_accuracy did not improve from 0.98763\n",
            "Epoch 351/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 9.7699e-04 - accuracy: 1.0000 - val_loss: 0.0436 - val_accuracy: 0.9876\n",
            "\n",
            "Epoch 00351: val_accuracy did not improve from 0.98763\n",
            "Epoch 352/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0479 - val_accuracy: 0.9863\n",
            "\n",
            "Epoch 00352: val_accuracy did not improve from 0.98763\n",
            "Epoch 353/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.0503 - val_accuracy: 0.9857\n",
            "\n",
            "Epoch 00353: val_accuracy did not improve from 0.98763\n",
            "Epoch 354/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 0.0574 - val_accuracy: 0.9818\n",
            "\n",
            "Epoch 00354: val_accuracy did not improve from 0.98763\n",
            "Epoch 355/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0042 - accuracy: 0.9987 - val_loss: 0.0660 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00355: val_accuracy did not improve from 0.98763\n",
            "Epoch 356/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.0530 - val_accuracy: 0.9857\n",
            "\n",
            "Epoch 00356: val_accuracy did not improve from 0.98763\n",
            "Epoch 357/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.0505 - val_accuracy: 0.9850\n",
            "\n",
            "Epoch 00357: val_accuracy did not improve from 0.98763\n",
            "Epoch 358/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0021 - accuracy: 0.9992 - val_loss: 0.0424 - val_accuracy: 0.9870\n",
            "\n",
            "Epoch 00358: val_accuracy did not improve from 0.98763\n",
            "Epoch 359/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0020 - accuracy: 0.9993 - val_loss: 0.0444 - val_accuracy: 0.9870\n",
            "\n",
            "Epoch 00359: val_accuracy did not improve from 0.98763\n",
            "Epoch 360/500\n",
            "192/192 [==============================] - 1s 7ms/step - loss: 0.0036 - accuracy: 0.9985 - val_loss: 0.1119 - val_accuracy: 0.9674\n",
            "\n",
            "Epoch 00360: val_accuracy did not improve from 0.98763\n",
            "Epoch 361/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0076 - accuracy: 0.9977 - val_loss: 0.0528 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00361: val_accuracy did not improve from 0.98763\n",
            "Epoch 362/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.0510 - val_accuracy: 0.9857\n",
            "\n",
            "Epoch 00362: val_accuracy did not improve from 0.98763\n",
            "Epoch 363/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.0434 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00363: val_accuracy improved from 0.98763 to 0.98828, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 364/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.0476 - val_accuracy: 0.9863\n",
            "\n",
            "Epoch 00364: val_accuracy did not improve from 0.98828\n",
            "Epoch 365/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0042 - accuracy: 0.9987 - val_loss: 0.4090 - val_accuracy: 0.9251\n",
            "\n",
            "Epoch 00365: val_accuracy did not improve from 0.98828\n",
            "Epoch 366/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0022 - accuracy: 0.9992 - val_loss: 0.0418 - val_accuracy: 0.9876\n",
            "\n",
            "Epoch 00366: val_accuracy did not improve from 0.98828\n",
            "Epoch 367/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.0455 - val_accuracy: 0.9870\n",
            "\n",
            "Epoch 00367: val_accuracy did not improve from 0.98828\n",
            "Epoch 368/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0452 - val_accuracy: 0.9863\n",
            "\n",
            "Epoch 00368: val_accuracy did not improve from 0.98828\n",
            "Epoch 369/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.0398 - val_accuracy: 0.9889\n",
            "\n",
            "Epoch 00369: val_accuracy improved from 0.98828 to 0.98893, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 370/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.0459 - val_accuracy: 0.9857\n",
            "\n",
            "Epoch 00370: val_accuracy did not improve from 0.98893\n",
            "Epoch 371/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0021 - accuracy: 0.9993 - val_loss: 0.0430 - val_accuracy: 0.9870\n",
            "\n",
            "Epoch 00371: val_accuracy did not improve from 0.98893\n",
            "Epoch 372/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0016 - accuracy: 0.9995 - val_loss: 0.0419 - val_accuracy: 0.9850\n",
            "\n",
            "Epoch 00372: val_accuracy did not improve from 0.98893\n",
            "Epoch 373/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.0384 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00373: val_accuracy did not improve from 0.98893\n",
            "Epoch 374/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0032 - accuracy: 0.9989 - val_loss: 0.0523 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00374: val_accuracy did not improve from 0.98893\n",
            "Epoch 375/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.0384 - val_accuracy: 0.9876\n",
            "\n",
            "Epoch 00375: val_accuracy did not improve from 0.98893\n",
            "Epoch 376/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0023 - accuracy: 0.9990 - val_loss: 0.0466 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00376: val_accuracy did not improve from 0.98893\n",
            "Epoch 377/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0020 - accuracy: 0.9993 - val_loss: 0.0403 - val_accuracy: 0.9876\n",
            "\n",
            "Epoch 00377: val_accuracy did not improve from 0.98893\n",
            "Epoch 378/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0041 - accuracy: 0.9990 - val_loss: 0.0456 - val_accuracy: 0.9876\n",
            "\n",
            "Epoch 00378: val_accuracy did not improve from 0.98893\n",
            "Epoch 379/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.0551 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00379: val_accuracy did not improve from 0.98893\n",
            "Epoch 380/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0529 - val_accuracy: 0.9850\n",
            "\n",
            "Epoch 00380: val_accuracy did not improve from 0.98893\n",
            "Epoch 381/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.0497 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00381: val_accuracy did not improve from 0.98893\n",
            "Epoch 382/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0040 - accuracy: 0.9987 - val_loss: 0.0404 - val_accuracy: 0.9870\n",
            "\n",
            "Epoch 00382: val_accuracy did not improve from 0.98893\n",
            "Epoch 383/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0136 - accuracy: 0.9959 - val_loss: 0.0452 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00383: val_accuracy did not improve from 0.98893\n",
            "Epoch 384/500\n",
            "192/192 [==============================] - 1s 7ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0384 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00384: val_accuracy did not improve from 0.98893\n",
            "Epoch 385/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.0403 - val_accuracy: 0.9889\n",
            "\n",
            "Epoch 00385: val_accuracy did not improve from 0.98893\n",
            "Epoch 386/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0018 - accuracy: 0.9993 - val_loss: 0.0453 - val_accuracy: 0.9870\n",
            "\n",
            "Epoch 00386: val_accuracy did not improve from 0.98893\n",
            "Epoch 387/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0022 - accuracy: 0.9992 - val_loss: 0.0393 - val_accuracy: 0.9857\n",
            "\n",
            "Epoch 00387: val_accuracy did not improve from 0.98893\n",
            "Epoch 388/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0029 - accuracy: 0.9990 - val_loss: 0.6167 - val_accuracy: 0.8613\n",
            "\n",
            "Epoch 00388: val_accuracy did not improve from 0.98893\n",
            "Epoch 389/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0020 - accuracy: 0.9993 - val_loss: 0.0403 - val_accuracy: 0.9870\n",
            "\n",
            "Epoch 00389: val_accuracy did not improve from 0.98893\n",
            "Epoch 390/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0016 - accuracy: 0.9995 - val_loss: 0.0558 - val_accuracy: 0.9831\n",
            "\n",
            "Epoch 00390: val_accuracy did not improve from 0.98893\n",
            "Epoch 391/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0045 - accuracy: 0.9987 - val_loss: 0.0632 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00391: val_accuracy did not improve from 0.98893\n",
            "Epoch 392/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.0391 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00392: val_accuracy did not improve from 0.98893\n",
            "Epoch 393/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0018 - accuracy: 0.9993 - val_loss: 0.0479 - val_accuracy: 0.9863\n",
            "\n",
            "Epoch 00393: val_accuracy did not improve from 0.98893\n",
            "Epoch 394/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0026 - accuracy: 0.9990 - val_loss: 0.0595 - val_accuracy: 0.9831\n",
            "\n",
            "Epoch 00394: val_accuracy did not improve from 0.98893\n",
            "Epoch 395/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0016 - accuracy: 0.9993 - val_loss: 0.0438 - val_accuracy: 0.9876\n",
            "\n",
            "Epoch 00395: val_accuracy did not improve from 0.98893\n",
            "Epoch 396/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 8.5891e-04 - accuracy: 1.0000 - val_loss: 0.0409 - val_accuracy: 0.9876\n",
            "\n",
            "Epoch 00396: val_accuracy did not improve from 0.98893\n",
            "Epoch 397/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.0407 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00397: val_accuracy did not improve from 0.98893\n",
            "Epoch 398/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 8.4586e-04 - accuracy: 0.9998 - val_loss: 0.0413 - val_accuracy: 0.9870\n",
            "\n",
            "Epoch 00398: val_accuracy did not improve from 0.98893\n",
            "Epoch 399/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 8.2414e-04 - accuracy: 0.9998 - val_loss: 0.0400 - val_accuracy: 0.9889\n",
            "\n",
            "Epoch 00399: val_accuracy did not improve from 0.98893\n",
            "Epoch 400/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0403 - val_accuracy: 0.9863\n",
            "\n",
            "Epoch 00400: val_accuracy did not improve from 0.98893\n",
            "Epoch 401/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 6.6798e-04 - accuracy: 0.9998 - val_loss: 0.0391 - val_accuracy: 0.9889\n",
            "\n",
            "Epoch 00401: val_accuracy did not improve from 0.98893\n",
            "Epoch 402/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 5.2128e-04 - accuracy: 1.0000 - val_loss: 0.0402 - val_accuracy: 0.9896\n",
            "\n",
            "Epoch 00402: val_accuracy improved from 0.98893 to 0.98958, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 403/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 5.0642e-04 - accuracy: 1.0000 - val_loss: 0.0371 - val_accuracy: 0.9896\n",
            "\n",
            "Epoch 00403: val_accuracy did not improve from 0.98958\n",
            "Epoch 404/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 6.9011e-04 - accuracy: 0.9998 - val_loss: 0.0459 - val_accuracy: 0.9850\n",
            "\n",
            "Epoch 00404: val_accuracy did not improve from 0.98958\n",
            "Epoch 405/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 9.5192e-04 - accuracy: 0.9997 - val_loss: 0.0391 - val_accuracy: 0.9889\n",
            "\n",
            "Epoch 00405: val_accuracy did not improve from 0.98958\n",
            "Epoch 406/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0396 - val_accuracy: 0.9889\n",
            "\n",
            "Epoch 00406: val_accuracy did not improve from 0.98958\n",
            "Epoch 407/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.0435 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00407: val_accuracy did not improve from 0.98958\n",
            "Epoch 408/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 9.6067e-04 - accuracy: 1.0000 - val_loss: 0.0383 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00408: val_accuracy did not improve from 0.98958\n",
            "Epoch 409/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 7.8120e-04 - accuracy: 0.9998 - val_loss: 0.0445 - val_accuracy: 0.9863\n",
            "\n",
            "Epoch 00409: val_accuracy did not improve from 0.98958\n",
            "Epoch 410/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0044 - accuracy: 0.9982 - val_loss: 0.0522 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00410: val_accuracy did not improve from 0.98958\n",
            "Epoch 411/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0022 - accuracy: 0.9993 - val_loss: 0.0418 - val_accuracy: 0.9889\n",
            "\n",
            "Epoch 00411: val_accuracy did not improve from 0.98958\n",
            "Epoch 412/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 0.0429 - val_accuracy: 0.9876\n",
            "\n",
            "Epoch 00412: val_accuracy did not improve from 0.98958\n",
            "Epoch 413/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0028 - accuracy: 0.9990 - val_loss: 0.0705 - val_accuracy: 0.9818\n",
            "\n",
            "Epoch 00413: val_accuracy did not improve from 0.98958\n",
            "Epoch 414/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.0435 - val_accuracy: 0.9870\n",
            "\n",
            "Epoch 00414: val_accuracy did not improve from 0.98958\n",
            "Epoch 415/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 7.9074e-04 - accuracy: 0.9998 - val_loss: 0.0368 - val_accuracy: 0.9896\n",
            "\n",
            "Epoch 00415: val_accuracy did not improve from 0.98958\n",
            "Epoch 416/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.0438 - val_accuracy: 0.9857\n",
            "\n",
            "Epoch 00416: val_accuracy did not improve from 0.98958\n",
            "Epoch 417/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.0414 - val_accuracy: 0.9876\n",
            "\n",
            "Epoch 00417: val_accuracy did not improve from 0.98958\n",
            "Epoch 418/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 3.0811e-04 - accuracy: 1.0000 - val_loss: 0.0392 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00418: val_accuracy did not improve from 0.98958\n",
            "Epoch 419/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 7.2787e-04 - accuracy: 0.9998 - val_loss: 0.0521 - val_accuracy: 0.9857\n",
            "\n",
            "Epoch 00419: val_accuracy did not improve from 0.98958\n",
            "Epoch 420/500\n",
            "192/192 [==============================] - 1s 7ms/step - loss: 6.2488e-04 - accuracy: 1.0000 - val_loss: 0.0349 - val_accuracy: 0.9909\n",
            "\n",
            "Epoch 00420: val_accuracy improved from 0.98958 to 0.99089, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 421/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 3.8670e-04 - accuracy: 1.0000 - val_loss: 0.0390 - val_accuracy: 0.9889\n",
            "\n",
            "Epoch 00421: val_accuracy did not improve from 0.99089\n",
            "Epoch 422/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 4.7418e-04 - accuracy: 0.9998 - val_loss: 0.0402 - val_accuracy: 0.9863\n",
            "\n",
            "Epoch 00422: val_accuracy did not improve from 0.99089\n",
            "Epoch 423/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 3.1787e-04 - accuracy: 1.0000 - val_loss: 0.0400 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00423: val_accuracy did not improve from 0.99089\n",
            "Epoch 424/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 4.8247e-04 - accuracy: 1.0000 - val_loss: 0.0394 - val_accuracy: 0.9876\n",
            "\n",
            "Epoch 00424: val_accuracy did not improve from 0.99089\n",
            "Epoch 425/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0389 - val_accuracy: 0.9896\n",
            "\n",
            "Epoch 00425: val_accuracy did not improve from 0.99089\n",
            "Epoch 426/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 7.4913e-04 - accuracy: 0.9998 - val_loss: 0.0373 - val_accuracy: 0.9896\n",
            "\n",
            "Epoch 00426: val_accuracy did not improve from 0.99089\n",
            "Epoch 427/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 4.2657e-04 - accuracy: 1.0000 - val_loss: 0.0397 - val_accuracy: 0.9889\n",
            "\n",
            "Epoch 00427: val_accuracy did not improve from 0.99089\n",
            "Epoch 428/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 4.8519e-04 - accuracy: 1.0000 - val_loss: 0.0361 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00428: val_accuracy did not improve from 0.99089\n",
            "Epoch 429/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 9.8670e-04 - accuracy: 0.9993 - val_loss: 0.2479 - val_accuracy: 0.9395\n",
            "\n",
            "Epoch 00429: val_accuracy did not improve from 0.99089\n",
            "Epoch 430/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 4.8613e-04 - accuracy: 1.0000 - val_loss: 0.0412 - val_accuracy: 0.9870\n",
            "\n",
            "Epoch 00430: val_accuracy did not improve from 0.99089\n",
            "Epoch 431/500\n",
            "192/192 [==============================] - 1s 7ms/step - loss: 9.2819e-04 - accuracy: 0.9997 - val_loss: 0.0381 - val_accuracy: 0.9889\n",
            "\n",
            "Epoch 00431: val_accuracy did not improve from 0.99089\n",
            "Epoch 432/500\n",
            "192/192 [==============================] - 1s 7ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0491 - val_accuracy: 0.9863\n",
            "\n",
            "Epoch 00432: val_accuracy did not improve from 0.99089\n",
            "Epoch 433/500\n",
            "192/192 [==============================] - 1s 8ms/step - loss: 5.1226e-04 - accuracy: 1.0000 - val_loss: 0.0417 - val_accuracy: 0.9876\n",
            "\n",
            "Epoch 00433: val_accuracy did not improve from 0.99089\n",
            "Epoch 434/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 7.9383e-04 - accuracy: 0.9997 - val_loss: 0.0402 - val_accuracy: 0.9876\n",
            "\n",
            "Epoch 00434: val_accuracy did not improve from 0.99089\n",
            "Epoch 435/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 4.2283e-04 - accuracy: 1.0000 - val_loss: 0.0439 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00435: val_accuracy did not improve from 0.99089\n",
            "Epoch 436/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 2.9379e-04 - accuracy: 1.0000 - val_loss: 0.0410 - val_accuracy: 0.9896\n",
            "\n",
            "Epoch 00436: val_accuracy did not improve from 0.99089\n",
            "Epoch 437/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 4.4716e-04 - accuracy: 1.0000 - val_loss: 0.0465 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00437: val_accuracy did not improve from 0.99089\n",
            "Epoch 438/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 4.8760e-04 - accuracy: 1.0000 - val_loss: 0.0458 - val_accuracy: 0.9876\n",
            "\n",
            "Epoch 00438: val_accuracy did not improve from 0.99089\n",
            "Epoch 439/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 4.0023e-04 - accuracy: 1.0000 - val_loss: 0.0393 - val_accuracy: 0.9876\n",
            "\n",
            "Epoch 00439: val_accuracy did not improve from 0.99089\n",
            "Epoch 440/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 2.9798e-04 - accuracy: 1.0000 - val_loss: 0.0436 - val_accuracy: 0.9889\n",
            "\n",
            "Epoch 00440: val_accuracy did not improve from 0.99089\n",
            "Epoch 441/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 4.8518e-04 - accuracy: 0.9998 - val_loss: 0.0418 - val_accuracy: 0.9889\n",
            "\n",
            "Epoch 00441: val_accuracy did not improve from 0.99089\n",
            "Epoch 442/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.0422 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00442: val_accuracy did not improve from 0.99089\n",
            "Epoch 443/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 2.5062e-04 - accuracy: 1.0000 - val_loss: 0.0412 - val_accuracy: 0.9889\n",
            "\n",
            "Epoch 00443: val_accuracy did not improve from 0.99089\n",
            "Epoch 444/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 6.5324e-04 - accuracy: 0.9998 - val_loss: 0.0404 - val_accuracy: 0.9889\n",
            "\n",
            "Epoch 00444: val_accuracy did not improve from 0.99089\n",
            "Epoch 445/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 2.9320e-04 - accuracy: 1.0000 - val_loss: 0.0382 - val_accuracy: 0.9876\n",
            "\n",
            "Epoch 00445: val_accuracy did not improve from 0.99089\n",
            "Epoch 446/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 5.3902e-04 - accuracy: 0.9998 - val_loss: 0.0411 - val_accuracy: 0.9876\n",
            "\n",
            "Epoch 00446: val_accuracy did not improve from 0.99089\n",
            "Epoch 447/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 3.3712e-04 - accuracy: 1.0000 - val_loss: 0.0407 - val_accuracy: 0.9876\n",
            "\n",
            "Epoch 00447: val_accuracy did not improve from 0.99089\n",
            "Epoch 448/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 3.1118e-04 - accuracy: 1.0000 - val_loss: 0.0410 - val_accuracy: 0.9889\n",
            "\n",
            "Epoch 00448: val_accuracy did not improve from 0.99089\n",
            "Epoch 449/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 2.5968e-04 - accuracy: 1.0000 - val_loss: 0.0375 - val_accuracy: 0.9896\n",
            "\n",
            "Epoch 00449: val_accuracy did not improve from 0.99089\n",
            "Epoch 450/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 6.8713e-04 - accuracy: 0.9997 - val_loss: 0.0411 - val_accuracy: 0.9889\n",
            "\n",
            "Epoch 00450: val_accuracy did not improve from 0.99089\n",
            "Epoch 451/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0048 - accuracy: 0.9984 - val_loss: 0.0610 - val_accuracy: 0.9850\n",
            "\n",
            "Epoch 00451: val_accuracy did not improve from 0.99089\n",
            "Epoch 452/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 6.6837e-04 - accuracy: 0.9998 - val_loss: 0.0399 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00452: val_accuracy did not improve from 0.99089\n",
            "Epoch 453/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 5.4489e-04 - accuracy: 0.9998 - val_loss: 0.0436 - val_accuracy: 0.9870\n",
            "\n",
            "Epoch 00453: val_accuracy did not improve from 0.99089\n",
            "Epoch 454/500\n",
            "192/192 [==============================] - 1s 7ms/step - loss: 0.0016 - accuracy: 0.9993 - val_loss: 0.0380 - val_accuracy: 0.9896\n",
            "\n",
            "Epoch 00454: val_accuracy did not improve from 0.99089\n",
            "Epoch 455/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 3.4181e-04 - accuracy: 1.0000 - val_loss: 0.0428 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00455: val_accuracy did not improve from 0.99089\n",
            "Epoch 456/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 8.0922e-04 - accuracy: 1.0000 - val_loss: 0.0379 - val_accuracy: 0.9902\n",
            "\n",
            "Epoch 00456: val_accuracy did not improve from 0.99089\n",
            "Epoch 457/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 3.1425e-04 - accuracy: 1.0000 - val_loss: 0.0382 - val_accuracy: 0.9889\n",
            "\n",
            "Epoch 00457: val_accuracy did not improve from 0.99089\n",
            "Epoch 458/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 7.5437e-04 - accuracy: 0.9998 - val_loss: 0.0424 - val_accuracy: 0.9870\n",
            "\n",
            "Epoch 00458: val_accuracy did not improve from 0.99089\n",
            "Epoch 459/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.0535 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00459: val_accuracy did not improve from 0.99089\n",
            "Epoch 460/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 4.6596e-04 - accuracy: 1.0000 - val_loss: 0.0429 - val_accuracy: 0.9896\n",
            "\n",
            "Epoch 00460: val_accuracy did not improve from 0.99089\n",
            "Epoch 461/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.0388 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00461: val_accuracy did not improve from 0.99089\n",
            "Epoch 462/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.0362 - val_accuracy: 0.9902\n",
            "\n",
            "Epoch 00462: val_accuracy did not improve from 0.99089\n",
            "Epoch 463/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 3.0426e-04 - accuracy: 1.0000 - val_loss: 0.0353 - val_accuracy: 0.9902\n",
            "\n",
            "Epoch 00463: val_accuracy did not improve from 0.99089\n",
            "Epoch 464/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 1.9517e-04 - accuracy: 1.0000 - val_loss: 0.0365 - val_accuracy: 0.9889\n",
            "\n",
            "Epoch 00464: val_accuracy did not improve from 0.99089\n",
            "Epoch 465/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 8.1574e-04 - accuracy: 0.9997 - val_loss: 0.0661 - val_accuracy: 0.9798\n",
            "\n",
            "Epoch 00465: val_accuracy did not improve from 0.99089\n",
            "Epoch 466/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 9.4031e-04 - accuracy: 0.9997 - val_loss: 0.0372 - val_accuracy: 0.9902\n",
            "\n",
            "Epoch 00466: val_accuracy did not improve from 0.99089\n",
            "Epoch 467/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 3.4326e-04 - accuracy: 1.0000 - val_loss: 0.0348 - val_accuracy: 0.9909\n",
            "\n",
            "Epoch 00467: val_accuracy did not improve from 0.99089\n",
            "Epoch 468/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 4.3814e-04 - accuracy: 1.0000 - val_loss: 0.0371 - val_accuracy: 0.9902\n",
            "\n",
            "Epoch 00468: val_accuracy did not improve from 0.99089\n",
            "Epoch 469/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 2.2504e-04 - accuracy: 1.0000 - val_loss: 0.0362 - val_accuracy: 0.9896\n",
            "\n",
            "Epoch 00469: val_accuracy did not improve from 0.99089\n",
            "Epoch 470/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 3.5012e-04 - accuracy: 0.9998 - val_loss: 0.0361 - val_accuracy: 0.9896\n",
            "\n",
            "Epoch 00470: val_accuracy did not improve from 0.99089\n",
            "Epoch 471/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 2.3142e-04 - accuracy: 1.0000 - val_loss: 0.0373 - val_accuracy: 0.9902\n",
            "\n",
            "Epoch 00471: val_accuracy did not improve from 0.99089\n",
            "Epoch 472/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 1.7316e-04 - accuracy: 1.0000 - val_loss: 0.0376 - val_accuracy: 0.9902\n",
            "\n",
            "Epoch 00472: val_accuracy did not improve from 0.99089\n",
            "Epoch 473/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 1.4127e-04 - accuracy: 1.0000 - val_loss: 0.0391 - val_accuracy: 0.9896\n",
            "\n",
            "Epoch 00473: val_accuracy did not improve from 0.99089\n",
            "Epoch 474/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 1.9326e-04 - accuracy: 1.0000 - val_loss: 0.0380 - val_accuracy: 0.9896\n",
            "\n",
            "Epoch 00474: val_accuracy did not improve from 0.99089\n",
            "Epoch 475/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 2.1694e-04 - accuracy: 1.0000 - val_loss: 0.0364 - val_accuracy: 0.9896\n",
            "\n",
            "Epoch 00475: val_accuracy did not improve from 0.99089\n",
            "Epoch 476/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 1.9546e-04 - accuracy: 1.0000 - val_loss: 0.0380 - val_accuracy: 0.9889\n",
            "\n",
            "Epoch 00476: val_accuracy did not improve from 0.99089\n",
            "Epoch 477/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 2.3091e-04 - accuracy: 1.0000 - val_loss: 0.0388 - val_accuracy: 0.9896\n",
            "\n",
            "Epoch 00477: val_accuracy did not improve from 0.99089\n",
            "Epoch 478/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 3.8719e-04 - accuracy: 0.9998 - val_loss: 0.0406 - val_accuracy: 0.9909\n",
            "\n",
            "Epoch 00478: val_accuracy did not improve from 0.99089\n",
            "Epoch 479/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 1.8877e-04 - accuracy: 1.0000 - val_loss: 0.0395 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00479: val_accuracy did not improve from 0.99089\n",
            "Epoch 480/500\n",
            "192/192 [==============================] - 1s 7ms/step - loss: 2.3551e-04 - accuracy: 1.0000 - val_loss: 0.0379 - val_accuracy: 0.9896\n",
            "\n",
            "Epoch 00480: val_accuracy did not improve from 0.99089\n",
            "Epoch 481/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 3.8748e-04 - accuracy: 0.9998 - val_loss: 0.0394 - val_accuracy: 0.9889\n",
            "\n",
            "Epoch 00481: val_accuracy did not improve from 0.99089\n",
            "Epoch 482/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 2.3398e-04 - accuracy: 1.0000 - val_loss: 0.0400 - val_accuracy: 0.9889\n",
            "\n",
            "Epoch 00482: val_accuracy did not improve from 0.99089\n",
            "Epoch 483/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 4.8123e-04 - accuracy: 0.9997 - val_loss: 0.0370 - val_accuracy: 0.9902\n",
            "\n",
            "Epoch 00483: val_accuracy did not improve from 0.99089\n",
            "Epoch 484/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 2.4334e-04 - accuracy: 1.0000 - val_loss: 0.0375 - val_accuracy: 0.9902\n",
            "\n",
            "Epoch 00484: val_accuracy did not improve from 0.99089\n",
            "Epoch 485/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.0507 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00485: val_accuracy did not improve from 0.99089\n",
            "Epoch 486/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 3.4014e-04 - accuracy: 0.9998 - val_loss: 0.0375 - val_accuracy: 0.9889\n",
            "\n",
            "Epoch 00486: val_accuracy did not improve from 0.99089\n",
            "Epoch 487/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 0.0013 - accuracy: 0.9995 - val_loss: 0.0386 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00487: val_accuracy did not improve from 0.99089\n",
            "Epoch 488/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 8.2191e-04 - accuracy: 0.9997 - val_loss: 0.0400 - val_accuracy: 0.9876\n",
            "\n",
            "Epoch 00488: val_accuracy did not improve from 0.99089\n",
            "Epoch 489/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 1.9098e-04 - accuracy: 1.0000 - val_loss: 0.0375 - val_accuracy: 0.9902\n",
            "\n",
            "Epoch 00489: val_accuracy did not improve from 0.99089\n",
            "Epoch 490/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 1.4232e-04 - accuracy: 1.0000 - val_loss: 0.0377 - val_accuracy: 0.9902\n",
            "\n",
            "Epoch 00490: val_accuracy did not improve from 0.99089\n",
            "Epoch 491/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 2.0193e-04 - accuracy: 1.0000 - val_loss: 0.0396 - val_accuracy: 0.9889\n",
            "\n",
            "Epoch 00491: val_accuracy did not improve from 0.99089\n",
            "Epoch 492/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 2.5764e-04 - accuracy: 1.0000 - val_loss: 0.0380 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00492: val_accuracy did not improve from 0.99089\n",
            "Epoch 493/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 1.5138e-04 - accuracy: 1.0000 - val_loss: 0.0354 - val_accuracy: 0.9902\n",
            "\n",
            "Epoch 00493: val_accuracy did not improve from 0.99089\n",
            "Epoch 494/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 8.3123e-04 - accuracy: 0.9997 - val_loss: 0.0384 - val_accuracy: 0.9896\n",
            "\n",
            "Epoch 00494: val_accuracy did not improve from 0.99089\n",
            "Epoch 495/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 1.7219e-04 - accuracy: 1.0000 - val_loss: 0.0375 - val_accuracy: 0.9902\n",
            "\n",
            "Epoch 00495: val_accuracy did not improve from 0.99089\n",
            "Epoch 496/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 1.5447e-04 - accuracy: 1.0000 - val_loss: 0.0376 - val_accuracy: 0.9889\n",
            "\n",
            "Epoch 00496: val_accuracy did not improve from 0.99089\n",
            "Epoch 497/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 1.9936e-04 - accuracy: 1.0000 - val_loss: 0.0372 - val_accuracy: 0.9896\n",
            "\n",
            "Epoch 00497: val_accuracy did not improve from 0.99089\n",
            "Epoch 498/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 1.3781e-04 - accuracy: 1.0000 - val_loss: 0.0373 - val_accuracy: 0.9896\n",
            "\n",
            "Epoch 00498: val_accuracy did not improve from 0.99089\n",
            "Epoch 499/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 1.3462e-04 - accuracy: 1.0000 - val_loss: 0.0374 - val_accuracy: 0.9909\n",
            "\n",
            "Epoch 00499: val_accuracy did not improve from 0.99089\n",
            "Epoch 500/500\n",
            "192/192 [==============================] - 1s 6ms/step - loss: 1.9588e-04 - accuracy: 1.0000 - val_loss: 0.0422 - val_accuracy: 0.9889\n",
            "\n",
            "Epoch 00500: val_accuracy did not improve from 0.99089\n",
            "Training/validation time was: 925.683 seconds\n",
            "\n",
            " \n",
            "EVALUATING THE MODEL:\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0415 - accuracy: 0.9875\n",
            "Evaluate time was 0.201447 seconds\n",
            "0.987500011920929\n",
            "\n",
            "SUMMARY: \n",
            "With 600 cases, the max val_accuracy was: 99.0885 % and the test_accuracy was: 98.7500 %. \n",
            "It is a difference of 0.3385 %.\n",
            "Fitness history is: [1] \n",
            "Cases history is: [600] \n",
            "acc_val history is: [0.9908854365348816] \n",
            "acc_test history is: [0.987500011920929] \n",
            "Times history is: [925.6832382678986]\n",
            "\n",
            "---> ITERATING NOW WITH: 550 CASES ! ( 91.67 % FROM TOTAL ) \n",
            "----> AND THE HISTORY OF CASES DECREASE IS: [600] \n",
            " \n",
            "TRAINING AND VALIDATING THE MODEL: \n",
            "It will take a while ;D...\n",
            "Epoch 1/500\n",
            "176/176 [==============================] - 2s 7ms/step - loss: 1.1083 - accuracy: 0.5645 - val_loss: 3.0888 - val_accuracy: 0.1214\n",
            "\n",
            "Epoch 00001: val_accuracy did not improve from 0.99089\n",
            "Epoch 2/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.8654 - accuracy: 0.6312 - val_loss: 3.1507 - val_accuracy: 0.1634\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.99089\n",
            "Epoch 3/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.7583 - accuracy: 0.6749 - val_loss: 1.7562 - val_accuracy: 0.3615\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.99089\n",
            "Epoch 4/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.6395 - accuracy: 0.7353 - val_loss: 13.9596 - val_accuracy: 0.0632\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.99089\n",
            "Epoch 5/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.5661 - accuracy: 0.7678 - val_loss: 1.2389 - val_accuracy: 0.5263\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.99089\n",
            "Epoch 6/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.4287 - accuracy: 0.8322 - val_loss: 0.8612 - val_accuracy: 0.6257\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.99089\n",
            "Epoch 7/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.6383 - accuracy: 0.7402 - val_loss: 2.0278 - val_accuracy: 0.3111\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.99089\n",
            "Epoch 8/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.7418 - accuracy: 0.6864 - val_loss: 4.6701 - val_accuracy: 0.1477\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.99089\n",
            "Epoch 9/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.8138 - accuracy: 0.6523 - val_loss: 0.8500 - val_accuracy: 0.6754\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.99089\n",
            "Epoch 10/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.6530 - accuracy: 0.7234 - val_loss: 1.0775 - val_accuracy: 0.5249\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.99089\n",
            "Epoch 11/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.3756 - accuracy: 0.8544 - val_loss: 0.7783 - val_accuracy: 0.6790\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.99089\n",
            "Epoch 12/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.3876 - accuracy: 0.8473 - val_loss: 3.9642 - val_accuracy: 0.2727\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.99089\n",
            "Epoch 13/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.5114 - accuracy: 0.7937 - val_loss: 0.9779 - val_accuracy: 0.5866\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.99089\n",
            "Epoch 14/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.4568 - accuracy: 0.8127 - val_loss: 2.4732 - val_accuracy: 0.2862\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.99089\n",
            "Epoch 15/500\n",
            "176/176 [==============================] - 1s 7ms/step - loss: 0.5091 - accuracy: 0.7919 - val_loss: 0.7360 - val_accuracy: 0.7017\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.99089\n",
            "Epoch 16/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.3530 - accuracy: 0.8539 - val_loss: 0.5147 - val_accuracy: 0.8047\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.99089\n",
            "Epoch 17/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.3193 - accuracy: 0.8674 - val_loss: 0.4789 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.99089\n",
            "Epoch 18/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.3444 - accuracy: 0.8610 - val_loss: 0.3442 - val_accuracy: 0.8601\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.99089\n",
            "Epoch 19/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.2978 - accuracy: 0.8771 - val_loss: 0.3429 - val_accuracy: 0.8736\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.99089\n",
            "Epoch 20/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.2953 - accuracy: 0.8746 - val_loss: 0.3316 - val_accuracy: 0.8509\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.99089\n",
            "Epoch 21/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.2808 - accuracy: 0.8835 - val_loss: 0.2880 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.99089\n",
            "Epoch 22/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.2858 - accuracy: 0.8762 - val_loss: 0.3082 - val_accuracy: 0.8643\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.99089\n",
            "Epoch 23/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.2748 - accuracy: 0.8782 - val_loss: 0.4078 - val_accuracy: 0.8281\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.99089\n",
            "Epoch 24/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.2595 - accuracy: 0.8910 - val_loss: 0.3155 - val_accuracy: 0.8636\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.99089\n",
            "Epoch 25/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.2721 - accuracy: 0.8807 - val_loss: 0.3431 - val_accuracy: 0.8530\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.99089\n",
            "Epoch 26/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.2596 - accuracy: 0.8894 - val_loss: 0.2761 - val_accuracy: 0.8871\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.99089\n",
            "Epoch 27/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.2645 - accuracy: 0.8920 - val_loss: 0.3020 - val_accuracy: 0.8587\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.99089\n",
            "Epoch 28/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.2620 - accuracy: 0.8919 - val_loss: 0.3035 - val_accuracy: 0.8885\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.99089\n",
            "Epoch 29/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.2517 - accuracy: 0.8920 - val_loss: 0.5015 - val_accuracy: 0.8004\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.99089\n",
            "Epoch 30/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.2519 - accuracy: 0.8929 - val_loss: 0.3933 - val_accuracy: 0.8473\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.99089\n",
            "Epoch 31/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.2517 - accuracy: 0.8920 - val_loss: 0.2880 - val_accuracy: 0.8913\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.99089\n",
            "Epoch 32/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.2322 - accuracy: 0.9022 - val_loss: 0.5179 - val_accuracy: 0.7955\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.99089\n",
            "Epoch 33/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.2464 - accuracy: 0.8975 - val_loss: 0.2325 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.99089\n",
            "Epoch 34/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.2207 - accuracy: 0.9071 - val_loss: 0.3448 - val_accuracy: 0.8849\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.99089\n",
            "Epoch 35/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.2258 - accuracy: 0.9047 - val_loss: 0.2784 - val_accuracy: 0.8899\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.99089\n",
            "Epoch 36/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.2039 - accuracy: 0.9165 - val_loss: 0.2505 - val_accuracy: 0.8942\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.99089\n",
            "Epoch 37/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.2122 - accuracy: 0.9134 - val_loss: 0.2732 - val_accuracy: 0.8828\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.99089\n",
            "Epoch 38/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.2054 - accuracy: 0.9125 - val_loss: 0.2629 - val_accuracy: 0.9013\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.99089\n",
            "Epoch 39/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.2024 - accuracy: 0.9178 - val_loss: 0.3582 - val_accuracy: 0.8580\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.99089\n",
            "Epoch 40/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.2029 - accuracy: 0.9157 - val_loss: 0.2341 - val_accuracy: 0.9048\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.99089\n",
            "Epoch 41/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1886 - accuracy: 0.9242 - val_loss: 0.4858 - val_accuracy: 0.8033\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.99089\n",
            "Epoch 42/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1815 - accuracy: 0.9240 - val_loss: 0.2517 - val_accuracy: 0.8935\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.99089\n",
            "Epoch 43/500\n",
            "176/176 [==============================] - 1s 7ms/step - loss: 0.1785 - accuracy: 0.9268 - val_loss: 0.3335 - val_accuracy: 0.8580\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.99089\n",
            "Epoch 44/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1901 - accuracy: 0.9217 - val_loss: 0.2207 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.99089\n",
            "Epoch 45/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1771 - accuracy: 0.9274 - val_loss: 0.3014 - val_accuracy: 0.8906\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.99089\n",
            "Epoch 46/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1800 - accuracy: 0.9272 - val_loss: 0.2824 - val_accuracy: 0.9112\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.99089\n",
            "Epoch 47/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1762 - accuracy: 0.9288 - val_loss: 0.2603 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.99089\n",
            "Epoch 48/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1743 - accuracy: 0.9308 - val_loss: 0.4386 - val_accuracy: 0.8416\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.99089\n",
            "Epoch 49/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1763 - accuracy: 0.9274 - val_loss: 0.3772 - val_accuracy: 0.8317\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.99089\n",
            "Epoch 50/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1888 - accuracy: 0.9228 - val_loss: 0.2538 - val_accuracy: 0.8956\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.99089\n",
            "Epoch 51/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1703 - accuracy: 0.9299 - val_loss: 0.2246 - val_accuracy: 0.9048\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.99089\n",
            "Epoch 52/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1683 - accuracy: 0.9325 - val_loss: 0.2586 - val_accuracy: 0.8999\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.99089\n",
            "Epoch 53/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1611 - accuracy: 0.9373 - val_loss: 0.2145 - val_accuracy: 0.9183\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.99089\n",
            "Epoch 54/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1729 - accuracy: 0.9316 - val_loss: 0.2392 - val_accuracy: 0.9048\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.99089\n",
            "Epoch 55/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1571 - accuracy: 0.9398 - val_loss: 0.2255 - val_accuracy: 0.9141\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.99089\n",
            "Epoch 56/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1553 - accuracy: 0.9400 - val_loss: 0.2431 - val_accuracy: 0.8963\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.99089\n",
            "Epoch 57/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1418 - accuracy: 0.9426 - val_loss: 0.6147 - val_accuracy: 0.7692\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.99089\n",
            "Epoch 58/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1514 - accuracy: 0.9402 - val_loss: 0.2628 - val_accuracy: 0.9134\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.99089\n",
            "Epoch 59/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1491 - accuracy: 0.9382 - val_loss: 0.3636 - val_accuracy: 0.8480\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.99089\n",
            "Epoch 60/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1412 - accuracy: 0.9419 - val_loss: 0.6363 - val_accuracy: 0.7464\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.99089\n",
            "Epoch 61/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1356 - accuracy: 0.9450 - val_loss: 0.1621 - val_accuracy: 0.9403\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.99089\n",
            "Epoch 62/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1303 - accuracy: 0.9471 - val_loss: 0.1536 - val_accuracy: 0.9439\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.99089\n",
            "Epoch 63/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1279 - accuracy: 0.9494 - val_loss: 0.1907 - val_accuracy: 0.9197\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.99089\n",
            "Epoch 64/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1311 - accuracy: 0.9489 - val_loss: 0.2986 - val_accuracy: 0.8956\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.99089\n",
            "Epoch 65/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1196 - accuracy: 0.9547 - val_loss: 0.2323 - val_accuracy: 0.9254\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.99089\n",
            "Epoch 66/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1124 - accuracy: 0.9554 - val_loss: 0.5577 - val_accuracy: 0.7955\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.99089\n",
            "Epoch 67/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1116 - accuracy: 0.9561 - val_loss: 0.4054 - val_accuracy: 0.8295\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.99089\n",
            "Epoch 68/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1150 - accuracy: 0.9558 - val_loss: 0.3333 - val_accuracy: 0.8899\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.99089\n",
            "Epoch 69/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1061 - accuracy: 0.9567 - val_loss: 0.4715 - val_accuracy: 0.8303\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.99089\n",
            "Epoch 70/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0992 - accuracy: 0.9620 - val_loss: 0.2768 - val_accuracy: 0.9240\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.99089\n",
            "Epoch 71/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1018 - accuracy: 0.9616 - val_loss: 0.3541 - val_accuracy: 0.8871\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.99089\n",
            "Epoch 72/500\n",
            "176/176 [==============================] - 1s 7ms/step - loss: 0.1002 - accuracy: 0.9613 - val_loss: 0.6520 - val_accuracy: 0.7919\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.99089\n",
            "Epoch 73/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0903 - accuracy: 0.9657 - val_loss: 0.2496 - val_accuracy: 0.8885\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.99089\n",
            "Epoch 74/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0895 - accuracy: 0.9656 - val_loss: 0.1421 - val_accuracy: 0.9503\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.99089\n",
            "Epoch 75/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0846 - accuracy: 0.9709 - val_loss: 0.4065 - val_accuracy: 0.8416\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.99089\n",
            "Epoch 76/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0937 - accuracy: 0.9645 - val_loss: 0.8820 - val_accuracy: 0.7337\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.99089\n",
            "Epoch 77/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0811 - accuracy: 0.9709 - val_loss: 0.4701 - val_accuracy: 0.8651\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.99089\n",
            "Epoch 78/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0794 - accuracy: 0.9696 - val_loss: 0.2667 - val_accuracy: 0.8963\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.99089\n",
            "Epoch 79/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0889 - accuracy: 0.9641 - val_loss: 0.1539 - val_accuracy: 0.9347\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.99089\n",
            "Epoch 80/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0701 - accuracy: 0.9746 - val_loss: 0.3521 - val_accuracy: 0.8764\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.99089\n",
            "Epoch 81/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0714 - accuracy: 0.9730 - val_loss: 0.8456 - val_accuracy: 0.7692\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.99089\n",
            "Epoch 82/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0635 - accuracy: 0.9757 - val_loss: 0.1619 - val_accuracy: 0.9354\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.99089\n",
            "Epoch 83/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0707 - accuracy: 0.9753 - val_loss: 0.4058 - val_accuracy: 0.8658\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.99089\n",
            "Epoch 84/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0725 - accuracy: 0.9741 - val_loss: 0.3049 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.99089\n",
            "Epoch 85/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0829 - accuracy: 0.9721 - val_loss: 0.2823 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.99089\n",
            "Epoch 86/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0751 - accuracy: 0.9709 - val_loss: 0.2082 - val_accuracy: 0.9325\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.99089\n",
            "Epoch 87/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0563 - accuracy: 0.9790 - val_loss: 0.3777 - val_accuracy: 0.8800\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.99089\n",
            "Epoch 88/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0504 - accuracy: 0.9821 - val_loss: 0.1705 - val_accuracy: 0.9254\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.99089\n",
            "Epoch 89/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0542 - accuracy: 0.9803 - val_loss: 0.2368 - val_accuracy: 0.9048\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.99089\n",
            "Epoch 90/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0569 - accuracy: 0.9790 - val_loss: 0.3026 - val_accuracy: 0.8928\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.99089\n",
            "Epoch 91/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0840 - accuracy: 0.9721 - val_loss: 0.4159 - val_accuracy: 0.8594\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.99089\n",
            "Epoch 92/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0589 - accuracy: 0.9778 - val_loss: 0.2712 - val_accuracy: 0.9197\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.99089\n",
            "Epoch 93/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0611 - accuracy: 0.9771 - val_loss: 0.3298 - val_accuracy: 0.8885\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.99089\n",
            "Epoch 94/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0524 - accuracy: 0.9794 - val_loss: 1.0979 - val_accuracy: 0.7479\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.99089\n",
            "Epoch 95/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0414 - accuracy: 0.9854 - val_loss: 0.1277 - val_accuracy: 0.9460\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.99089\n",
            "Epoch 96/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0516 - accuracy: 0.9812 - val_loss: 0.4186 - val_accuracy: 0.9006\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.99089\n",
            "Epoch 97/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0421 - accuracy: 0.9862 - val_loss: 0.1341 - val_accuracy: 0.9510\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.99089\n",
            "Epoch 98/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0392 - accuracy: 0.9863 - val_loss: 0.0987 - val_accuracy: 0.9595\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.99089\n",
            "Epoch 99/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0363 - accuracy: 0.9863 - val_loss: 0.1665 - val_accuracy: 0.9446\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.99089\n",
            "Epoch 100/500\n",
            "176/176 [==============================] - 1s 7ms/step - loss: 0.0388 - accuracy: 0.9856 - val_loss: 0.0917 - val_accuracy: 0.9645\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.99089\n",
            "Epoch 101/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0359 - accuracy: 0.9865 - val_loss: 0.1187 - val_accuracy: 0.9567\n",
            "\n",
            "Epoch 00101: val_accuracy did not improve from 0.99089\n",
            "Epoch 102/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0430 - accuracy: 0.9831 - val_loss: 0.3943 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00102: val_accuracy did not improve from 0.99089\n",
            "Epoch 103/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0584 - accuracy: 0.9773 - val_loss: 0.1300 - val_accuracy: 0.9545\n",
            "\n",
            "Epoch 00103: val_accuracy did not improve from 0.99089\n",
            "Epoch 104/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0414 - accuracy: 0.9842 - val_loss: 0.1923 - val_accuracy: 0.9290\n",
            "\n",
            "Epoch 00104: val_accuracy did not improve from 0.99089\n",
            "Epoch 105/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0444 - accuracy: 0.9831 - val_loss: 0.1296 - val_accuracy: 0.9524\n",
            "\n",
            "Epoch 00105: val_accuracy did not improve from 0.99089\n",
            "Epoch 106/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0474 - accuracy: 0.9801 - val_loss: 0.0850 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00106: val_accuracy did not improve from 0.99089\n",
            "Epoch 107/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0325 - accuracy: 0.9885 - val_loss: 0.4248 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00107: val_accuracy did not improve from 0.99089\n",
            "Epoch 108/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0375 - accuracy: 0.9854 - val_loss: 0.0834 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00108: val_accuracy did not improve from 0.99089\n",
            "Epoch 109/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0336 - accuracy: 0.9876 - val_loss: 0.5037 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00109: val_accuracy did not improve from 0.99089\n",
            "Epoch 110/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0351 - accuracy: 0.9870 - val_loss: 0.3078 - val_accuracy: 0.9297\n",
            "\n",
            "Epoch 00110: val_accuracy did not improve from 0.99089\n",
            "Epoch 111/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0351 - accuracy: 0.9854 - val_loss: 0.1937 - val_accuracy: 0.9339\n",
            "\n",
            "Epoch 00111: val_accuracy did not improve from 0.99089\n",
            "Epoch 112/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0345 - accuracy: 0.9856 - val_loss: 0.2383 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00112: val_accuracy did not improve from 0.99089\n",
            "Epoch 113/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0684 - accuracy: 0.9762 - val_loss: 0.0872 - val_accuracy: 0.9673\n",
            "\n",
            "Epoch 00113: val_accuracy did not improve from 0.99089\n",
            "Epoch 114/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0371 - accuracy: 0.9867 - val_loss: 0.3715 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00114: val_accuracy did not improve from 0.99089\n",
            "Epoch 115/500\n",
            "176/176 [==============================] - 1s 7ms/step - loss: 0.0286 - accuracy: 0.9895 - val_loss: 0.4435 - val_accuracy: 0.8913\n",
            "\n",
            "Epoch 00115: val_accuracy did not improve from 0.99089\n",
            "Epoch 116/500\n",
            "176/176 [==============================] - 1s 7ms/step - loss: 0.0281 - accuracy: 0.9892 - val_loss: 0.2479 - val_accuracy: 0.9403\n",
            "\n",
            "Epoch 00116: val_accuracy did not improve from 0.99089\n",
            "Epoch 117/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0298 - accuracy: 0.9879 - val_loss: 0.1131 - val_accuracy: 0.9524\n",
            "\n",
            "Epoch 00117: val_accuracy did not improve from 0.99089\n",
            "Epoch 118/500\n",
            "176/176 [==============================] - 1s 7ms/step - loss: 0.0271 - accuracy: 0.9892 - val_loss: 0.0887 - val_accuracy: 0.9673\n",
            "\n",
            "Epoch 00118: val_accuracy did not improve from 0.99089\n",
            "Epoch 119/500\n",
            "176/176 [==============================] - 1s 7ms/step - loss: 0.0247 - accuracy: 0.9909 - val_loss: 0.0795 - val_accuracy: 0.9737\n",
            "\n",
            "Epoch 00119: val_accuracy did not improve from 0.99089\n",
            "Epoch 120/500\n",
            "176/176 [==============================] - 1s 7ms/step - loss: 0.0218 - accuracy: 0.9924 - val_loss: 0.2129 - val_accuracy: 0.9290\n",
            "\n",
            "Epoch 00120: val_accuracy did not improve from 0.99089\n",
            "Epoch 121/500\n",
            "176/176 [==============================] - 1s 7ms/step - loss: 0.0236 - accuracy: 0.9904 - val_loss: 0.1727 - val_accuracy: 0.9467\n",
            "\n",
            "Epoch 00121: val_accuracy did not improve from 0.99089\n",
            "Epoch 122/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0236 - accuracy: 0.9899 - val_loss: 0.1090 - val_accuracy: 0.9560\n",
            "\n",
            "Epoch 00122: val_accuracy did not improve from 0.99089\n",
            "Epoch 123/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0250 - accuracy: 0.9915 - val_loss: 0.1638 - val_accuracy: 0.9403\n",
            "\n",
            "Epoch 00123: val_accuracy did not improve from 0.99089\n",
            "Epoch 124/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0225 - accuracy: 0.9915 - val_loss: 0.1176 - val_accuracy: 0.9567\n",
            "\n",
            "Epoch 00124: val_accuracy did not improve from 0.99089\n",
            "Epoch 125/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0368 - accuracy: 0.9856 - val_loss: 0.2962 - val_accuracy: 0.9205\n",
            "\n",
            "Epoch 00125: val_accuracy did not improve from 0.99089\n",
            "Epoch 126/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0300 - accuracy: 0.9877 - val_loss: 0.1367 - val_accuracy: 0.9538\n",
            "\n",
            "Epoch 00126: val_accuracy did not improve from 0.99089\n",
            "Epoch 127/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0317 - accuracy: 0.9881 - val_loss: 0.1183 - val_accuracy: 0.9418\n",
            "\n",
            "Epoch 00127: val_accuracy did not improve from 0.99089\n",
            "Epoch 128/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0453 - accuracy: 0.9835 - val_loss: 0.1509 - val_accuracy: 0.9503\n",
            "\n",
            "Epoch 00128: val_accuracy did not improve from 0.99089\n",
            "Epoch 129/500\n",
            "176/176 [==============================] - 1s 7ms/step - loss: 0.0341 - accuracy: 0.9877 - val_loss: 0.6198 - val_accuracy: 0.8217\n",
            "\n",
            "Epoch 00129: val_accuracy did not improve from 0.99089\n",
            "Epoch 130/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0447 - accuracy: 0.9838 - val_loss: 0.1418 - val_accuracy: 0.9545\n",
            "\n",
            "Epoch 00130: val_accuracy did not improve from 0.99089\n",
            "Epoch 131/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0293 - accuracy: 0.9893 - val_loss: 0.3006 - val_accuracy: 0.9027\n",
            "\n",
            "Epoch 00131: val_accuracy did not improve from 0.99089\n",
            "Epoch 132/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0234 - accuracy: 0.9908 - val_loss: 0.1986 - val_accuracy: 0.9332\n",
            "\n",
            "Epoch 00132: val_accuracy did not improve from 0.99089\n",
            "Epoch 133/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0232 - accuracy: 0.9906 - val_loss: 0.0917 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00133: val_accuracy did not improve from 0.99089\n",
            "Epoch 134/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0212 - accuracy: 0.9913 - val_loss: 0.1004 - val_accuracy: 0.9695\n",
            "\n",
            "Epoch 00134: val_accuracy did not improve from 0.99089\n",
            "Epoch 135/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0209 - accuracy: 0.9918 - val_loss: 0.1705 - val_accuracy: 0.9403\n",
            "\n",
            "Epoch 00135: val_accuracy did not improve from 0.99089\n",
            "Epoch 136/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0212 - accuracy: 0.9922 - val_loss: 0.1392 - val_accuracy: 0.9624\n",
            "\n",
            "Epoch 00136: val_accuracy did not improve from 0.99089\n",
            "Epoch 137/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0223 - accuracy: 0.9920 - val_loss: 0.1038 - val_accuracy: 0.9595\n",
            "\n",
            "Epoch 00137: val_accuracy did not improve from 0.99089\n",
            "Epoch 138/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0206 - accuracy: 0.9927 - val_loss: 0.1176 - val_accuracy: 0.9602\n",
            "\n",
            "Epoch 00138: val_accuracy did not improve from 0.99089\n",
            "Epoch 139/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0231 - accuracy: 0.9902 - val_loss: 0.7604 - val_accuracy: 0.8480\n",
            "\n",
            "Epoch 00139: val_accuracy did not improve from 0.99089\n",
            "Epoch 140/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0218 - accuracy: 0.9917 - val_loss: 0.3649 - val_accuracy: 0.8707\n",
            "\n",
            "Epoch 00140: val_accuracy did not improve from 0.99089\n",
            "Epoch 141/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0234 - accuracy: 0.9899 - val_loss: 0.1164 - val_accuracy: 0.9560\n",
            "\n",
            "Epoch 00141: val_accuracy did not improve from 0.99089\n",
            "Epoch 142/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0192 - accuracy: 0.9929 - val_loss: 0.1004 - val_accuracy: 0.9609\n",
            "\n",
            "Epoch 00142: val_accuracy did not improve from 0.99089\n",
            "Epoch 143/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0232 - accuracy: 0.9927 - val_loss: 1.1870 - val_accuracy: 0.6442\n",
            "\n",
            "Epoch 00143: val_accuracy did not improve from 0.99089\n",
            "Epoch 144/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0411 - accuracy: 0.9858 - val_loss: 0.1201 - val_accuracy: 0.9567\n",
            "\n",
            "Epoch 00144: val_accuracy did not improve from 0.99089\n",
            "Epoch 145/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0448 - accuracy: 0.9838 - val_loss: 0.0800 - val_accuracy: 0.9723\n",
            "\n",
            "Epoch 00145: val_accuracy did not improve from 0.99089\n",
            "Epoch 146/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0203 - accuracy: 0.9929 - val_loss: 0.0969 - val_accuracy: 0.9638\n",
            "\n",
            "Epoch 00146: val_accuracy did not improve from 0.99089\n",
            "Epoch 147/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0165 - accuracy: 0.9943 - val_loss: 0.1974 - val_accuracy: 0.9276\n",
            "\n",
            "Epoch 00147: val_accuracy did not improve from 0.99089\n",
            "Epoch 148/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0155 - accuracy: 0.9940 - val_loss: 0.3106 - val_accuracy: 0.9169\n",
            "\n",
            "Epoch 00148: val_accuracy did not improve from 0.99089\n",
            "Epoch 149/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0146 - accuracy: 0.9954 - val_loss: 0.1042 - val_accuracy: 0.9659\n",
            "\n",
            "Epoch 00149: val_accuracy did not improve from 0.99089\n",
            "Epoch 150/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0119 - accuracy: 0.9961 - val_loss: 0.1297 - val_accuracy: 0.9581\n",
            "\n",
            "Epoch 00150: val_accuracy did not improve from 0.99089\n",
            "Epoch 151/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0178 - accuracy: 0.9934 - val_loss: 0.0886 - val_accuracy: 0.9716\n",
            "\n",
            "Epoch 00151: val_accuracy did not improve from 0.99089\n",
            "Epoch 152/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0187 - accuracy: 0.9927 - val_loss: 0.0702 - val_accuracy: 0.9730\n",
            "\n",
            "Epoch 00152: val_accuracy did not improve from 0.99089\n",
            "Epoch 153/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0137 - accuracy: 0.9957 - val_loss: 0.1287 - val_accuracy: 0.9553\n",
            "\n",
            "Epoch 00153: val_accuracy did not improve from 0.99089\n",
            "Epoch 154/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0134 - accuracy: 0.9954 - val_loss: 0.0732 - val_accuracy: 0.9737\n",
            "\n",
            "Epoch 00154: val_accuracy did not improve from 0.99089\n",
            "Epoch 155/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0124 - accuracy: 0.9956 - val_loss: 0.1307 - val_accuracy: 0.9531\n",
            "\n",
            "Epoch 00155: val_accuracy did not improve from 0.99089\n",
            "Epoch 156/500\n",
            "176/176 [==============================] - 1s 7ms/step - loss: 0.0135 - accuracy: 0.9963 - val_loss: 0.0822 - val_accuracy: 0.9673\n",
            "\n",
            "Epoch 00156: val_accuracy did not improve from 0.99089\n",
            "Epoch 157/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0163 - accuracy: 0.9941 - val_loss: 0.0756 - val_accuracy: 0.9751\n",
            "\n",
            "Epoch 00157: val_accuracy did not improve from 0.99089\n",
            "Epoch 158/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0103 - accuracy: 0.9968 - val_loss: 0.0801 - val_accuracy: 0.9794\n",
            "\n",
            "Epoch 00158: val_accuracy did not improve from 0.99089\n",
            "Epoch 159/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0124 - accuracy: 0.9966 - val_loss: 0.1752 - val_accuracy: 0.9254\n",
            "\n",
            "Epoch 00159: val_accuracy did not improve from 0.99089\n",
            "Epoch 160/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0385 - accuracy: 0.9890 - val_loss: 0.0861 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00160: val_accuracy did not improve from 0.99089\n",
            "Epoch 161/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0200 - accuracy: 0.9936 - val_loss: 0.0678 - val_accuracy: 0.9744\n",
            "\n",
            "Epoch 00161: val_accuracy did not improve from 0.99089\n",
            "Epoch 162/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0107 - accuracy: 0.9968 - val_loss: 0.0636 - val_accuracy: 0.9787\n",
            "\n",
            "Epoch 00162: val_accuracy did not improve from 0.99089\n",
            "Epoch 163/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0305 - accuracy: 0.9892 - val_loss: 1.3341 - val_accuracy: 0.7663\n",
            "\n",
            "Epoch 00163: val_accuracy did not improve from 0.99089\n",
            "Epoch 164/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0852 - accuracy: 0.9700 - val_loss: 0.3773 - val_accuracy: 0.8402\n",
            "\n",
            "Epoch 00164: val_accuracy did not improve from 0.99089\n",
            "Epoch 165/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0254 - accuracy: 0.9917 - val_loss: 0.1267 - val_accuracy: 0.9602\n",
            "\n",
            "Epoch 00165: val_accuracy did not improve from 0.99089\n",
            "Epoch 166/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0173 - accuracy: 0.9933 - val_loss: 0.0815 - val_accuracy: 0.9702\n",
            "\n",
            "Epoch 00166: val_accuracy did not improve from 0.99089\n",
            "Epoch 167/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0106 - accuracy: 0.9970 - val_loss: 0.2676 - val_accuracy: 0.9197\n",
            "\n",
            "Epoch 00167: val_accuracy did not improve from 0.99089\n",
            "Epoch 168/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0073 - accuracy: 0.9988 - val_loss: 0.0590 - val_accuracy: 0.9787\n",
            "\n",
            "Epoch 00168: val_accuracy did not improve from 0.99089\n",
            "Epoch 169/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0078 - accuracy: 0.9977 - val_loss: 0.1186 - val_accuracy: 0.9609\n",
            "\n",
            "Epoch 00169: val_accuracy did not improve from 0.99089\n",
            "Epoch 170/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0140 - accuracy: 0.9952 - val_loss: 0.2082 - val_accuracy: 0.9446\n",
            "\n",
            "Epoch 00170: val_accuracy did not improve from 0.99089\n",
            "Epoch 171/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0154 - accuracy: 0.9954 - val_loss: 0.1269 - val_accuracy: 0.9553\n",
            "\n",
            "Epoch 00171: val_accuracy did not improve from 0.99089\n",
            "Epoch 172/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0126 - accuracy: 0.9966 - val_loss: 0.1643 - val_accuracy: 0.9467\n",
            "\n",
            "Epoch 00172: val_accuracy did not improve from 0.99089\n",
            "Epoch 173/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0092 - accuracy: 0.9975 - val_loss: 0.1258 - val_accuracy: 0.9560\n",
            "\n",
            "Epoch 00173: val_accuracy did not improve from 0.99089\n",
            "Epoch 174/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0093 - accuracy: 0.9979 - val_loss: 0.1832 - val_accuracy: 0.9361\n",
            "\n",
            "Epoch 00174: val_accuracy did not improve from 0.99089\n",
            "Epoch 175/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0113 - accuracy: 0.9968 - val_loss: 0.0661 - val_accuracy: 0.9801\n",
            "\n",
            "Epoch 00175: val_accuracy did not improve from 0.99089\n",
            "Epoch 176/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0097 - accuracy: 0.9968 - val_loss: 0.2259 - val_accuracy: 0.9368\n",
            "\n",
            "Epoch 00176: val_accuracy did not improve from 0.99089\n",
            "Epoch 177/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0073 - accuracy: 0.9984 - val_loss: 0.1325 - val_accuracy: 0.9588\n",
            "\n",
            "Epoch 00177: val_accuracy did not improve from 0.99089\n",
            "Epoch 178/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0056 - accuracy: 0.9991 - val_loss: 0.1120 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00178: val_accuracy did not improve from 0.99089\n",
            "Epoch 179/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0079 - accuracy: 0.9968 - val_loss: 0.1439 - val_accuracy: 0.9375\n",
            "\n",
            "Epoch 00179: val_accuracy did not improve from 0.99089\n",
            "Epoch 180/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0045 - accuracy: 0.9995 - val_loss: 0.0963 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00180: val_accuracy did not improve from 0.99089\n",
            "Epoch 181/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0078 - accuracy: 0.9980 - val_loss: 0.0633 - val_accuracy: 0.9787\n",
            "\n",
            "Epoch 00181: val_accuracy did not improve from 0.99089\n",
            "Epoch 182/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0090 - accuracy: 0.9970 - val_loss: 0.0639 - val_accuracy: 0.9787\n",
            "\n",
            "Epoch 00182: val_accuracy did not improve from 0.99089\n",
            "Epoch 183/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0039 - accuracy: 0.9989 - val_loss: 1.2479 - val_accuracy: 0.7848\n",
            "\n",
            "Epoch 00183: val_accuracy did not improve from 0.99089\n",
            "Epoch 184/500\n",
            "176/176 [==============================] - 1s 7ms/step - loss: 0.0066 - accuracy: 0.9979 - val_loss: 0.0771 - val_accuracy: 0.9702\n",
            "\n",
            "Epoch 00184: val_accuracy did not improve from 0.99089\n",
            "Epoch 185/500\n",
            "176/176 [==============================] - 1s 8ms/step - loss: 0.0037 - accuracy: 0.9995 - val_loss: 0.0680 - val_accuracy: 0.9730\n",
            "\n",
            "Epoch 00185: val_accuracy did not improve from 0.99089\n",
            "Epoch 186/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0062 - accuracy: 0.9973 - val_loss: 0.0737 - val_accuracy: 0.9787\n",
            "\n",
            "Epoch 00186: val_accuracy did not improve from 0.99089\n",
            "Epoch 187/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0046 - accuracy: 0.9988 - val_loss: 0.0581 - val_accuracy: 0.9815\n",
            "\n",
            "Epoch 00187: val_accuracy did not improve from 0.99089\n",
            "Epoch 188/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.0784 - val_accuracy: 0.9794\n",
            "\n",
            "Epoch 00188: val_accuracy did not improve from 0.99089\n",
            "Epoch 189/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0040 - accuracy: 0.9993 - val_loss: 0.3584 - val_accuracy: 0.9446\n",
            "\n",
            "Epoch 00189: val_accuracy did not improve from 0.99089\n",
            "Epoch 190/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0156 - accuracy: 0.9952 - val_loss: 0.0749 - val_accuracy: 0.9744\n",
            "\n",
            "Epoch 00190: val_accuracy did not improve from 0.99089\n",
            "Epoch 191/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0039 - accuracy: 0.9993 - val_loss: 0.0517 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00191: val_accuracy did not improve from 0.99089\n",
            "Epoch 192/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0041 - accuracy: 0.9991 - val_loss: 0.1781 - val_accuracy: 0.9538\n",
            "\n",
            "Epoch 00192: val_accuracy did not improve from 0.99089\n",
            "Epoch 193/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0058 - accuracy: 0.9988 - val_loss: 0.1663 - val_accuracy: 0.9574\n",
            "\n",
            "Epoch 00193: val_accuracy did not improve from 0.99089\n",
            "Epoch 194/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0081 - accuracy: 0.9973 - val_loss: 0.0784 - val_accuracy: 0.9773\n",
            "\n",
            "Epoch 00194: val_accuracy did not improve from 0.99089\n",
            "Epoch 195/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0080 - accuracy: 0.9979 - val_loss: 0.1142 - val_accuracy: 0.9538\n",
            "\n",
            "Epoch 00195: val_accuracy did not improve from 0.99089\n",
            "Epoch 196/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0056 - accuracy: 0.9984 - val_loss: 0.0700 - val_accuracy: 0.9730\n",
            "\n",
            "Epoch 00196: val_accuracy did not improve from 0.99089\n",
            "Epoch 197/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.1033 - val_accuracy: 0.9638\n",
            "\n",
            "Epoch 00197: val_accuracy did not improve from 0.99089\n",
            "Epoch 198/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0032 - accuracy: 0.9996 - val_loss: 0.1752 - val_accuracy: 0.9467\n",
            "\n",
            "Epoch 00198: val_accuracy did not improve from 0.99089\n",
            "Epoch 199/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0067 - accuracy: 0.9984 - val_loss: 0.0533 - val_accuracy: 0.9822\n",
            "\n",
            "Epoch 00199: val_accuracy did not improve from 0.99089\n",
            "Epoch 200/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0043 - accuracy: 0.9984 - val_loss: 0.0657 - val_accuracy: 0.9766\n",
            "\n",
            "Epoch 00200: val_accuracy did not improve from 0.99089\n",
            "Epoch 201/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0087 - accuracy: 0.9975 - val_loss: 0.0635 - val_accuracy: 0.9808\n",
            "\n",
            "Epoch 00201: val_accuracy did not improve from 0.99089\n",
            "Epoch 202/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0030 - accuracy: 0.9996 - val_loss: 0.0538 - val_accuracy: 0.9808\n",
            "\n",
            "Epoch 00202: val_accuracy did not improve from 0.99089\n",
            "Epoch 203/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0607 - val_accuracy: 0.9801\n",
            "\n",
            "Epoch 00203: val_accuracy did not improve from 0.99089\n",
            "Epoch 204/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0748 - val_accuracy: 0.9737\n",
            "\n",
            "Epoch 00204: val_accuracy did not improve from 0.99089\n",
            "Epoch 205/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 3.3174 - val_accuracy: 0.6982\n",
            "\n",
            "Epoch 00205: val_accuracy did not improve from 0.99089\n",
            "Epoch 206/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0562 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00206: val_accuracy did not improve from 0.99089\n",
            "Epoch 207/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0179 - accuracy: 0.9961 - val_loss: 0.4345 - val_accuracy: 0.9304\n",
            "\n",
            "Epoch 00207: val_accuracy did not improve from 0.99089\n",
            "Epoch 208/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0069 - accuracy: 0.9980 - val_loss: 0.0519 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00208: val_accuracy did not improve from 0.99089\n",
            "Epoch 209/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0023 - accuracy: 0.9998 - val_loss: 0.0501 - val_accuracy: 0.9815\n",
            "\n",
            "Epoch 00209: val_accuracy did not improve from 0.99089\n",
            "Epoch 210/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.0519 - val_accuracy: 0.9822\n",
            "\n",
            "Epoch 00210: val_accuracy did not improve from 0.99089\n",
            "Epoch 211/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0548 - val_accuracy: 0.9794\n",
            "\n",
            "Epoch 00211: val_accuracy did not improve from 0.99089\n",
            "Epoch 212/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0935 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00212: val_accuracy did not improve from 0.99089\n",
            "Epoch 213/500\n",
            "176/176 [==============================] - 1s 7ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0548 - val_accuracy: 0.9801\n",
            "\n",
            "Epoch 00213: val_accuracy did not improve from 0.99089\n",
            "Epoch 214/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0022 - accuracy: 0.9993 - val_loss: 0.0617 - val_accuracy: 0.9780\n",
            "\n",
            "Epoch 00214: val_accuracy did not improve from 0.99089\n",
            "Epoch 215/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0955 - val_accuracy: 0.9631\n",
            "\n",
            "Epoch 00215: val_accuracy did not improve from 0.99089\n",
            "Epoch 216/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0492 - val_accuracy: 0.9822\n",
            "\n",
            "Epoch 00216: val_accuracy did not improve from 0.99089\n",
            "Epoch 217/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0467 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00217: val_accuracy did not improve from 0.99089\n",
            "Epoch 218/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0140 - accuracy: 0.9956 - val_loss: 0.0641 - val_accuracy: 0.9737\n",
            "\n",
            "Epoch 00218: val_accuracy did not improve from 0.99089\n",
            "Epoch 219/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0222 - accuracy: 0.9941 - val_loss: 0.1675 - val_accuracy: 0.9418\n",
            "\n",
            "Epoch 00219: val_accuracy did not improve from 0.99089\n",
            "Epoch 220/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0075 - accuracy: 0.9988 - val_loss: 1.0852 - val_accuracy: 0.7983\n",
            "\n",
            "Epoch 00220: val_accuracy did not improve from 0.99089\n",
            "Epoch 221/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0072 - accuracy: 0.9980 - val_loss: 0.2662 - val_accuracy: 0.9474\n",
            "\n",
            "Epoch 00221: val_accuracy did not improve from 0.99089\n",
            "Epoch 222/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0125 - accuracy: 0.9961 - val_loss: 0.0624 - val_accuracy: 0.9787\n",
            "\n",
            "Epoch 00222: val_accuracy did not improve from 0.99089\n",
            "Epoch 223/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.1078 - val_accuracy: 0.9602\n",
            "\n",
            "Epoch 00223: val_accuracy did not improve from 0.99089\n",
            "Epoch 224/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.0627 - val_accuracy: 0.9801\n",
            "\n",
            "Epoch 00224: val_accuracy did not improve from 0.99089\n",
            "Epoch 225/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0558 - val_accuracy: 0.9794\n",
            "\n",
            "Epoch 00225: val_accuracy did not improve from 0.99089\n",
            "Epoch 226/500\n",
            "176/176 [==============================] - 1s 7ms/step - loss: 0.0050 - accuracy: 0.9984 - val_loss: 0.0742 - val_accuracy: 0.9751\n",
            "\n",
            "Epoch 00226: val_accuracy did not improve from 0.99089\n",
            "Epoch 227/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.0505 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00227: val_accuracy did not improve from 0.99089\n",
            "Epoch 228/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0548 - val_accuracy: 0.9808\n",
            "\n",
            "Epoch 00228: val_accuracy did not improve from 0.99089\n",
            "Epoch 229/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0119 - accuracy: 0.9964 - val_loss: 0.0933 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00229: val_accuracy did not improve from 0.99089\n",
            "Epoch 230/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.0604 - val_accuracy: 0.9780\n",
            "\n",
            "Epoch 00230: val_accuracy did not improve from 0.99089\n",
            "Epoch 231/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.0659 - val_accuracy: 0.9780\n",
            "\n",
            "Epoch 00231: val_accuracy did not improve from 0.99089\n",
            "Epoch 232/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.0613 - val_accuracy: 0.9766\n",
            "\n",
            "Epoch 00232: val_accuracy did not improve from 0.99089\n",
            "Epoch 233/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.0589 - val_accuracy: 0.9766\n",
            "\n",
            "Epoch 00233: val_accuracy did not improve from 0.99089\n",
            "Epoch 234/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0630 - val_accuracy: 0.9794\n",
            "\n",
            "Epoch 00234: val_accuracy did not improve from 0.99089\n",
            "Epoch 235/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 0.3221 - val_accuracy: 0.8999\n",
            "\n",
            "Epoch 00235: val_accuracy did not improve from 0.99089\n",
            "Epoch 236/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.0523 - val_accuracy: 0.9808\n",
            "\n",
            "Epoch 00236: val_accuracy did not improve from 0.99089\n",
            "Epoch 237/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0941 - accuracy: 0.9760 - val_loss: 1.8767 - val_accuracy: 0.7429\n",
            "\n",
            "Epoch 00237: val_accuracy did not improve from 0.99089\n",
            "Epoch 238/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0528 - accuracy: 0.9847 - val_loss: 0.3724 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00238: val_accuracy did not improve from 0.99089\n",
            "Epoch 239/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0074 - accuracy: 0.9986 - val_loss: 0.0694 - val_accuracy: 0.9773\n",
            "\n",
            "Epoch 00239: val_accuracy did not improve from 0.99089\n",
            "Epoch 240/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0056 - accuracy: 0.9988 - val_loss: 0.0744 - val_accuracy: 0.9773\n",
            "\n",
            "Epoch 00240: val_accuracy did not improve from 0.99089\n",
            "Epoch 241/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0031 - accuracy: 0.9996 - val_loss: 0.0638 - val_accuracy: 0.9780\n",
            "\n",
            "Epoch 00241: val_accuracy did not improve from 0.99089\n",
            "Epoch 242/500\n",
            "176/176 [==============================] - 1s 7ms/step - loss: 0.0051 - accuracy: 0.9986 - val_loss: 0.0528 - val_accuracy: 0.9815\n",
            "\n",
            "Epoch 00242: val_accuracy did not improve from 0.99089\n",
            "Epoch 243/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.0549 - val_accuracy: 0.9808\n",
            "\n",
            "Epoch 00243: val_accuracy did not improve from 0.99089\n",
            "Epoch 244/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0830 - val_accuracy: 0.9716\n",
            "\n",
            "Epoch 00244: val_accuracy did not improve from 0.99089\n",
            "Epoch 245/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1127 - val_accuracy: 0.9645\n",
            "\n",
            "Epoch 00245: val_accuracy did not improve from 0.99089\n",
            "Epoch 246/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0608 - val_accuracy: 0.9815\n",
            "\n",
            "Epoch 00246: val_accuracy did not improve from 0.99089\n",
            "Epoch 247/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0284 - accuracy: 0.9906 - val_loss: 0.2337 - val_accuracy: 0.9197\n",
            "\n",
            "Epoch 00247: val_accuracy did not improve from 0.99089\n",
            "Epoch 248/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0034 - accuracy: 0.9996 - val_loss: 0.0993 - val_accuracy: 0.9609\n",
            "\n",
            "Epoch 00248: val_accuracy did not improve from 0.99089\n",
            "Epoch 249/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0026 - accuracy: 0.9998 - val_loss: 0.0591 - val_accuracy: 0.9808\n",
            "\n",
            "Epoch 00249: val_accuracy did not improve from 0.99089\n",
            "Epoch 250/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.0533 - val_accuracy: 0.9787\n",
            "\n",
            "Epoch 00250: val_accuracy did not improve from 0.99089\n",
            "Epoch 251/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0023 - accuracy: 0.9998 - val_loss: 0.0569 - val_accuracy: 0.9787\n",
            "\n",
            "Epoch 00251: val_accuracy did not improve from 0.99089\n",
            "Epoch 252/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0595 - val_accuracy: 0.9808\n",
            "\n",
            "Epoch 00252: val_accuracy did not improve from 0.99089\n",
            "Epoch 253/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.0560 - val_accuracy: 0.9815\n",
            "\n",
            "Epoch 00253: val_accuracy did not improve from 0.99089\n",
            "Epoch 254/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 8.9187e-04 - accuracy: 1.0000 - val_loss: 0.0533 - val_accuracy: 0.9815\n",
            "\n",
            "Epoch 00254: val_accuracy did not improve from 0.99089\n",
            "Epoch 255/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 8.1661e-04 - accuracy: 1.0000 - val_loss: 0.0501 - val_accuracy: 0.9822\n",
            "\n",
            "Epoch 00255: val_accuracy did not improve from 0.99089\n",
            "Epoch 256/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0566 - val_accuracy: 0.9780\n",
            "\n",
            "Epoch 00256: val_accuracy did not improve from 0.99089\n",
            "Epoch 257/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0021 - accuracy: 0.9998 - val_loss: 0.0670 - val_accuracy: 0.9759\n",
            "\n",
            "Epoch 00257: val_accuracy did not improve from 0.99089\n",
            "Epoch 258/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0544 - val_accuracy: 0.9815\n",
            "\n",
            "Epoch 00258: val_accuracy did not improve from 0.99089\n",
            "Epoch 259/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0026 - accuracy: 0.9991 - val_loss: 0.1638 - val_accuracy: 0.9581\n",
            "\n",
            "Epoch 00259: val_accuracy did not improve from 0.99089\n",
            "Epoch 260/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.0544 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00260: val_accuracy did not improve from 0.99089\n",
            "Epoch 261/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0853 - val_accuracy: 0.9723\n",
            "\n",
            "Epoch 00261: val_accuracy did not improve from 0.99089\n",
            "Epoch 262/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0545 - val_accuracy: 0.9808\n",
            "\n",
            "Epoch 00262: val_accuracy did not improve from 0.99089\n",
            "Epoch 263/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0500 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00263: val_accuracy did not improve from 0.99089\n",
            "Epoch 264/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 7.6756e-04 - accuracy: 1.0000 - val_loss: 0.0499 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00264: val_accuracy did not improve from 0.99089\n",
            "Epoch 265/500\n",
            "176/176 [==============================] - 1s 7ms/step - loss: 9.2600e-04 - accuracy: 1.0000 - val_loss: 0.0479 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00265: val_accuracy did not improve from 0.99089\n",
            "Epoch 266/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0032 - accuracy: 0.9988 - val_loss: 0.0585 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00266: val_accuracy did not improve from 0.99089\n",
            "Epoch 267/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 9.2646e-04 - accuracy: 1.0000 - val_loss: 0.0554 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00267: val_accuracy did not improve from 0.99089\n",
            "Epoch 268/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.0514 - val_accuracy: 0.9822\n",
            "\n",
            "Epoch 00268: val_accuracy did not improve from 0.99089\n",
            "Epoch 269/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 1.1136 - val_accuracy: 0.7848\n",
            "\n",
            "Epoch 00269: val_accuracy did not improve from 0.99089\n",
            "Epoch 270/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0031 - accuracy: 0.9989 - val_loss: 0.0543 - val_accuracy: 0.9822\n",
            "\n",
            "Epoch 00270: val_accuracy did not improve from 0.99089\n",
            "Epoch 271/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.0618 - val_accuracy: 0.9794\n",
            "\n",
            "Epoch 00271: val_accuracy did not improve from 0.99089\n",
            "Epoch 272/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 8.3399e-04 - accuracy: 1.0000 - val_loss: 0.0511 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00272: val_accuracy did not improve from 0.99089\n",
            "Epoch 273/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 0.0518 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00273: val_accuracy did not improve from 0.99089\n",
            "Epoch 274/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 5.0574e-04 - accuracy: 1.0000 - val_loss: 0.0494 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00274: val_accuracy did not improve from 0.99089\n",
            "Epoch 275/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.0681 - val_accuracy: 0.9766\n",
            "\n",
            "Epoch 00275: val_accuracy did not improve from 0.99089\n",
            "Epoch 276/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 5.3333e-04 - accuracy: 1.0000 - val_loss: 0.0501 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00276: val_accuracy did not improve from 0.99089\n",
            "Epoch 277/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 6.7348e-04 - accuracy: 1.0000 - val_loss: 0.0539 - val_accuracy: 0.9822\n",
            "\n",
            "Epoch 00277: val_accuracy did not improve from 0.99089\n",
            "Epoch 278/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 4.4880e-04 - accuracy: 1.0000 - val_loss: 0.0496 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00278: val_accuracy did not improve from 0.99089\n",
            "Epoch 279/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1960 - accuracy: 0.9487 - val_loss: 2.5287 - val_accuracy: 0.5341\n",
            "\n",
            "Epoch 00279: val_accuracy did not improve from 0.99089\n",
            "Epoch 280/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.1498 - accuracy: 0.9492 - val_loss: 0.4058 - val_accuracy: 0.8764\n",
            "\n",
            "Epoch 00280: val_accuracy did not improve from 0.99089\n",
            "Epoch 281/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0387 - accuracy: 0.9869 - val_loss: 0.2142 - val_accuracy: 0.9148\n",
            "\n",
            "Epoch 00281: val_accuracy did not improve from 0.99089\n",
            "Epoch 282/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0177 - accuracy: 0.9941 - val_loss: 0.0857 - val_accuracy: 0.9695\n",
            "\n",
            "Epoch 00282: val_accuracy did not improve from 0.99089\n",
            "Epoch 283/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0110 - accuracy: 0.9964 - val_loss: 0.0548 - val_accuracy: 0.9751\n",
            "\n",
            "Epoch 00283: val_accuracy did not improve from 0.99089\n",
            "Epoch 284/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0064 - accuracy: 0.9989 - val_loss: 0.1571 - val_accuracy: 0.9567\n",
            "\n",
            "Epoch 00284: val_accuracy did not improve from 0.99089\n",
            "Epoch 285/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0121 - accuracy: 0.9957 - val_loss: 0.0743 - val_accuracy: 0.9751\n",
            "\n",
            "Epoch 00285: val_accuracy did not improve from 0.99089\n",
            "Epoch 286/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0221 - accuracy: 0.9924 - val_loss: 0.2704 - val_accuracy: 0.9141\n",
            "\n",
            "Epoch 00286: val_accuracy did not improve from 0.99089\n",
            "Epoch 287/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0091 - accuracy: 0.9972 - val_loss: 0.1490 - val_accuracy: 0.9453\n",
            "\n",
            "Epoch 00287: val_accuracy did not improve from 0.99089\n",
            "Epoch 288/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0036 - accuracy: 0.9995 - val_loss: 0.0815 - val_accuracy: 0.9695\n",
            "\n",
            "Epoch 00288: val_accuracy did not improve from 0.99089\n",
            "Epoch 289/500\n",
            "176/176 [==============================] - 1s 7ms/step - loss: 0.0044 - accuracy: 0.9988 - val_loss: 0.0535 - val_accuracy: 0.9787\n",
            "\n",
            "Epoch 00289: val_accuracy did not improve from 0.99089\n",
            "Epoch 290/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0448 - val_accuracy: 0.9815\n",
            "\n",
            "Epoch 00290: val_accuracy did not improve from 0.99089\n",
            "Epoch 291/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0055 - accuracy: 0.9980 - val_loss: 0.0486 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00291: val_accuracy did not improve from 0.99089\n",
            "Epoch 292/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0062 - accuracy: 0.9989 - val_loss: 0.0433 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00292: val_accuracy did not improve from 0.99089\n",
            "Epoch 293/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0459 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00293: val_accuracy did not improve from 0.99089\n",
            "Epoch 294/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.0639 - val_accuracy: 0.9759\n",
            "\n",
            "Epoch 00294: val_accuracy did not improve from 0.99089\n",
            "Epoch 295/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0051 - accuracy: 0.9984 - val_loss: 0.0531 - val_accuracy: 0.9787\n",
            "\n",
            "Epoch 00295: val_accuracy did not improve from 0.99089\n",
            "Epoch 296/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.0555 - val_accuracy: 0.9759\n",
            "\n",
            "Epoch 00296: val_accuracy did not improve from 0.99089\n",
            "Epoch 297/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0474 - val_accuracy: 0.9822\n",
            "\n",
            "Epoch 00297: val_accuracy did not improve from 0.99089\n",
            "Epoch 298/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.0532 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00298: val_accuracy did not improve from 0.99089\n",
            "Epoch 299/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 9.1387e-04 - accuracy: 1.0000 - val_loss: 0.0496 - val_accuracy: 0.9815\n",
            "\n",
            "Epoch 00299: val_accuracy did not improve from 0.99089\n",
            "Epoch 300/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 9.7266e-04 - accuracy: 1.0000 - val_loss: 0.0544 - val_accuracy: 0.9815\n",
            "\n",
            "Epoch 00300: val_accuracy did not improve from 0.99089\n",
            "Epoch 301/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0514 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00301: val_accuracy did not improve from 0.99089\n",
            "Epoch 302/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.0456 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00302: val_accuracy did not improve from 0.99089\n",
            "Epoch 303/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.0457 - val_accuracy: 0.9822\n",
            "\n",
            "Epoch 00303: val_accuracy did not improve from 0.99089\n",
            "Epoch 304/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.0653 - val_accuracy: 0.9766\n",
            "\n",
            "Epoch 00304: val_accuracy did not improve from 0.99089\n",
            "Epoch 305/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0036 - accuracy: 0.9991 - val_loss: 0.0520 - val_accuracy: 0.9808\n",
            "\n",
            "Epoch 00305: val_accuracy did not improve from 0.99089\n",
            "Epoch 306/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0532 - val_accuracy: 0.9794\n",
            "\n",
            "Epoch 00306: val_accuracy did not improve from 0.99089\n",
            "Epoch 307/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0480 - val_accuracy: 0.9858\n",
            "\n",
            "Epoch 00307: val_accuracy did not improve from 0.99089\n",
            "Epoch 308/500\n",
            "176/176 [==============================] - 1s 7ms/step - loss: 8.5410e-04 - accuracy: 1.0000 - val_loss: 0.0569 - val_accuracy: 0.9815\n",
            "\n",
            "Epoch 00308: val_accuracy did not improve from 0.99089\n",
            "Epoch 309/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 0.0499 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00309: val_accuracy did not improve from 0.99089\n",
            "Epoch 310/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 4.7969e-04 - accuracy: 1.0000 - val_loss: 0.0477 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00310: val_accuracy did not improve from 0.99089\n",
            "Epoch 311/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 5.8583e-04 - accuracy: 1.0000 - val_loss: 0.0446 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00311: val_accuracy did not improve from 0.99089\n",
            "Epoch 312/500\n",
            "176/176 [==============================] - 1s 7ms/step - loss: 0.0065 - accuracy: 0.9984 - val_loss: 0.0880 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00312: val_accuracy did not improve from 0.99089\n",
            "Epoch 313/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0059 - accuracy: 0.9982 - val_loss: 0.1521 - val_accuracy: 0.9453\n",
            "\n",
            "Epoch 00313: val_accuracy did not improve from 0.99089\n",
            "Epoch 314/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0024 - accuracy: 0.9998 - val_loss: 0.0566 - val_accuracy: 0.9780\n",
            "\n",
            "Epoch 00314: val_accuracy did not improve from 0.99089\n",
            "Epoch 315/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0044 - accuracy: 0.9988 - val_loss: 0.0586 - val_accuracy: 0.9794\n",
            "\n",
            "Epoch 00315: val_accuracy did not improve from 0.99089\n",
            "Epoch 316/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.0659 - val_accuracy: 0.9759\n",
            "\n",
            "Epoch 00316: val_accuracy did not improve from 0.99089\n",
            "Epoch 317/500\n",
            "176/176 [==============================] - 1s 7ms/step - loss: 9.6611e-04 - accuracy: 1.0000 - val_loss: 0.1384 - val_accuracy: 0.9545\n",
            "\n",
            "Epoch 00317: val_accuracy did not improve from 0.99089\n",
            "Epoch 318/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0540 - val_accuracy: 0.9815\n",
            "\n",
            "Epoch 00318: val_accuracy did not improve from 0.99089\n",
            "Epoch 319/500\n",
            "176/176 [==============================] - 1s 7ms/step - loss: 7.8703e-04 - accuracy: 1.0000 - val_loss: 0.0534 - val_accuracy: 0.9815\n",
            "\n",
            "Epoch 00319: val_accuracy did not improve from 0.99089\n",
            "Epoch 320/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 9.2221e-04 - accuracy: 1.0000 - val_loss: 0.0619 - val_accuracy: 0.9766\n",
            "\n",
            "Epoch 00320: val_accuracy did not improve from 0.99089\n",
            "Epoch 321/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 7.2413e-04 - accuracy: 1.0000 - val_loss: 0.0534 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00321: val_accuracy did not improve from 0.99089\n",
            "Epoch 322/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 5.9973e-04 - accuracy: 1.0000 - val_loss: 0.0477 - val_accuracy: 0.9822\n",
            "\n",
            "Epoch 00322: val_accuracy did not improve from 0.99089\n",
            "Epoch 323/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 5.4195e-04 - accuracy: 1.0000 - val_loss: 0.0523 - val_accuracy: 0.9808\n",
            "\n",
            "Epoch 00323: val_accuracy did not improve from 0.99089\n",
            "Epoch 324/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 4.0840e-04 - accuracy: 1.0000 - val_loss: 0.0495 - val_accuracy: 0.9822\n",
            "\n",
            "Epoch 00324: val_accuracy did not improve from 0.99089\n",
            "Epoch 325/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 6.7697e-04 - accuracy: 1.0000 - val_loss: 0.0496 - val_accuracy: 0.9822\n",
            "\n",
            "Epoch 00325: val_accuracy did not improve from 0.99089\n",
            "Epoch 326/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 5.7619e-04 - accuracy: 1.0000 - val_loss: 0.0545 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00326: val_accuracy did not improve from 0.99089\n",
            "Epoch 327/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 6.8688e-04 - accuracy: 1.0000 - val_loss: 0.0454 - val_accuracy: 0.9851\n",
            "\n",
            "Epoch 00327: val_accuracy did not improve from 0.99089\n",
            "Epoch 328/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.0655 - val_accuracy: 0.9815\n",
            "\n",
            "Epoch 00328: val_accuracy did not improve from 0.99089\n",
            "Epoch 329/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 6.8924e-04 - accuracy: 1.0000 - val_loss: 0.0523 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00329: val_accuracy did not improve from 0.99089\n",
            "Epoch 330/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 5.6190e-04 - accuracy: 1.0000 - val_loss: 0.0506 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00330: val_accuracy did not improve from 0.99089\n",
            "Epoch 331/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0013 - accuracy: 0.9995 - val_loss: 0.0478 - val_accuracy: 0.9851\n",
            "\n",
            "Epoch 00331: val_accuracy did not improve from 0.99089\n",
            "Epoch 332/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 5.0540e-04 - accuracy: 1.0000 - val_loss: 0.0469 - val_accuracy: 0.9851\n",
            "\n",
            "Epoch 00332: val_accuracy did not improve from 0.99089\n",
            "Epoch 333/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 4.0741e-04 - accuracy: 1.0000 - val_loss: 0.0495 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00333: val_accuracy did not improve from 0.99089\n",
            "Epoch 334/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 5.0794e-04 - accuracy: 1.0000 - val_loss: 0.0482 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00334: val_accuracy did not improve from 0.99089\n",
            "Epoch 335/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 4.9870e-04 - accuracy: 1.0000 - val_loss: 0.0472 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00335: val_accuracy did not improve from 0.99089\n",
            "Epoch 336/500\n",
            "176/176 [==============================] - 1s 7ms/step - loss: 4.4571e-04 - accuracy: 1.0000 - val_loss: 0.0488 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00336: val_accuracy did not improve from 0.99089\n",
            "Epoch 337/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 3.6868e-04 - accuracy: 1.0000 - val_loss: 0.0480 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00337: val_accuracy did not improve from 0.99089\n",
            "Epoch 338/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 3.9731e-04 - accuracy: 1.0000 - val_loss: 0.0487 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00338: val_accuracy did not improve from 0.99089\n",
            "Epoch 339/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.0863 - val_accuracy: 0.9737\n",
            "\n",
            "Epoch 00339: val_accuracy did not improve from 0.99089\n",
            "Epoch 340/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 0.0521 - val_accuracy: 0.9822\n",
            "\n",
            "Epoch 00340: val_accuracy did not improve from 0.99089\n",
            "Epoch 341/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 7.7916e-04 - accuracy: 0.9998 - val_loss: 0.0624 - val_accuracy: 0.9787\n",
            "\n",
            "Epoch 00341: val_accuracy did not improve from 0.99089\n",
            "Epoch 342/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 5.5736e-04 - accuracy: 1.0000 - val_loss: 0.0514 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00342: val_accuracy did not improve from 0.99089\n",
            "Epoch 343/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 4.6997e-04 - accuracy: 1.0000 - val_loss: 0.0469 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00343: val_accuracy did not improve from 0.99089\n",
            "Epoch 344/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 2.6204e-04 - accuracy: 1.0000 - val_loss: 0.0463 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00344: val_accuracy did not improve from 0.99089\n",
            "Epoch 345/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 2.7532e-04 - accuracy: 1.0000 - val_loss: 0.0478 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00345: val_accuracy did not improve from 0.99089\n",
            "Epoch 346/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 3.4378e-04 - accuracy: 1.0000 - val_loss: 0.0487 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00346: val_accuracy did not improve from 0.99089\n",
            "Epoch 347/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 5.7884e-04 - accuracy: 1.0000 - val_loss: 0.0663 - val_accuracy: 0.9759\n",
            "\n",
            "Epoch 00347: val_accuracy did not improve from 0.99089\n",
            "Epoch 348/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 3.9634e-04 - accuracy: 1.0000 - val_loss: 0.0510 - val_accuracy: 0.9822\n",
            "\n",
            "Epoch 00348: val_accuracy did not improve from 0.99089\n",
            "Epoch 349/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 3.2568e-04 - accuracy: 1.0000 - val_loss: 0.0466 - val_accuracy: 0.9851\n",
            "\n",
            "Epoch 00349: val_accuracy did not improve from 0.99089\n",
            "Epoch 350/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 4.0546e-04 - accuracy: 1.0000 - val_loss: 0.0507 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00350: val_accuracy did not improve from 0.99089\n",
            "Epoch 351/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 4.5282e-04 - accuracy: 1.0000 - val_loss: 0.0487 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00351: val_accuracy did not improve from 0.99089\n",
            "Epoch 352/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0058 - accuracy: 0.9984 - val_loss: 3.0452 - val_accuracy: 0.6357\n",
            "\n",
            "Epoch 00352: val_accuracy did not improve from 0.99089\n",
            "Epoch 353/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0154 - accuracy: 0.9950 - val_loss: 0.0998 - val_accuracy: 0.9673\n",
            "\n",
            "Epoch 00353: val_accuracy did not improve from 0.99089\n",
            "Epoch 354/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0050 - accuracy: 0.9984 - val_loss: 0.0676 - val_accuracy: 0.9759\n",
            "\n",
            "Epoch 00354: val_accuracy did not improve from 0.99089\n",
            "Epoch 355/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 5.7071e-04 - accuracy: 1.0000 - val_loss: 0.0520 - val_accuracy: 0.9808\n",
            "\n",
            "Epoch 00355: val_accuracy did not improve from 0.99089\n",
            "Epoch 356/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 5.4876e-04 - accuracy: 1.0000 - val_loss: 0.0465 - val_accuracy: 0.9822\n",
            "\n",
            "Epoch 00356: val_accuracy did not improve from 0.99089\n",
            "Epoch 357/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 6.1871e-04 - accuracy: 1.0000 - val_loss: 0.0479 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00357: val_accuracy did not improve from 0.99089\n",
            "Epoch 358/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 4.4890e-04 - accuracy: 1.0000 - val_loss: 0.0495 - val_accuracy: 0.9822\n",
            "\n",
            "Epoch 00358: val_accuracy did not improve from 0.99089\n",
            "Epoch 359/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 5.1599e-04 - accuracy: 1.0000 - val_loss: 0.0480 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00359: val_accuracy did not improve from 0.99089\n",
            "Epoch 360/500\n",
            "176/176 [==============================] - 1s 7ms/step - loss: 4.2966e-04 - accuracy: 1.0000 - val_loss: 0.0494 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00360: val_accuracy did not improve from 0.99089\n",
            "Epoch 361/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 6.6576e-04 - accuracy: 0.9998 - val_loss: 0.0526 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00361: val_accuracy did not improve from 0.99089\n",
            "Epoch 362/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 4.1610e-04 - accuracy: 1.0000 - val_loss: 0.0556 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00362: val_accuracy did not improve from 0.99089\n",
            "Epoch 363/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 4.4889e-04 - accuracy: 1.0000 - val_loss: 0.0727 - val_accuracy: 0.9723\n",
            "\n",
            "Epoch 00363: val_accuracy did not improve from 0.99089\n",
            "Epoch 364/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 4.2108e-04 - accuracy: 1.0000 - val_loss: 0.0497 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00364: val_accuracy did not improve from 0.99089\n",
            "Epoch 365/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 3.3210e-04 - accuracy: 1.0000 - val_loss: 0.0565 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00365: val_accuracy did not improve from 0.99089\n",
            "Epoch 366/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 3.2629e-04 - accuracy: 1.0000 - val_loss: 0.0511 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00366: val_accuracy did not improve from 0.99089\n",
            "Epoch 367/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 3.5815e-04 - accuracy: 1.0000 - val_loss: 0.0516 - val_accuracy: 0.9822\n",
            "\n",
            "Epoch 00367: val_accuracy did not improve from 0.99089\n",
            "Epoch 368/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 2.4607e-04 - accuracy: 1.0000 - val_loss: 0.0500 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00368: val_accuracy did not improve from 0.99089\n",
            "Epoch 369/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 2.3100e-04 - accuracy: 1.0000 - val_loss: 0.0500 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00369: val_accuracy did not improve from 0.99089\n",
            "Epoch 370/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 2.6659e-04 - accuracy: 1.0000 - val_loss: 0.0502 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00370: val_accuracy did not improve from 0.99089\n",
            "Epoch 371/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 8.5528e-04 - accuracy: 0.9998 - val_loss: 0.0510 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00371: val_accuracy did not improve from 0.99089\n",
            "Epoch 372/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 5.3464e-04 - accuracy: 1.0000 - val_loss: 0.0496 - val_accuracy: 0.9822\n",
            "\n",
            "Epoch 00372: val_accuracy did not improve from 0.99089\n",
            "Epoch 373/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 3.2133e-04 - accuracy: 1.0000 - val_loss: 0.0466 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00373: val_accuracy did not improve from 0.99089\n",
            "Epoch 374/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 3.0251e-04 - accuracy: 1.0000 - val_loss: 0.0504 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00374: val_accuracy did not improve from 0.99089\n",
            "Epoch 375/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 2.4803e-04 - accuracy: 1.0000 - val_loss: 0.0483 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00375: val_accuracy did not improve from 0.99089\n",
            "Epoch 376/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 2.5407e-04 - accuracy: 1.0000 - val_loss: 0.0486 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00376: val_accuracy did not improve from 0.99089\n",
            "Epoch 377/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 3.0399e-04 - accuracy: 1.0000 - val_loss: 0.0504 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00377: val_accuracy did not improve from 0.99089\n",
            "Epoch 378/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0040 - accuracy: 0.9988 - val_loss: 0.0455 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00378: val_accuracy did not improve from 0.99089\n",
            "Epoch 379/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 5.6468e-04 - accuracy: 0.9998 - val_loss: 0.0457 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00379: val_accuracy did not improve from 0.99089\n",
            "Epoch 380/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 6.0088e-04 - accuracy: 0.9998 - val_loss: 0.0488 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00380: val_accuracy did not improve from 0.99089\n",
            "Epoch 381/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 3.0429e-04 - accuracy: 1.0000 - val_loss: 0.0558 - val_accuracy: 0.9794\n",
            "\n",
            "Epoch 00381: val_accuracy did not improve from 0.99089\n",
            "Epoch 382/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 2.5374e-04 - accuracy: 1.0000 - val_loss: 0.0475 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00382: val_accuracy did not improve from 0.99089\n",
            "Epoch 383/500\n",
            "176/176 [==============================] - 1s 7ms/step - loss: 3.0288e-04 - accuracy: 1.0000 - val_loss: 0.0458 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00383: val_accuracy did not improve from 0.99089\n",
            "Epoch 384/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 2.7930e-04 - accuracy: 1.0000 - val_loss: 0.0455 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00384: val_accuracy did not improve from 0.99089\n",
            "Epoch 385/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.2752 - val_accuracy: 0.9517\n",
            "\n",
            "Epoch 00385: val_accuracy did not improve from 0.99089\n",
            "Epoch 386/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1087 - val_accuracy: 0.9616\n",
            "\n",
            "Epoch 00386: val_accuracy did not improve from 0.99089\n",
            "Epoch 387/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0021 - accuracy: 0.9993 - val_loss: 0.0916 - val_accuracy: 0.9723\n",
            "\n",
            "Epoch 00387: val_accuracy did not improve from 0.99089\n",
            "Epoch 388/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 4.7145e-04 - accuracy: 1.0000 - val_loss: 0.0531 - val_accuracy: 0.9808\n",
            "\n",
            "Epoch 00388: val_accuracy did not improve from 0.99089\n",
            "Epoch 389/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 4.3992e-04 - accuracy: 1.0000 - val_loss: 0.0488 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00389: val_accuracy did not improve from 0.99089\n",
            "Epoch 390/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.0498 - val_accuracy: 0.9822\n",
            "\n",
            "Epoch 00390: val_accuracy did not improve from 0.99089\n",
            "Epoch 391/500\n",
            "176/176 [==============================] - 1s 7ms/step - loss: 6.1587e-04 - accuracy: 0.9998 - val_loss: 0.0494 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00391: val_accuracy did not improve from 0.99089\n",
            "Epoch 392/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 0.0517 - val_accuracy: 0.9815\n",
            "\n",
            "Epoch 00392: val_accuracy did not improve from 0.99089\n",
            "Epoch 393/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.6950 - val_accuracy: 0.8871\n",
            "\n",
            "Epoch 00393: val_accuracy did not improve from 0.99089\n",
            "Epoch 394/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0019 - accuracy: 0.9993 - val_loss: 0.0623 - val_accuracy: 0.9773\n",
            "\n",
            "Epoch 00394: val_accuracy did not improve from 0.99089\n",
            "Epoch 395/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.0498 - val_accuracy: 0.9822\n",
            "\n",
            "Epoch 00395: val_accuracy did not improve from 0.99089\n",
            "Epoch 396/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 3.7968e-04 - accuracy: 1.0000 - val_loss: 0.0475 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00396: val_accuracy did not improve from 0.99089\n",
            "Epoch 397/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 3.4338e-04 - accuracy: 1.0000 - val_loss: 0.0568 - val_accuracy: 0.9794\n",
            "\n",
            "Epoch 00397: val_accuracy did not improve from 0.99089\n",
            "Epoch 398/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 2.8146e-04 - accuracy: 1.0000 - val_loss: 0.0472 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00398: val_accuracy did not improve from 0.99089\n",
            "Epoch 399/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 3.5225e-04 - accuracy: 1.0000 - val_loss: 0.0462 - val_accuracy: 0.9851\n",
            "\n",
            "Epoch 00399: val_accuracy did not improve from 0.99089\n",
            "Epoch 400/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.0011 - accuracy: 0.9995 - val_loss: 0.0525 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00400: val_accuracy did not improve from 0.99089\n",
            "Epoch 401/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 3.4242e-04 - accuracy: 1.0000 - val_loss: 0.0510 - val_accuracy: 0.9822\n",
            "\n",
            "Epoch 00401: val_accuracy did not improve from 0.99089\n",
            "Epoch 402/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 1.9379e-04 - accuracy: 1.0000 - val_loss: 0.0516 - val_accuracy: 0.9822\n",
            "\n",
            "Epoch 00402: val_accuracy did not improve from 0.99089\n",
            "Epoch 403/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 1.9934e-04 - accuracy: 1.0000 - val_loss: 0.0510 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00403: val_accuracy did not improve from 0.99089\n",
            "Epoch 404/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 2.0493e-04 - accuracy: 1.0000 - val_loss: 0.0503 - val_accuracy: 0.9822\n",
            "\n",
            "Epoch 00404: val_accuracy did not improve from 0.99089\n",
            "Epoch 405/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 1.9534e-04 - accuracy: 1.0000 - val_loss: 0.0498 - val_accuracy: 0.9830\n",
            "\n",
            "Epoch 00405: val_accuracy did not improve from 0.99089\n",
            "Epoch 406/500\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 2.5138e-04 - accuracy: 1.0000 - val_loss: 0.0467 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00406: val_accuracy did not improve from 0.99089\n",
            "Epoch 407/500\n",
            "176/176 [==============================] - 1s 7ms/step - loss: 1.9417e-04 - accuracy: 1.0000 - val_loss: 0.0472 - val_accuracy: 0.9837\n",
            "\n",
            "Epoch 00407: val_accuracy did not improve from 0.99089\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00407: early stopping\n",
            "Training/validation time was: 643.717 seconds\n",
            "\n",
            " \n",
            "EVALUATING THE MODEL:\n",
            "55/55 [==============================] - 0s 3ms/step - loss: 0.0412 - accuracy: 0.9841\n",
            "Evaluate time was 0.187264 seconds\n",
            "0.9840909242630005\n",
            "\n",
            "SUMMARY: \n",
            "With 550 cases, the max val_accuracy was: 98.5795 % and the test_accuracy was: 98.4091 %. \n",
            "It is a difference of 0.1705 %.\n",
            "Fitness history is: [1, 0.987500011920929] \n",
            "Cases history is: [600, 550] \n",
            "acc_val history is: [0.9908854365348816, 0.9857954382896423] \n",
            "acc_test history is: [0.987500011920929, 0.9840909242630005] \n",
            "Times history is: [925.6832382678986, 643.7173902988434]\n",
            "\n",
            "---> ITERATING NOW WITH: 500 CASES ! ( 83.33 % FROM TOTAL ) \n",
            "----> AND THE HISTORY OF CASES DECREASE IS: [600, 550] \n",
            " \n",
            "TRAINING AND VALIDATING THE MODEL: \n",
            "It will take a while ;D...\n",
            "Epoch 1/500\n",
            "160/160 [==============================] - 2s 8ms/step - loss: 1.1243 - accuracy: 0.5535 - val_loss: 2.9290 - val_accuracy: 0.0617\n",
            "\n",
            "Epoch 00001: val_accuracy did not improve from 0.99089\n",
            "Epoch 2/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.8548 - accuracy: 0.6322 - val_loss: 3.1614 - val_accuracy: 0.1570\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.99089\n",
            "Epoch 3/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.7131 - accuracy: 0.7064 - val_loss: 1.1798 - val_accuracy: 0.5289\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.99089\n",
            "Epoch 4/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.6368 - accuracy: 0.7330 - val_loss: 1.6551 - val_accuracy: 0.3672\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.99089\n",
            "Epoch 5/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.6468 - accuracy: 0.7312 - val_loss: 3.3349 - val_accuracy: 0.2328\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.99089\n",
            "Epoch 6/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.8271 - accuracy: 0.6426 - val_loss: 2.3705 - val_accuracy: 0.3234\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.99089\n",
            "Epoch 7/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.7874 - accuracy: 0.6562 - val_loss: 2.9087 - val_accuracy: 0.1828\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.99089\n",
            "Epoch 8/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.6509 - accuracy: 0.7281 - val_loss: 0.7623 - val_accuracy: 0.7305\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.99089\n",
            "Epoch 9/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.7625 - accuracy: 0.6713 - val_loss: 0.9930 - val_accuracy: 0.5695\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.99089\n",
            "Epoch 10/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.6813 - accuracy: 0.7064 - val_loss: 1.3516 - val_accuracy: 0.3633\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.99089\n",
            "Epoch 11/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.4188 - accuracy: 0.8303 - val_loss: 0.7160 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.99089\n",
            "Epoch 12/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.3711 - accuracy: 0.8512 - val_loss: 0.6810 - val_accuracy: 0.7250\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.99089\n",
            "Epoch 13/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.2981 - accuracy: 0.8779 - val_loss: 1.4115 - val_accuracy: 0.6383\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.99089\n",
            "Epoch 14/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.2932 - accuracy: 0.8777 - val_loss: 1.4980 - val_accuracy: 0.6070\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.99089\n",
            "Epoch 15/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.2711 - accuracy: 0.8828 - val_loss: 0.5328 - val_accuracy: 0.7812\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.99089\n",
            "Epoch 16/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.2617 - accuracy: 0.8869 - val_loss: 0.2628 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.99089\n",
            "Epoch 17/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.2736 - accuracy: 0.8838 - val_loss: 0.8392 - val_accuracy: 0.6742\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.99089\n",
            "Epoch 18/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.2616 - accuracy: 0.8838 - val_loss: 0.4803 - val_accuracy: 0.8070\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.99089\n",
            "Epoch 19/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.2558 - accuracy: 0.8918 - val_loss: 1.2412 - val_accuracy: 0.6492\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.99089\n",
            "Epoch 20/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.2492 - accuracy: 0.8941 - val_loss: 0.8916 - val_accuracy: 0.7398\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.99089\n",
            "Epoch 21/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.2476 - accuracy: 0.8945 - val_loss: 1.0287 - val_accuracy: 0.7273\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.99089\n",
            "Epoch 22/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.2464 - accuracy: 0.8967 - val_loss: 1.0779 - val_accuracy: 0.6586\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.99089\n",
            "Epoch 23/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.2433 - accuracy: 0.8936 - val_loss: 0.2414 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.99089\n",
            "Epoch 24/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.2434 - accuracy: 0.8963 - val_loss: 0.9165 - val_accuracy: 0.6711\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.99089\n",
            "Epoch 25/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.2282 - accuracy: 0.8990 - val_loss: 0.2536 - val_accuracy: 0.8852\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.99089\n",
            "Epoch 26/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.2367 - accuracy: 0.8953 - val_loss: 0.6958 - val_accuracy: 0.7844\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.99089\n",
            "Epoch 27/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.2272 - accuracy: 0.9004 - val_loss: 0.2286 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.99089\n",
            "Epoch 28/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.2239 - accuracy: 0.9000 - val_loss: 0.2683 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.99089\n",
            "Epoch 29/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.2331 - accuracy: 0.8945 - val_loss: 1.2558 - val_accuracy: 0.6570\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.99089\n",
            "Epoch 30/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.2253 - accuracy: 0.9010 - val_loss: 0.2749 - val_accuracy: 0.8938\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.99089\n",
            "Epoch 31/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.2162 - accuracy: 0.9100 - val_loss: 0.2171 - val_accuracy: 0.9109\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.99089\n",
            "Epoch 32/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.2147 - accuracy: 0.9090 - val_loss: 0.7840 - val_accuracy: 0.7391\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.99089\n",
            "Epoch 33/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.2055 - accuracy: 0.9115 - val_loss: 0.2430 - val_accuracy: 0.9031\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.99089\n",
            "Epoch 34/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.2090 - accuracy: 0.9094 - val_loss: 0.2212 - val_accuracy: 0.9023\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.99089\n",
            "Epoch 35/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.2023 - accuracy: 0.9115 - val_loss: 0.2194 - val_accuracy: 0.9148\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.99089\n",
            "Epoch 36/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.2020 - accuracy: 0.9135 - val_loss: 0.4024 - val_accuracy: 0.8383\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.99089\n",
            "Epoch 37/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.1980 - accuracy: 0.9137 - val_loss: 0.2050 - val_accuracy: 0.9297\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.99089\n",
            "Epoch 38/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.1892 - accuracy: 0.9260 - val_loss: 0.2600 - val_accuracy: 0.8797\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.99089\n",
            "Epoch 39/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.1858 - accuracy: 0.9225 - val_loss: 1.3773 - val_accuracy: 0.6492\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.99089\n",
            "Epoch 40/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.1896 - accuracy: 0.9229 - val_loss: 0.5247 - val_accuracy: 0.7797\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.99089\n",
            "Epoch 41/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.1772 - accuracy: 0.9248 - val_loss: 0.2220 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.99089\n",
            "Epoch 42/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.1744 - accuracy: 0.9305 - val_loss: 0.1965 - val_accuracy: 0.9242\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.99089\n",
            "Epoch 43/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.1711 - accuracy: 0.9262 - val_loss: 0.8053 - val_accuracy: 0.7937\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.99089\n",
            "Epoch 44/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.1658 - accuracy: 0.9322 - val_loss: 0.2975 - val_accuracy: 0.8727\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.99089\n",
            "Epoch 45/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.1642 - accuracy: 0.9344 - val_loss: 1.3668 - val_accuracy: 0.5875\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.99089\n",
            "Epoch 46/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.1585 - accuracy: 0.9361 - val_loss: 0.1590 - val_accuracy: 0.9445\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.99089\n",
            "Epoch 47/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.1497 - accuracy: 0.9379 - val_loss: 0.1643 - val_accuracy: 0.9336\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.99089\n",
            "Epoch 48/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.1539 - accuracy: 0.9393 - val_loss: 0.1859 - val_accuracy: 0.9258\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.99089\n",
            "Epoch 49/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.1538 - accuracy: 0.9396 - val_loss: 0.2003 - val_accuracy: 0.9195\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.99089\n",
            "Epoch 50/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.1461 - accuracy: 0.9387 - val_loss: 0.5244 - val_accuracy: 0.8219\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.99089\n",
            "Epoch 51/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.1593 - accuracy: 0.9352 - val_loss: 1.9670 - val_accuracy: 0.5719\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.99089\n",
            "Epoch 52/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.1370 - accuracy: 0.9455 - val_loss: 0.1609 - val_accuracy: 0.9375\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.99089\n",
            "Epoch 53/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.1375 - accuracy: 0.9439 - val_loss: 0.1938 - val_accuracy: 0.9312\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.99089\n",
            "Epoch 54/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.1288 - accuracy: 0.9510 - val_loss: 0.3945 - val_accuracy: 0.8547\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.99089\n",
            "Epoch 55/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.1289 - accuracy: 0.9465 - val_loss: 1.2860 - val_accuracy: 0.7094\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.99089\n",
            "Epoch 56/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.1287 - accuracy: 0.9490 - val_loss: 0.1418 - val_accuracy: 0.9523\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.99089\n",
            "Epoch 57/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.1254 - accuracy: 0.9486 - val_loss: 0.2787 - val_accuracy: 0.9047\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.99089\n",
            "Epoch 58/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.1187 - accuracy: 0.9512 - val_loss: 0.2040 - val_accuracy: 0.9164\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.99089\n",
            "Epoch 59/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.1171 - accuracy: 0.9516 - val_loss: 0.7398 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.99089\n",
            "Epoch 60/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.1112 - accuracy: 0.9561 - val_loss: 0.2250 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.99089\n",
            "Epoch 61/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.1088 - accuracy: 0.9582 - val_loss: 0.1189 - val_accuracy: 0.9563\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.99089\n",
            "Epoch 62/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.1054 - accuracy: 0.9563 - val_loss: 1.0014 - val_accuracy: 0.6531\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.99089\n",
            "Epoch 63/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0982 - accuracy: 0.9611 - val_loss: 0.1344 - val_accuracy: 0.9531\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.99089\n",
            "Epoch 64/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0945 - accuracy: 0.9631 - val_loss: 0.7772 - val_accuracy: 0.6875\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.99089\n",
            "Epoch 65/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0971 - accuracy: 0.9619 - val_loss: 0.1828 - val_accuracy: 0.9281\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.99089\n",
            "Epoch 66/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0937 - accuracy: 0.9637 - val_loss: 1.9342 - val_accuracy: 0.8023\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.99089\n",
            "Epoch 67/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0953 - accuracy: 0.9652 - val_loss: 0.2369 - val_accuracy: 0.9000\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.99089\n",
            "Epoch 68/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0842 - accuracy: 0.9688 - val_loss: 0.6033 - val_accuracy: 0.8133\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.99089\n",
            "Epoch 69/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0763 - accuracy: 0.9725 - val_loss: 0.4731 - val_accuracy: 0.8398\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.99089\n",
            "Epoch 70/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.0777 - accuracy: 0.9727 - val_loss: 0.1287 - val_accuracy: 0.9492\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.99089\n",
            "Epoch 71/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0730 - accuracy: 0.9723 - val_loss: 0.2420 - val_accuracy: 0.9172\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.99089\n",
            "Epoch 72/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0657 - accuracy: 0.9756 - val_loss: 0.6630 - val_accuracy: 0.8008\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.99089\n",
            "Epoch 73/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0693 - accuracy: 0.9742 - val_loss: 0.2516 - val_accuracy: 0.8891\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.99089\n",
            "Epoch 74/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0720 - accuracy: 0.9725 - val_loss: 0.2052 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.99089\n",
            "Epoch 75/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0679 - accuracy: 0.9752 - val_loss: 0.5124 - val_accuracy: 0.8367\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.99089\n",
            "Epoch 76/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0685 - accuracy: 0.9723 - val_loss: 0.1125 - val_accuracy: 0.9578\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.99089\n",
            "Epoch 77/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.0591 - accuracy: 0.9777 - val_loss: 0.1708 - val_accuracy: 0.9219\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.99089\n",
            "Epoch 78/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0606 - accuracy: 0.9750 - val_loss: 0.0751 - val_accuracy: 0.9766\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.99089\n",
            "Epoch 79/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0479 - accuracy: 0.9814 - val_loss: 0.5878 - val_accuracy: 0.8141\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.99089\n",
            "Epoch 80/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0430 - accuracy: 0.9840 - val_loss: 0.6424 - val_accuracy: 0.8227\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.99089\n",
            "Epoch 81/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0466 - accuracy: 0.9816 - val_loss: 0.5607 - val_accuracy: 0.8211\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.99089\n",
            "Epoch 82/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0532 - accuracy: 0.9824 - val_loss: 0.0855 - val_accuracy: 0.9727\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.99089\n",
            "Epoch 83/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0497 - accuracy: 0.9799 - val_loss: 0.2007 - val_accuracy: 0.9070\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.99089\n",
            "Epoch 84/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.0471 - accuracy: 0.9814 - val_loss: 0.5301 - val_accuracy: 0.8398\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.99089\n",
            "Epoch 85/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0443 - accuracy: 0.9832 - val_loss: 0.1225 - val_accuracy: 0.9555\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.99089\n",
            "Epoch 86/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0386 - accuracy: 0.9865 - val_loss: 0.1548 - val_accuracy: 0.9461\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.99089\n",
            "Epoch 87/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0472 - accuracy: 0.9820 - val_loss: 0.1238 - val_accuracy: 0.9445\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.99089\n",
            "Epoch 88/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0600 - accuracy: 0.9777 - val_loss: 1.7085 - val_accuracy: 0.7070\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.99089\n",
            "Epoch 89/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0432 - accuracy: 0.9850 - val_loss: 0.3562 - val_accuracy: 0.8703\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.99089\n",
            "Epoch 90/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0376 - accuracy: 0.9871 - val_loss: 1.3371 - val_accuracy: 0.7141\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.99089\n",
            "Epoch 91/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0463 - accuracy: 0.9824 - val_loss: 0.3424 - val_accuracy: 0.8797\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.99089\n",
            "Epoch 92/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0456 - accuracy: 0.9824 - val_loss: 0.2996 - val_accuracy: 0.8906\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.99089\n",
            "Epoch 93/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0675 - accuracy: 0.9764 - val_loss: 3.6613 - val_accuracy: 0.4477\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.99089\n",
            "Epoch 94/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0584 - accuracy: 0.9762 - val_loss: 1.0299 - val_accuracy: 0.7094\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.99089\n",
            "Epoch 95/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0449 - accuracy: 0.9840 - val_loss: 0.1646 - val_accuracy: 0.9281\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.99089\n",
            "Epoch 96/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0425 - accuracy: 0.9846 - val_loss: 0.1718 - val_accuracy: 0.9422\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.99089\n",
            "Epoch 97/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0305 - accuracy: 0.9883 - val_loss: 0.6052 - val_accuracy: 0.8320\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.99089\n",
            "Epoch 98/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0318 - accuracy: 0.9873 - val_loss: 1.0738 - val_accuracy: 0.7828\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.99089\n",
            "Epoch 99/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0303 - accuracy: 0.9896 - val_loss: 0.1211 - val_accuracy: 0.9617\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.99089\n",
            "Epoch 100/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0301 - accuracy: 0.9887 - val_loss: 0.1244 - val_accuracy: 0.9555\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.99089\n",
            "Epoch 101/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0282 - accuracy: 0.9889 - val_loss: 0.0798 - val_accuracy: 0.9672\n",
            "\n",
            "Epoch 00101: val_accuracy did not improve from 0.99089\n",
            "Epoch 102/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0248 - accuracy: 0.9914 - val_loss: 0.3183 - val_accuracy: 0.8852\n",
            "\n",
            "Epoch 00102: val_accuracy did not improve from 0.99089\n",
            "Epoch 103/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0271 - accuracy: 0.9910 - val_loss: 0.0769 - val_accuracy: 0.9695\n",
            "\n",
            "Epoch 00103: val_accuracy did not improve from 0.99089\n",
            "Epoch 104/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0231 - accuracy: 0.9914 - val_loss: 0.3182 - val_accuracy: 0.8992\n",
            "\n",
            "Epoch 00104: val_accuracy did not improve from 0.99089\n",
            "Epoch 105/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0296 - accuracy: 0.9889 - val_loss: 0.9095 - val_accuracy: 0.7656\n",
            "\n",
            "Epoch 00105: val_accuracy did not improve from 0.99089\n",
            "Epoch 106/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0442 - accuracy: 0.9852 - val_loss: 0.1101 - val_accuracy: 0.9742\n",
            "\n",
            "Epoch 00106: val_accuracy did not improve from 0.99089\n",
            "Epoch 107/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0245 - accuracy: 0.9902 - val_loss: 0.0559 - val_accuracy: 0.9852\n",
            "\n",
            "Epoch 00107: val_accuracy did not improve from 0.99089\n",
            "Epoch 108/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.0218 - accuracy: 0.9924 - val_loss: 0.0478 - val_accuracy: 0.9859\n",
            "\n",
            "Epoch 00108: val_accuracy did not improve from 0.99089\n",
            "Epoch 109/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0282 - accuracy: 0.9887 - val_loss: 0.0634 - val_accuracy: 0.9812\n",
            "\n",
            "Epoch 00109: val_accuracy did not improve from 0.99089\n",
            "Epoch 110/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0175 - accuracy: 0.9941 - val_loss: 0.0662 - val_accuracy: 0.9727\n",
            "\n",
            "Epoch 00110: val_accuracy did not improve from 0.99089\n",
            "Epoch 111/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0196 - accuracy: 0.9926 - val_loss: 0.1502 - val_accuracy: 0.9469\n",
            "\n",
            "Epoch 00111: val_accuracy did not improve from 0.99089\n",
            "Epoch 112/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.0245 - accuracy: 0.9918 - val_loss: 0.0456 - val_accuracy: 0.9828\n",
            "\n",
            "Epoch 00112: val_accuracy did not improve from 0.99089\n",
            "Epoch 113/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.0199 - accuracy: 0.9928 - val_loss: 0.3215 - val_accuracy: 0.9055\n",
            "\n",
            "Epoch 00113: val_accuracy did not improve from 0.99089\n",
            "Epoch 114/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0172 - accuracy: 0.9939 - val_loss: 0.1676 - val_accuracy: 0.9406\n",
            "\n",
            "Epoch 00114: val_accuracy did not improve from 0.99089\n",
            "Epoch 115/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.0135 - accuracy: 0.9949 - val_loss: 0.0400 - val_accuracy: 0.9891\n",
            "\n",
            "Epoch 00115: val_accuracy did not improve from 0.99089\n",
            "Epoch 116/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.0134 - accuracy: 0.9949 - val_loss: 0.0515 - val_accuracy: 0.9781\n",
            "\n",
            "Epoch 00116: val_accuracy did not improve from 0.99089\n",
            "Epoch 117/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.0128 - accuracy: 0.9947 - val_loss: 0.5007 - val_accuracy: 0.8555\n",
            "\n",
            "Epoch 00117: val_accuracy did not improve from 0.99089\n",
            "Epoch 118/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.0453 - accuracy: 0.9846 - val_loss: 0.7669 - val_accuracy: 0.8555\n",
            "\n",
            "Epoch 00118: val_accuracy did not improve from 0.99089\n",
            "Epoch 119/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0196 - accuracy: 0.9936 - val_loss: 0.0641 - val_accuracy: 0.9727\n",
            "\n",
            "Epoch 00119: val_accuracy did not improve from 0.99089\n",
            "Epoch 120/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.0190 - accuracy: 0.9945 - val_loss: 0.3269 - val_accuracy: 0.9023\n",
            "\n",
            "Epoch 00120: val_accuracy did not improve from 0.99089\n",
            "Epoch 121/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0164 - accuracy: 0.9949 - val_loss: 0.7970 - val_accuracy: 0.7547\n",
            "\n",
            "Epoch 00121: val_accuracy did not improve from 0.99089\n",
            "Epoch 122/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.0123 - accuracy: 0.9961 - val_loss: 0.0558 - val_accuracy: 0.9852\n",
            "\n",
            "Epoch 00122: val_accuracy did not improve from 0.99089\n",
            "Epoch 123/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0151 - accuracy: 0.9949 - val_loss: 0.0460 - val_accuracy: 0.9836\n",
            "\n",
            "Epoch 00123: val_accuracy did not improve from 0.99089\n",
            "Epoch 124/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0099 - accuracy: 0.9973 - val_loss: 0.0623 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00124: val_accuracy did not improve from 0.99089\n",
            "Epoch 125/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0113 - accuracy: 0.9959 - val_loss: 0.0426 - val_accuracy: 0.9875\n",
            "\n",
            "Epoch 00125: val_accuracy did not improve from 0.99089\n",
            "Epoch 126/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0158 - accuracy: 0.9934 - val_loss: 0.0578 - val_accuracy: 0.9836\n",
            "\n",
            "Epoch 00126: val_accuracy did not improve from 0.99089\n",
            "Epoch 127/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0228 - accuracy: 0.9928 - val_loss: 0.2823 - val_accuracy: 0.9031\n",
            "\n",
            "Epoch 00127: val_accuracy did not improve from 0.99089\n",
            "Epoch 128/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0178 - accuracy: 0.9936 - val_loss: 0.2306 - val_accuracy: 0.9109\n",
            "\n",
            "Epoch 00128: val_accuracy did not improve from 0.99089\n",
            "Epoch 129/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.0078 - accuracy: 0.9979 - val_loss: 0.8036 - val_accuracy: 0.7695\n",
            "\n",
            "Epoch 00129: val_accuracy did not improve from 0.99089\n",
            "Epoch 130/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0139 - accuracy: 0.9955 - val_loss: 0.0430 - val_accuracy: 0.9859\n",
            "\n",
            "Epoch 00130: val_accuracy did not improve from 0.99089\n",
            "Epoch 131/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0488 - accuracy: 0.9859 - val_loss: 0.6528 - val_accuracy: 0.7734\n",
            "\n",
            "Epoch 00131: val_accuracy did not improve from 0.99089\n",
            "Epoch 132/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.0458 - accuracy: 0.9844 - val_loss: 0.0647 - val_accuracy: 0.9781\n",
            "\n",
            "Epoch 00132: val_accuracy did not improve from 0.99089\n",
            "Epoch 133/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0209 - accuracy: 0.9916 - val_loss: 0.0490 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00133: val_accuracy did not improve from 0.99089\n",
            "Epoch 134/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0096 - accuracy: 0.9971 - val_loss: 0.0337 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00134: val_accuracy did not improve from 0.99089\n",
            "Epoch 135/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0080 - accuracy: 0.9982 - val_loss: 0.0287 - val_accuracy: 0.9898\n",
            "\n",
            "Epoch 00135: val_accuracy did not improve from 0.99089\n",
            "Epoch 136/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0079 - accuracy: 0.9977 - val_loss: 0.0587 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00136: val_accuracy did not improve from 0.99089\n",
            "Epoch 137/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0075 - accuracy: 0.9984 - val_loss: 0.0680 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00137: val_accuracy did not improve from 0.99089\n",
            "Epoch 138/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0081 - accuracy: 0.9979 - val_loss: 0.0656 - val_accuracy: 0.9828\n",
            "\n",
            "Epoch 00138: val_accuracy did not improve from 0.99089\n",
            "Epoch 139/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0117 - accuracy: 0.9975 - val_loss: 0.0358 - val_accuracy: 0.9906\n",
            "\n",
            "Epoch 00139: val_accuracy did not improve from 0.99089\n",
            "Epoch 140/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0070 - accuracy: 0.9980 - val_loss: 0.0557 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00140: val_accuracy did not improve from 0.99089\n",
            "Epoch 141/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0051 - accuracy: 0.9988 - val_loss: 0.0589 - val_accuracy: 0.9781\n",
            "\n",
            "Epoch 00141: val_accuracy did not improve from 0.99089\n",
            "Epoch 142/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0064 - accuracy: 0.9982 - val_loss: 0.4724 - val_accuracy: 0.8844\n",
            "\n",
            "Epoch 00142: val_accuracy did not improve from 0.99089\n",
            "Epoch 143/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0077 - accuracy: 0.9980 - val_loss: 0.3407 - val_accuracy: 0.8531\n",
            "\n",
            "Epoch 00143: val_accuracy did not improve from 0.99089\n",
            "Epoch 144/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0041 - accuracy: 0.9992 - val_loss: 0.0818 - val_accuracy: 0.9742\n",
            "\n",
            "Epoch 00144: val_accuracy did not improve from 0.99089\n",
            "Epoch 145/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0053 - accuracy: 0.9982 - val_loss: 0.0444 - val_accuracy: 0.9859\n",
            "\n",
            "Epoch 00145: val_accuracy did not improve from 0.99089\n",
            "Epoch 146/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0055 - accuracy: 0.9980 - val_loss: 0.0370 - val_accuracy: 0.9891\n",
            "\n",
            "Epoch 00146: val_accuracy did not improve from 0.99089\n",
            "Epoch 147/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.1858 - val_accuracy: 0.9258\n",
            "\n",
            "Epoch 00147: val_accuracy did not improve from 0.99089\n",
            "Epoch 148/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0553 - val_accuracy: 0.9820\n",
            "\n",
            "Epoch 00148: val_accuracy did not improve from 0.99089\n",
            "Epoch 149/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0049 - accuracy: 0.9988 - val_loss: 0.0322 - val_accuracy: 0.9898\n",
            "\n",
            "Epoch 00149: val_accuracy did not improve from 0.99089\n",
            "Epoch 150/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.6071 - val_accuracy: 0.7906\n",
            "\n",
            "Epoch 00150: val_accuracy did not improve from 0.99089\n",
            "Epoch 151/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0052 - accuracy: 0.9982 - val_loss: 0.0298 - val_accuracy: 0.9906\n",
            "\n",
            "Epoch 00151: val_accuracy did not improve from 0.99089\n",
            "Epoch 152/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0028 - accuracy: 0.9996 - val_loss: 0.0304 - val_accuracy: 0.9898\n",
            "\n",
            "Epoch 00152: val_accuracy did not improve from 0.99089\n",
            "Epoch 153/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 0.0407 - val_accuracy: 0.9867\n",
            "\n",
            "Epoch 00153: val_accuracy did not improve from 0.99089\n",
            "Epoch 154/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 0.0794 - val_accuracy: 0.9773\n",
            "\n",
            "Epoch 00154: val_accuracy did not improve from 0.99089\n",
            "Epoch 155/500\n",
            "160/160 [==============================] - 1s 8ms/step - loss: 0.0133 - accuracy: 0.9959 - val_loss: 0.1130 - val_accuracy: 0.9570\n",
            "\n",
            "Epoch 00155: val_accuracy did not improve from 0.99089\n",
            "Epoch 156/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0070 - accuracy: 0.9979 - val_loss: 0.0601 - val_accuracy: 0.9859\n",
            "\n",
            "Epoch 00156: val_accuracy did not improve from 0.99089\n",
            "Epoch 157/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0028 - accuracy: 0.9998 - val_loss: 0.0400 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00157: val_accuracy did not improve from 0.99089\n",
            "Epoch 158/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 0.1887 - val_accuracy: 0.9211\n",
            "\n",
            "Epoch 00158: val_accuracy did not improve from 0.99089\n",
            "Epoch 159/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0048 - accuracy: 0.9990 - val_loss: 0.0307 - val_accuracy: 0.9930\n",
            "\n",
            "Epoch 00159: val_accuracy improved from 0.99089 to 0.99297, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 160/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.0024 - accuracy: 0.9998 - val_loss: 0.0311 - val_accuracy: 0.9891\n",
            "\n",
            "Epoch 00160: val_accuracy did not improve from 0.99297\n",
            "Epoch 161/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0520 - val_accuracy: 0.9836\n",
            "\n",
            "Epoch 00161: val_accuracy did not improve from 0.99297\n",
            "Epoch 162/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0906 - val_accuracy: 0.9703\n",
            "\n",
            "Epoch 00162: val_accuracy did not improve from 0.99297\n",
            "Epoch 163/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0049 - accuracy: 0.9986 - val_loss: 6.5525 - val_accuracy: 0.5781\n",
            "\n",
            "Epoch 00163: val_accuracy did not improve from 0.99297\n",
            "Epoch 164/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0895 - accuracy: 0.9750 - val_loss: 2.2268 - val_accuracy: 0.6250\n",
            "\n",
            "Epoch 00164: val_accuracy did not improve from 0.99297\n",
            "Epoch 165/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.0300 - accuracy: 0.9902 - val_loss: 1.3242 - val_accuracy: 0.7234\n",
            "\n",
            "Epoch 00165: val_accuracy did not improve from 0.99297\n",
            "Epoch 166/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.6400 - accuracy: 0.7902 - val_loss: 3.0624 - val_accuracy: 0.3844\n",
            "\n",
            "Epoch 00166: val_accuracy did not improve from 0.99297\n",
            "Epoch 167/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.5439 - accuracy: 0.7863 - val_loss: 3.6419 - val_accuracy: 0.2992\n",
            "\n",
            "Epoch 00167: val_accuracy did not improve from 0.99297\n",
            "Epoch 168/500\n",
            "160/160 [==============================] - 1s 8ms/step - loss: 0.2949 - accuracy: 0.8828 - val_loss: 0.6640 - val_accuracy: 0.6789\n",
            "\n",
            "Epoch 00168: val_accuracy did not improve from 0.99297\n",
            "Epoch 169/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.2180 - accuracy: 0.9180 - val_loss: 0.5630 - val_accuracy: 0.8117\n",
            "\n",
            "Epoch 00169: val_accuracy did not improve from 0.99297\n",
            "Epoch 170/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.2777 - accuracy: 0.8969 - val_loss: 1.3502 - val_accuracy: 0.5523\n",
            "\n",
            "Epoch 00170: val_accuracy did not improve from 0.99297\n",
            "Epoch 171/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.3743 - accuracy: 0.8633 - val_loss: 0.5475 - val_accuracy: 0.7617\n",
            "\n",
            "Epoch 00171: val_accuracy did not improve from 0.99297\n",
            "Epoch 172/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.1864 - accuracy: 0.9322 - val_loss: 1.0373 - val_accuracy: 0.6352\n",
            "\n",
            "Epoch 00172: val_accuracy did not improve from 0.99297\n",
            "Epoch 173/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.2605 - accuracy: 0.9047 - val_loss: 4.2313 - val_accuracy: 0.4703\n",
            "\n",
            "Epoch 00173: val_accuracy did not improve from 0.99297\n",
            "Epoch 174/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.1425 - accuracy: 0.9482 - val_loss: 0.2542 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00174: val_accuracy did not improve from 0.99297\n",
            "Epoch 175/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0915 - accuracy: 0.9654 - val_loss: 0.1688 - val_accuracy: 0.9500\n",
            "\n",
            "Epoch 00175: val_accuracy did not improve from 0.99297\n",
            "Epoch 176/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0660 - accuracy: 0.9777 - val_loss: 0.7316 - val_accuracy: 0.8031\n",
            "\n",
            "Epoch 00176: val_accuracy did not improve from 0.99297\n",
            "Epoch 177/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0563 - accuracy: 0.9799 - val_loss: 0.1009 - val_accuracy: 0.9711\n",
            "\n",
            "Epoch 00177: val_accuracy did not improve from 0.99297\n",
            "Epoch 178/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.0786 - accuracy: 0.9732 - val_loss: 0.2529 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00178: val_accuracy did not improve from 0.99297\n",
            "Epoch 179/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0568 - accuracy: 0.9809 - val_loss: 0.1091 - val_accuracy: 0.9563\n",
            "\n",
            "Epoch 00179: val_accuracy did not improve from 0.99297\n",
            "Epoch 180/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0449 - accuracy: 0.9832 - val_loss: 0.1509 - val_accuracy: 0.9430\n",
            "\n",
            "Epoch 00180: val_accuracy did not improve from 0.99297\n",
            "Epoch 181/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0333 - accuracy: 0.9889 - val_loss: 0.0667 - val_accuracy: 0.9812\n",
            "\n",
            "Epoch 00181: val_accuracy did not improve from 0.99297\n",
            "Epoch 182/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.0674 - accuracy: 0.9766 - val_loss: 0.0921 - val_accuracy: 0.9648\n",
            "\n",
            "Epoch 00182: val_accuracy did not improve from 0.99297\n",
            "Epoch 183/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0423 - accuracy: 0.9836 - val_loss: 0.0984 - val_accuracy: 0.9703\n",
            "\n",
            "Epoch 00183: val_accuracy did not improve from 0.99297\n",
            "Epoch 184/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0337 - accuracy: 0.9867 - val_loss: 0.2465 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00184: val_accuracy did not improve from 0.99297\n",
            "Epoch 185/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0297 - accuracy: 0.9885 - val_loss: 0.1495 - val_accuracy: 0.9383\n",
            "\n",
            "Epoch 00185: val_accuracy did not improve from 0.99297\n",
            "Epoch 186/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0362 - accuracy: 0.9869 - val_loss: 0.2866 - val_accuracy: 0.9117\n",
            "\n",
            "Epoch 00186: val_accuracy did not improve from 0.99297\n",
            "Epoch 187/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0253 - accuracy: 0.9916 - val_loss: 0.1912 - val_accuracy: 0.9258\n",
            "\n",
            "Epoch 00187: val_accuracy did not improve from 0.99297\n",
            "Epoch 188/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0380 - accuracy: 0.9873 - val_loss: 0.0745 - val_accuracy: 0.9734\n",
            "\n",
            "Epoch 00188: val_accuracy did not improve from 0.99297\n",
            "Epoch 189/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.0320 - accuracy: 0.9891 - val_loss: 0.0716 - val_accuracy: 0.9750\n",
            "\n",
            "Epoch 00189: val_accuracy did not improve from 0.99297\n",
            "Epoch 190/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0306 - accuracy: 0.9885 - val_loss: 0.0992 - val_accuracy: 0.9641\n",
            "\n",
            "Epoch 00190: val_accuracy did not improve from 0.99297\n",
            "Epoch 191/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0216 - accuracy: 0.9912 - val_loss: 0.2343 - val_accuracy: 0.9187\n",
            "\n",
            "Epoch 00191: val_accuracy did not improve from 0.99297\n",
            "Epoch 192/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.0166 - accuracy: 0.9957 - val_loss: 0.0533 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00192: val_accuracy did not improve from 0.99297\n",
            "Epoch 193/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0147 - accuracy: 0.9949 - val_loss: 0.0517 - val_accuracy: 0.9812\n",
            "\n",
            "Epoch 00193: val_accuracy did not improve from 0.99297\n",
            "Epoch 194/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0147 - accuracy: 0.9963 - val_loss: 0.0384 - val_accuracy: 0.9875\n",
            "\n",
            "Epoch 00194: val_accuracy did not improve from 0.99297\n",
            "Epoch 195/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0320 - accuracy: 0.9891 - val_loss: 0.1826 - val_accuracy: 0.9289\n",
            "\n",
            "Epoch 00195: val_accuracy did not improve from 0.99297\n",
            "Epoch 196/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0191 - accuracy: 0.9941 - val_loss: 0.2824 - val_accuracy: 0.8844\n",
            "\n",
            "Epoch 00196: val_accuracy did not improve from 0.99297\n",
            "Epoch 197/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0150 - accuracy: 0.9957 - val_loss: 0.0534 - val_accuracy: 0.9812\n",
            "\n",
            "Epoch 00197: val_accuracy did not improve from 0.99297\n",
            "Epoch 198/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0133 - accuracy: 0.9959 - val_loss: 0.0723 - val_accuracy: 0.9719\n",
            "\n",
            "Epoch 00198: val_accuracy did not improve from 0.99297\n",
            "Epoch 199/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.0208 - accuracy: 0.9936 - val_loss: 0.3352 - val_accuracy: 0.8711\n",
            "\n",
            "Epoch 00199: val_accuracy did not improve from 0.99297\n",
            "Epoch 200/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0514 - accuracy: 0.9826 - val_loss: 0.9109 - val_accuracy: 0.6492\n",
            "\n",
            "Epoch 00200: val_accuracy did not improve from 0.99297\n",
            "Epoch 201/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0364 - accuracy: 0.9871 - val_loss: 0.3643 - val_accuracy: 0.8828\n",
            "\n",
            "Epoch 00201: val_accuracy did not improve from 0.99297\n",
            "Epoch 202/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.0221 - accuracy: 0.9916 - val_loss: 0.2581 - val_accuracy: 0.9117\n",
            "\n",
            "Epoch 00202: val_accuracy did not improve from 0.99297\n",
            "Epoch 203/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0388 - accuracy: 0.9863 - val_loss: 0.0930 - val_accuracy: 0.9664\n",
            "\n",
            "Epoch 00203: val_accuracy did not improve from 0.99297\n",
            "Epoch 204/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0124 - accuracy: 0.9973 - val_loss: 0.0818 - val_accuracy: 0.9703\n",
            "\n",
            "Epoch 00204: val_accuracy did not improve from 0.99297\n",
            "Epoch 205/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0104 - accuracy: 0.9973 - val_loss: 0.0484 - val_accuracy: 0.9828\n",
            "\n",
            "Epoch 00205: val_accuracy did not improve from 0.99297\n",
            "Epoch 206/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0084 - accuracy: 0.9977 - val_loss: 0.0400 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00206: val_accuracy did not improve from 0.99297\n",
            "Epoch 207/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0092 - accuracy: 0.9982 - val_loss: 0.0655 - val_accuracy: 0.9773\n",
            "\n",
            "Epoch 00207: val_accuracy did not improve from 0.99297\n",
            "Epoch 208/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0064 - accuracy: 0.9988 - val_loss: 0.0329 - val_accuracy: 0.9898\n",
            "\n",
            "Epoch 00208: val_accuracy did not improve from 0.99297\n",
            "Epoch 209/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0083 - accuracy: 0.9977 - val_loss: 0.0287 - val_accuracy: 0.9914\n",
            "\n",
            "Epoch 00209: val_accuracy did not improve from 0.99297\n",
            "Epoch 210/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0049 - accuracy: 0.9984 - val_loss: 0.0681 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00210: val_accuracy did not improve from 0.99297\n",
            "Epoch 211/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0058 - accuracy: 0.9982 - val_loss: 0.0490 - val_accuracy: 0.9836\n",
            "\n",
            "Epoch 00211: val_accuracy did not improve from 0.99297\n",
            "Epoch 212/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0111 - accuracy: 0.9967 - val_loss: 0.0733 - val_accuracy: 0.9727\n",
            "\n",
            "Epoch 00212: val_accuracy did not improve from 0.99297\n",
            "Epoch 213/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0157 - accuracy: 0.9959 - val_loss: 0.0769 - val_accuracy: 0.9789\n",
            "\n",
            "Epoch 00213: val_accuracy did not improve from 0.99297\n",
            "Epoch 214/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0195 - accuracy: 0.9932 - val_loss: 0.0752 - val_accuracy: 0.9758\n",
            "\n",
            "Epoch 00214: val_accuracy did not improve from 0.99297\n",
            "Epoch 215/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.0091 - accuracy: 0.9975 - val_loss: 0.1273 - val_accuracy: 0.9406\n",
            "\n",
            "Epoch 00215: val_accuracy did not improve from 0.99297\n",
            "Epoch 216/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0112 - accuracy: 0.9967 - val_loss: 0.0404 - val_accuracy: 0.9828\n",
            "\n",
            "Epoch 00216: val_accuracy did not improve from 0.99297\n",
            "Epoch 217/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0076 - accuracy: 0.9984 - val_loss: 0.0919 - val_accuracy: 0.9711\n",
            "\n",
            "Epoch 00217: val_accuracy did not improve from 0.99297\n",
            "Epoch 218/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0052 - accuracy: 0.9986 - val_loss: 0.0598 - val_accuracy: 0.9820\n",
            "\n",
            "Epoch 00218: val_accuracy did not improve from 0.99297\n",
            "Epoch 219/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0043 - accuracy: 0.9990 - val_loss: 0.0316 - val_accuracy: 0.9914\n",
            "\n",
            "Epoch 00219: val_accuracy did not improve from 0.99297\n",
            "Epoch 220/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0089 - accuracy: 0.9969 - val_loss: 0.1743 - val_accuracy: 0.9539\n",
            "\n",
            "Epoch 00220: val_accuracy did not improve from 0.99297\n",
            "Epoch 221/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0116 - accuracy: 0.9951 - val_loss: 0.0349 - val_accuracy: 0.9898\n",
            "\n",
            "Epoch 00221: val_accuracy did not improve from 0.99297\n",
            "Epoch 222/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0054 - accuracy: 0.9988 - val_loss: 0.0383 - val_accuracy: 0.9859\n",
            "\n",
            "Epoch 00222: val_accuracy did not improve from 0.99297\n",
            "Epoch 223/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0231 - accuracy: 0.9910 - val_loss: 0.0541 - val_accuracy: 0.9781\n",
            "\n",
            "Epoch 00223: val_accuracy did not improve from 0.99297\n",
            "Epoch 224/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0091 - accuracy: 0.9977 - val_loss: 0.1049 - val_accuracy: 0.9641\n",
            "\n",
            "Epoch 00224: val_accuracy did not improve from 0.99297\n",
            "Epoch 225/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0079 - accuracy: 0.9982 - val_loss: 0.3956 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00225: val_accuracy did not improve from 0.99297\n",
            "Epoch 226/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0185 - accuracy: 0.9939 - val_loss: 0.0825 - val_accuracy: 0.9734\n",
            "\n",
            "Epoch 00226: val_accuracy did not improve from 0.99297\n",
            "Epoch 227/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0060 - accuracy: 0.9990 - val_loss: 0.0473 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00227: val_accuracy did not improve from 0.99297\n",
            "Epoch 228/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.0340 - accuracy: 0.9902 - val_loss: 0.1143 - val_accuracy: 0.9602\n",
            "\n",
            "Epoch 00228: val_accuracy did not improve from 0.99297\n",
            "Epoch 229/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0082 - accuracy: 0.9977 - val_loss: 0.0392 - val_accuracy: 0.9891\n",
            "\n",
            "Epoch 00229: val_accuracy did not improve from 0.99297\n",
            "Epoch 230/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0076 - accuracy: 0.9973 - val_loss: 0.0540 - val_accuracy: 0.9852\n",
            "\n",
            "Epoch 00230: val_accuracy did not improve from 0.99297\n",
            "Epoch 231/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0095 - accuracy: 0.9969 - val_loss: 0.0705 - val_accuracy: 0.9812\n",
            "\n",
            "Epoch 00231: val_accuracy did not improve from 0.99297\n",
            "Epoch 232/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0062 - accuracy: 0.9994 - val_loss: 0.0387 - val_accuracy: 0.9875\n",
            "\n",
            "Epoch 00232: val_accuracy did not improve from 0.99297\n",
            "Epoch 233/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0082 - accuracy: 0.9971 - val_loss: 0.1017 - val_accuracy: 0.9727\n",
            "\n",
            "Epoch 00233: val_accuracy did not improve from 0.99297\n",
            "Epoch 234/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0260 - accuracy: 0.9908 - val_loss: 1.6589 - val_accuracy: 0.6516\n",
            "\n",
            "Epoch 00234: val_accuracy did not improve from 0.99297\n",
            "Epoch 235/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0418 - accuracy: 0.9848 - val_loss: 0.6723 - val_accuracy: 0.8258\n",
            "\n",
            "Epoch 00235: val_accuracy did not improve from 0.99297\n",
            "Epoch 236/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0494 - accuracy: 0.9854 - val_loss: 0.4662 - val_accuracy: 0.8508\n",
            "\n",
            "Epoch 00236: val_accuracy did not improve from 0.99297\n",
            "Epoch 237/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0569 - accuracy: 0.9789 - val_loss: 0.1579 - val_accuracy: 0.9523\n",
            "\n",
            "Epoch 00237: val_accuracy did not improve from 0.99297\n",
            "Epoch 238/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0293 - accuracy: 0.9891 - val_loss: 0.2940 - val_accuracy: 0.8789\n",
            "\n",
            "Epoch 00238: val_accuracy did not improve from 0.99297\n",
            "Epoch 239/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.0174 - accuracy: 0.9941 - val_loss: 0.5605 - val_accuracy: 0.8305\n",
            "\n",
            "Epoch 00239: val_accuracy did not improve from 0.99297\n",
            "Epoch 240/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0199 - accuracy: 0.9943 - val_loss: 0.0966 - val_accuracy: 0.9680\n",
            "\n",
            "Epoch 00240: val_accuracy did not improve from 0.99297\n",
            "Epoch 241/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0088 - accuracy: 0.9965 - val_loss: 0.0445 - val_accuracy: 0.9859\n",
            "\n",
            "Epoch 00241: val_accuracy did not improve from 0.99297\n",
            "Epoch 242/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0148 - accuracy: 0.9949 - val_loss: 0.5134 - val_accuracy: 0.8211\n",
            "\n",
            "Epoch 00242: val_accuracy did not improve from 0.99297\n",
            "Epoch 243/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0165 - accuracy: 0.9951 - val_loss: 0.1087 - val_accuracy: 0.9594\n",
            "\n",
            "Epoch 00243: val_accuracy did not improve from 0.99297\n",
            "Epoch 244/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0232 - accuracy: 0.9916 - val_loss: 0.0786 - val_accuracy: 0.9773\n",
            "\n",
            "Epoch 00244: val_accuracy did not improve from 0.99297\n",
            "Epoch 245/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0257 - accuracy: 0.9922 - val_loss: 0.0815 - val_accuracy: 0.9773\n",
            "\n",
            "Epoch 00245: val_accuracy did not improve from 0.99297\n",
            "Epoch 246/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0078 - accuracy: 0.9979 - val_loss: 0.1292 - val_accuracy: 0.9492\n",
            "\n",
            "Epoch 00246: val_accuracy did not improve from 0.99297\n",
            "Epoch 247/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.0072 - accuracy: 0.9986 - val_loss: 0.0583 - val_accuracy: 0.9812\n",
            "\n",
            "Epoch 00247: val_accuracy did not improve from 0.99297\n",
            "Epoch 248/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0047 - accuracy: 0.9990 - val_loss: 0.0740 - val_accuracy: 0.9781\n",
            "\n",
            "Epoch 00248: val_accuracy did not improve from 0.99297\n",
            "Epoch 249/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0108 - accuracy: 0.9953 - val_loss: 0.0555 - val_accuracy: 0.9828\n",
            "\n",
            "Epoch 00249: val_accuracy did not improve from 0.99297\n",
            "Epoch 250/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0058 - accuracy: 0.9984 - val_loss: 0.0505 - val_accuracy: 0.9812\n",
            "\n",
            "Epoch 00250: val_accuracy did not improve from 0.99297\n",
            "Epoch 251/500\n",
            "160/160 [==============================] - 1s 7ms/step - loss: 0.0050 - accuracy: 0.9986 - val_loss: 0.2307 - val_accuracy: 0.9391\n",
            "\n",
            "Epoch 00251: val_accuracy did not improve from 0.99297\n",
            "Epoch 252/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0071 - accuracy: 0.9980 - val_loss: 1.9079 - val_accuracy: 0.5570\n",
            "\n",
            "Epoch 00252: val_accuracy did not improve from 0.99297\n",
            "Epoch 253/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0075 - accuracy: 0.9979 - val_loss: 0.0893 - val_accuracy: 0.9703\n",
            "\n",
            "Epoch 00253: val_accuracy did not improve from 0.99297\n",
            "Epoch 254/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0054 - accuracy: 0.9984 - val_loss: 0.1927 - val_accuracy: 0.9258\n",
            "\n",
            "Epoch 00254: val_accuracy did not improve from 0.99297\n",
            "Epoch 255/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0033 - accuracy: 0.9992 - val_loss: 0.0389 - val_accuracy: 0.9898\n",
            "\n",
            "Epoch 00255: val_accuracy did not improve from 0.99297\n",
            "Epoch 256/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0075 - accuracy: 0.9971 - val_loss: 0.0451 - val_accuracy: 0.9852\n",
            "\n",
            "Epoch 00256: val_accuracy did not improve from 0.99297\n",
            "Epoch 257/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.0455 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00257: val_accuracy did not improve from 0.99297\n",
            "Epoch 258/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.0441 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00258: val_accuracy did not improve from 0.99297\n",
            "Epoch 259/500\n",
            "160/160 [==============================] - 1s 6ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.0360 - val_accuracy: 0.9914\n",
            "\n",
            "Epoch 00259: val_accuracy did not improve from 0.99297\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00259: early stopping\n",
            "Training/validation time was: 386.44 seconds\n",
            "\n",
            " \n",
            "EVALUATING THE MODEL:\n",
            "50/50 [==============================] - 0s 3ms/step - loss: 0.0316 - accuracy: 0.9919\n",
            "Evaluate time was 0.182516 seconds\n",
            "0.9918749928474426\n",
            "\n",
            "SUMMARY: \n",
            "With 500 cases, the max val_accuracy was: 99.2969 % and the test_accuracy was: 99.1875 %. \n",
            "It is a difference of 0.1094 %.\n",
            "Fitness history is: [1, 0.987500011920929, 0.9840909242630005] \n",
            "Cases history is: [600, 550, 500] \n",
            "acc_val history is: [0.9908854365348816, 0.9857954382896423, 0.992968738079071] \n",
            "acc_test history is: [0.987500011920929, 0.9840909242630005, 0.9918749928474426] \n",
            "Times history is: [925.6832382678986, 643.7173902988434, 386.4396514892578]\n",
            "\n",
            "---> ITERATING NOW WITH: 450 CASES ! ( 75.00 % FROM TOTAL ) \n",
            "----> AND THE HISTORY OF CASES DECREASE IS: [600, 550, 500] \n",
            " \n",
            "TRAINING AND VALIDATING THE MODEL: \n",
            "It will take a while ;D...\n",
            "Epoch 1/500\n",
            "144/144 [==============================] - 2s 9ms/step - loss: 1.1193 - accuracy: 0.5486 - val_loss: 2.7886 - val_accuracy: 0.1276\n",
            "\n",
            "Epoch 00001: val_accuracy did not improve from 0.99297\n",
            "Epoch 2/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.8376 - accuracy: 0.6393 - val_loss: 2.5938 - val_accuracy: 0.2101\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.99297\n",
            "Epoch 3/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.7544 - accuracy: 0.6879 - val_loss: 2.3023 - val_accuracy: 0.3238\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.99297\n",
            "Epoch 4/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.6776 - accuracy: 0.7155 - val_loss: 1.7600 - val_accuracy: 0.3333\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.99297\n",
            "Epoch 5/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.4951 - accuracy: 0.7856 - val_loss: 0.6112 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.99297\n",
            "Epoch 6/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.3548 - accuracy: 0.8615 - val_loss: 1.1894 - val_accuracy: 0.5512\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.99297\n",
            "Epoch 7/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.3783 - accuracy: 0.8355 - val_loss: 2.2599 - val_accuracy: 0.5773\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.99297\n",
            "Epoch 8/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5816 - accuracy: 0.7667 - val_loss: 1.4462 - val_accuracy: 0.4748\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.99297\n",
            "Epoch 9/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.7517 - accuracy: 0.6895 - val_loss: 3.7603 - val_accuracy: 0.1311\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.99297\n",
            "Epoch 10/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.6760 - accuracy: 0.7101 - val_loss: 1.6473 - val_accuracy: 0.3533\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.99297\n",
            "Epoch 11/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5952 - accuracy: 0.7554 - val_loss: 3.6125 - val_accuracy: 0.4106\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.99297\n",
            "Epoch 12/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.3847 - accuracy: 0.8392 - val_loss: 2.8734 - val_accuracy: 0.2665\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.99297\n",
            "Epoch 13/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.2524 - accuracy: 0.8898 - val_loss: 1.6557 - val_accuracy: 0.5998\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.99297\n",
            "Epoch 14/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.6450 - accuracy: 0.7216 - val_loss: 1.4850 - val_accuracy: 0.4210\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.99297\n",
            "Epoch 15/500\n",
            "144/144 [==============================] - 1s 8ms/step - loss: 0.7900 - accuracy: 0.6348 - val_loss: 0.9178 - val_accuracy: 0.5738\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.99297\n",
            "Epoch 16/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.7745 - accuracy: 0.6374 - val_loss: 0.9848 - val_accuracy: 0.5538\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.99297\n",
            "Epoch 17/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.7619 - accuracy: 0.6413 - val_loss: 0.7407 - val_accuracy: 0.6493\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.99297\n",
            "Epoch 18/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.7553 - accuracy: 0.6365 - val_loss: 0.7132 - val_accuracy: 0.6580\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.99297\n",
            "Epoch 19/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.7495 - accuracy: 0.6487 - val_loss: 0.7379 - val_accuracy: 0.6502\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.99297\n",
            "Epoch 20/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.7440 - accuracy: 0.6463 - val_loss: 0.8577 - val_accuracy: 0.5903\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.99297\n",
            "Epoch 21/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.7225 - accuracy: 0.6569 - val_loss: 0.7568 - val_accuracy: 0.6415\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.99297\n",
            "Epoch 22/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.7422 - accuracy: 0.6421 - val_loss: 0.7209 - val_accuracy: 0.6502\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.99297\n",
            "Epoch 23/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.7241 - accuracy: 0.6510 - val_loss: 0.7193 - val_accuracy: 0.6519\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.99297\n",
            "Epoch 24/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.7275 - accuracy: 0.6532 - val_loss: 0.6920 - val_accuracy: 0.6641\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.99297\n",
            "Epoch 25/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.7324 - accuracy: 0.6521 - val_loss: 0.6870 - val_accuracy: 0.6858\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.99297\n",
            "Epoch 26/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.7308 - accuracy: 0.6491 - val_loss: 0.6898 - val_accuracy: 0.6675\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.99297\n",
            "Epoch 27/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.7197 - accuracy: 0.6599 - val_loss: 0.6894 - val_accuracy: 0.6753\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.99297\n",
            "Epoch 28/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.7291 - accuracy: 0.6554 - val_loss: 0.7266 - val_accuracy: 0.6502\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.99297\n",
            "Epoch 29/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.7140 - accuracy: 0.6604 - val_loss: 0.6858 - val_accuracy: 0.6727\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.99297\n",
            "Epoch 30/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.7193 - accuracy: 0.6573 - val_loss: 1.1123 - val_accuracy: 0.5608\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.99297\n",
            "Epoch 31/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.7121 - accuracy: 0.6580 - val_loss: 0.6813 - val_accuracy: 0.6719\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.99297\n",
            "Epoch 32/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.7109 - accuracy: 0.6556 - val_loss: 0.9668 - val_accuracy: 0.5625\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.99297\n",
            "Epoch 33/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.7071 - accuracy: 0.6684 - val_loss: 0.7017 - val_accuracy: 0.6623\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.99297\n",
            "Epoch 34/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.6965 - accuracy: 0.6630 - val_loss: 0.6765 - val_accuracy: 0.6823\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.99297\n",
            "Epoch 35/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.7104 - accuracy: 0.6634 - val_loss: 0.6958 - val_accuracy: 0.6753\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.99297\n",
            "Epoch 36/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.6889 - accuracy: 0.6736 - val_loss: 0.6766 - val_accuracy: 0.6745\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.99297\n",
            "Epoch 37/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.7001 - accuracy: 0.6610 - val_loss: 0.6987 - val_accuracy: 0.6780\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.99297\n",
            "Epoch 38/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.6772 - accuracy: 0.6760 - val_loss: 0.6753 - val_accuracy: 0.6806\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.99297\n",
            "Epoch 39/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.6732 - accuracy: 0.6771 - val_loss: 0.6374 - val_accuracy: 0.6962\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.99297\n",
            "Epoch 40/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.6744 - accuracy: 0.6753 - val_loss: 0.6628 - val_accuracy: 0.7049\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.99297\n",
            "Epoch 41/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.6664 - accuracy: 0.6827 - val_loss: 0.6550 - val_accuracy: 0.6962\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.99297\n",
            "Epoch 42/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.6535 - accuracy: 0.6877 - val_loss: 0.6607 - val_accuracy: 0.6901\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.99297\n",
            "Epoch 43/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.6567 - accuracy: 0.6858 - val_loss: 0.6531 - val_accuracy: 0.6875\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.99297\n",
            "Epoch 44/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.6519 - accuracy: 0.6875 - val_loss: 0.6733 - val_accuracy: 0.6927\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.99297\n",
            "Epoch 45/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.6505 - accuracy: 0.6895 - val_loss: 0.8157 - val_accuracy: 0.6684\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.99297\n",
            "Epoch 46/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.6459 - accuracy: 0.6934 - val_loss: 0.9730 - val_accuracy: 0.5911\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.99297\n",
            "Epoch 47/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.6440 - accuracy: 0.6927 - val_loss: 0.6781 - val_accuracy: 0.6806\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.99297\n",
            "Epoch 48/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.6475 - accuracy: 0.6886 - val_loss: 0.6235 - val_accuracy: 0.7014\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.99297\n",
            "Epoch 49/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.6433 - accuracy: 0.6944 - val_loss: 0.6694 - val_accuracy: 0.6944\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.99297\n",
            "Epoch 50/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.6328 - accuracy: 0.7016 - val_loss: 0.6129 - val_accuracy: 0.7127\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.99297\n",
            "Epoch 51/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.6228 - accuracy: 0.7077 - val_loss: 0.6938 - val_accuracy: 0.6849\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.99297\n",
            "Epoch 52/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.6236 - accuracy: 0.6986 - val_loss: 1.3200 - val_accuracy: 0.4514\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.99297\n",
            "Epoch 53/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.6306 - accuracy: 0.6986 - val_loss: 0.7162 - val_accuracy: 0.6562\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.99297\n",
            "Epoch 54/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.6214 - accuracy: 0.7144 - val_loss: 0.6128 - val_accuracy: 0.7222\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.99297\n",
            "Epoch 55/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.6112 - accuracy: 0.7118 - val_loss: 0.6396 - val_accuracy: 0.6988\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.99297\n",
            "Epoch 56/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.6154 - accuracy: 0.7099 - val_loss: 0.5968 - val_accuracy: 0.7214\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.99297\n",
            "Epoch 57/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.6066 - accuracy: 0.7064 - val_loss: 0.6923 - val_accuracy: 0.6910\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.99297\n",
            "Epoch 58/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.6144 - accuracy: 0.7116 - val_loss: 0.5835 - val_accuracy: 0.7274\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.99297\n",
            "Epoch 59/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.6004 - accuracy: 0.7144 - val_loss: 0.6078 - val_accuracy: 0.7135\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.99297\n",
            "Epoch 60/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5982 - accuracy: 0.7196 - val_loss: 0.6729 - val_accuracy: 0.6875\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.99297\n",
            "Epoch 61/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5955 - accuracy: 0.7131 - val_loss: 0.6096 - val_accuracy: 0.7005\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.99297\n",
            "Epoch 62/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5916 - accuracy: 0.7170 - val_loss: 0.5901 - val_accuracy: 0.7144\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.99297\n",
            "Epoch 63/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5863 - accuracy: 0.7159 - val_loss: 0.6264 - val_accuracy: 0.7118\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.99297\n",
            "Epoch 64/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5850 - accuracy: 0.7207 - val_loss: 0.6636 - val_accuracy: 0.7205\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.99297\n",
            "Epoch 65/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5883 - accuracy: 0.7118 - val_loss: 0.5912 - val_accuracy: 0.7188\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.99297\n",
            "Epoch 66/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5855 - accuracy: 0.7144 - val_loss: 0.6140 - val_accuracy: 0.7153\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.99297\n",
            "Epoch 67/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5690 - accuracy: 0.7242 - val_loss: 0.5745 - val_accuracy: 0.7292\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.99297\n",
            "Epoch 68/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5700 - accuracy: 0.7276 - val_loss: 0.6064 - val_accuracy: 0.6997\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.99297\n",
            "Epoch 69/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5758 - accuracy: 0.7211 - val_loss: 0.5727 - val_accuracy: 0.7326\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.99297\n",
            "Epoch 70/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5681 - accuracy: 0.7248 - val_loss: 0.5970 - val_accuracy: 0.7170\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.99297\n",
            "Epoch 71/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5648 - accuracy: 0.7281 - val_loss: 0.5633 - val_accuracy: 0.7387\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.99297\n",
            "Epoch 72/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5639 - accuracy: 0.7261 - val_loss: 0.6518 - val_accuracy: 0.7023\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.99297\n",
            "Epoch 73/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5591 - accuracy: 0.7307 - val_loss: 0.6386 - val_accuracy: 0.6953\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.99297\n",
            "Epoch 74/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5607 - accuracy: 0.7305 - val_loss: 0.6153 - val_accuracy: 0.7127\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.99297\n",
            "Epoch 75/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5545 - accuracy: 0.7263 - val_loss: 0.5573 - val_accuracy: 0.7413\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.99297\n",
            "Epoch 76/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5630 - accuracy: 0.7268 - val_loss: 0.5603 - val_accuracy: 0.7240\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.99297\n",
            "Epoch 77/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5643 - accuracy: 0.7233 - val_loss: 0.5871 - val_accuracy: 0.7257\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.99297\n",
            "Epoch 78/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5565 - accuracy: 0.7268 - val_loss: 0.6082 - val_accuracy: 0.7144\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.99297\n",
            "Epoch 79/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5671 - accuracy: 0.7248 - val_loss: 0.5951 - val_accuracy: 0.7153\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.99297\n",
            "Epoch 80/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5537 - accuracy: 0.7279 - val_loss: 0.5492 - val_accuracy: 0.7361\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.99297\n",
            "Epoch 81/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5489 - accuracy: 0.7346 - val_loss: 0.6541 - val_accuracy: 0.7109\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.99297\n",
            "Epoch 82/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5564 - accuracy: 0.7268 - val_loss: 0.5367 - val_accuracy: 0.7422\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.99297\n",
            "Epoch 83/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5471 - accuracy: 0.7335 - val_loss: 0.5617 - val_accuracy: 0.7370\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.99297\n",
            "Epoch 84/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5461 - accuracy: 0.7303 - val_loss: 0.5877 - val_accuracy: 0.7292\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.99297\n",
            "Epoch 85/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5458 - accuracy: 0.7352 - val_loss: 0.5964 - val_accuracy: 0.7101\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.99297\n",
            "Epoch 86/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5425 - accuracy: 0.7391 - val_loss: 0.5595 - val_accuracy: 0.7292\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.99297\n",
            "Epoch 87/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5427 - accuracy: 0.7270 - val_loss: 0.5834 - val_accuracy: 0.7188\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.99297\n",
            "Epoch 88/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5429 - accuracy: 0.7309 - val_loss: 0.6001 - val_accuracy: 0.7214\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.99297\n",
            "Epoch 89/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5435 - accuracy: 0.7372 - val_loss: 0.5376 - val_accuracy: 0.7465\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.99297\n",
            "Epoch 90/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5349 - accuracy: 0.7348 - val_loss: 0.5336 - val_accuracy: 0.7413\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.99297\n",
            "Epoch 91/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5448 - accuracy: 0.7383 - val_loss: 0.6493 - val_accuracy: 0.7005\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.99297\n",
            "Epoch 92/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5407 - accuracy: 0.7344 - val_loss: 0.7760 - val_accuracy: 0.6901\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.99297\n",
            "Epoch 93/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5455 - accuracy: 0.7389 - val_loss: 0.5982 - val_accuracy: 0.7274\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.99297\n",
            "Epoch 94/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5363 - accuracy: 0.7402 - val_loss: 0.5353 - val_accuracy: 0.7465\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.99297\n",
            "Epoch 95/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5383 - accuracy: 0.7355 - val_loss: 0.5601 - val_accuracy: 0.7318\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.99297\n",
            "Epoch 96/500\n",
            "144/144 [==============================] - 1s 8ms/step - loss: 0.5488 - accuracy: 0.7359 - val_loss: 0.5560 - val_accuracy: 0.7335\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.99297\n",
            "Epoch 97/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5375 - accuracy: 0.7357 - val_loss: 0.5448 - val_accuracy: 0.7318\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.99297\n",
            "Epoch 98/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5327 - accuracy: 0.7381 - val_loss: 0.5457 - val_accuracy: 0.7448\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.99297\n",
            "Epoch 99/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5312 - accuracy: 0.7383 - val_loss: 0.5711 - val_accuracy: 0.7335\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.99297\n",
            "Epoch 100/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5433 - accuracy: 0.7276 - val_loss: 0.5864 - val_accuracy: 0.7335\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.99297\n",
            "Epoch 101/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5334 - accuracy: 0.7389 - val_loss: 0.6152 - val_accuracy: 0.7309\n",
            "\n",
            "Epoch 00101: val_accuracy did not improve from 0.99297\n",
            "Epoch 102/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5306 - accuracy: 0.7400 - val_loss: 0.5392 - val_accuracy: 0.7344\n",
            "\n",
            "Epoch 00102: val_accuracy did not improve from 0.99297\n",
            "Epoch 103/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5273 - accuracy: 0.7422 - val_loss: 0.5938 - val_accuracy: 0.7231\n",
            "\n",
            "Epoch 00103: val_accuracy did not improve from 0.99297\n",
            "Epoch 104/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5241 - accuracy: 0.7435 - val_loss: 0.5311 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00104: val_accuracy did not improve from 0.99297\n",
            "Epoch 105/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5240 - accuracy: 0.7465 - val_loss: 0.5233 - val_accuracy: 0.7509\n",
            "\n",
            "Epoch 00105: val_accuracy did not improve from 0.99297\n",
            "Epoch 106/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5248 - accuracy: 0.7437 - val_loss: 0.5310 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00106: val_accuracy did not improve from 0.99297\n",
            "Epoch 107/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5283 - accuracy: 0.7411 - val_loss: 0.5495 - val_accuracy: 0.7292\n",
            "\n",
            "Epoch 00107: val_accuracy did not improve from 0.99297\n",
            "Epoch 108/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5313 - accuracy: 0.7413 - val_loss: 0.5647 - val_accuracy: 0.7509\n",
            "\n",
            "Epoch 00108: val_accuracy did not improve from 0.99297\n",
            "Epoch 109/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5215 - accuracy: 0.7465 - val_loss: 0.5481 - val_accuracy: 0.7457\n",
            "\n",
            "Epoch 00109: val_accuracy did not improve from 0.99297\n",
            "Epoch 110/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5204 - accuracy: 0.7426 - val_loss: 0.5276 - val_accuracy: 0.7448\n",
            "\n",
            "Epoch 00110: val_accuracy did not improve from 0.99297\n",
            "Epoch 111/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5221 - accuracy: 0.7420 - val_loss: 0.6216 - val_accuracy: 0.7101\n",
            "\n",
            "Epoch 00111: val_accuracy did not improve from 0.99297\n",
            "Epoch 112/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5246 - accuracy: 0.7420 - val_loss: 0.5632 - val_accuracy: 0.7370\n",
            "\n",
            "Epoch 00112: val_accuracy did not improve from 0.99297\n",
            "Epoch 113/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5200 - accuracy: 0.7391 - val_loss: 0.5372 - val_accuracy: 0.7509\n",
            "\n",
            "Epoch 00113: val_accuracy did not improve from 0.99297\n",
            "Epoch 114/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5208 - accuracy: 0.7409 - val_loss: 0.8768 - val_accuracy: 0.6632\n",
            "\n",
            "Epoch 00114: val_accuracy did not improve from 0.99297\n",
            "Epoch 115/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5529 - accuracy: 0.7385 - val_loss: 0.5510 - val_accuracy: 0.7318\n",
            "\n",
            "Epoch 00115: val_accuracy did not improve from 0.99297\n",
            "Epoch 116/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5257 - accuracy: 0.7413 - val_loss: 0.5601 - val_accuracy: 0.7413\n",
            "\n",
            "Epoch 00116: val_accuracy did not improve from 0.99297\n",
            "Epoch 117/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5202 - accuracy: 0.7474 - val_loss: 0.7738 - val_accuracy: 0.6623\n",
            "\n",
            "Epoch 00117: val_accuracy did not improve from 0.99297\n",
            "Epoch 118/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5171 - accuracy: 0.7474 - val_loss: 0.5512 - val_accuracy: 0.7431\n",
            "\n",
            "Epoch 00118: val_accuracy did not improve from 0.99297\n",
            "Epoch 119/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5219 - accuracy: 0.7400 - val_loss: 0.5451 - val_accuracy: 0.7361\n",
            "\n",
            "Epoch 00119: val_accuracy did not improve from 0.99297\n",
            "Epoch 120/500\n",
            "144/144 [==============================] - 1s 8ms/step - loss: 0.5305 - accuracy: 0.7361 - val_loss: 0.5385 - val_accuracy: 0.7370\n",
            "\n",
            "Epoch 00120: val_accuracy did not improve from 0.99297\n",
            "Epoch 121/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5217 - accuracy: 0.7361 - val_loss: 0.5583 - val_accuracy: 0.7378\n",
            "\n",
            "Epoch 00121: val_accuracy did not improve from 0.99297\n",
            "Epoch 122/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5253 - accuracy: 0.7457 - val_loss: 0.6213 - val_accuracy: 0.7214\n",
            "\n",
            "Epoch 00122: val_accuracy did not improve from 0.99297\n",
            "Epoch 123/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5210 - accuracy: 0.7480 - val_loss: 0.5717 - val_accuracy: 0.7326\n",
            "\n",
            "Epoch 00123: val_accuracy did not improve from 0.99297\n",
            "Epoch 124/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5224 - accuracy: 0.7407 - val_loss: 0.5679 - val_accuracy: 0.7413\n",
            "\n",
            "Epoch 00124: val_accuracy did not improve from 0.99297\n",
            "Epoch 125/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5274 - accuracy: 0.7389 - val_loss: 0.5371 - val_accuracy: 0.7457\n",
            "\n",
            "Epoch 00125: val_accuracy did not improve from 0.99297\n",
            "Epoch 126/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5219 - accuracy: 0.7376 - val_loss: 0.9624 - val_accuracy: 0.6840\n",
            "\n",
            "Epoch 00126: val_accuracy did not improve from 0.99297\n",
            "Epoch 127/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5250 - accuracy: 0.7435 - val_loss: 0.5449 - val_accuracy: 0.7483\n",
            "\n",
            "Epoch 00127: val_accuracy did not improve from 0.99297\n",
            "Epoch 128/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5202 - accuracy: 0.7396 - val_loss: 0.5387 - val_accuracy: 0.7405\n",
            "\n",
            "Epoch 00128: val_accuracy did not improve from 0.99297\n",
            "Epoch 129/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5159 - accuracy: 0.7496 - val_loss: 0.5238 - val_accuracy: 0.7509\n",
            "\n",
            "Epoch 00129: val_accuracy did not improve from 0.99297\n",
            "Epoch 130/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5202 - accuracy: 0.7457 - val_loss: 0.5251 - val_accuracy: 0.7543\n",
            "\n",
            "Epoch 00130: val_accuracy did not improve from 0.99297\n",
            "Epoch 131/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5149 - accuracy: 0.7461 - val_loss: 0.5225 - val_accuracy: 0.7535\n",
            "\n",
            "Epoch 00131: val_accuracy did not improve from 0.99297\n",
            "Epoch 132/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5232 - accuracy: 0.7452 - val_loss: 0.5311 - val_accuracy: 0.7543\n",
            "\n",
            "Epoch 00132: val_accuracy did not improve from 0.99297\n",
            "Epoch 133/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5169 - accuracy: 0.7444 - val_loss: 0.5189 - val_accuracy: 0.7509\n",
            "\n",
            "Epoch 00133: val_accuracy did not improve from 0.99297\n",
            "Epoch 134/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5151 - accuracy: 0.7418 - val_loss: 0.5924 - val_accuracy: 0.7240\n",
            "\n",
            "Epoch 00134: val_accuracy did not improve from 0.99297\n",
            "Epoch 135/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5163 - accuracy: 0.7504 - val_loss: 0.5306 - val_accuracy: 0.7491\n",
            "\n",
            "Epoch 00135: val_accuracy did not improve from 0.99297\n",
            "Epoch 136/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5154 - accuracy: 0.7437 - val_loss: 0.5544 - val_accuracy: 0.7309\n",
            "\n",
            "Epoch 00136: val_accuracy did not improve from 0.99297\n",
            "Epoch 137/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5167 - accuracy: 0.7448 - val_loss: 0.6025 - val_accuracy: 0.7222\n",
            "\n",
            "Epoch 00137: val_accuracy did not improve from 0.99297\n",
            "Epoch 138/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5152 - accuracy: 0.7457 - val_loss: 0.5438 - val_accuracy: 0.7509\n",
            "\n",
            "Epoch 00138: val_accuracy did not improve from 0.99297\n",
            "Epoch 139/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5135 - accuracy: 0.7496 - val_loss: 0.5530 - val_accuracy: 0.7344\n",
            "\n",
            "Epoch 00139: val_accuracy did not improve from 0.99297\n",
            "Epoch 140/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5139 - accuracy: 0.7381 - val_loss: 0.5246 - val_accuracy: 0.7387\n",
            "\n",
            "Epoch 00140: val_accuracy did not improve from 0.99297\n",
            "Epoch 141/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5115 - accuracy: 0.7491 - val_loss: 0.5256 - val_accuracy: 0.7543\n",
            "\n",
            "Epoch 00141: val_accuracy did not improve from 0.99297\n",
            "Epoch 142/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5137 - accuracy: 0.7422 - val_loss: 0.5947 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00142: val_accuracy did not improve from 0.99297\n",
            "Epoch 143/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5124 - accuracy: 0.7485 - val_loss: 0.5330 - val_accuracy: 0.7483\n",
            "\n",
            "Epoch 00143: val_accuracy did not improve from 0.99297\n",
            "Epoch 144/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5130 - accuracy: 0.7480 - val_loss: 0.5542 - val_accuracy: 0.7431\n",
            "\n",
            "Epoch 00144: val_accuracy did not improve from 0.99297\n",
            "Epoch 145/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5121 - accuracy: 0.7493 - val_loss: 0.5266 - val_accuracy: 0.7413\n",
            "\n",
            "Epoch 00145: val_accuracy did not improve from 0.99297\n",
            "Epoch 146/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5104 - accuracy: 0.7517 - val_loss: 0.5464 - val_accuracy: 0.7361\n",
            "\n",
            "Epoch 00146: val_accuracy did not improve from 0.99297\n",
            "Epoch 147/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5125 - accuracy: 0.7478 - val_loss: 0.5470 - val_accuracy: 0.7352\n",
            "\n",
            "Epoch 00147: val_accuracy did not improve from 0.99297\n",
            "Epoch 148/500\n",
            "144/144 [==============================] - 1s 8ms/step - loss: 0.5094 - accuracy: 0.7517 - val_loss: 0.5238 - val_accuracy: 0.7535\n",
            "\n",
            "Epoch 00148: val_accuracy did not improve from 0.99297\n",
            "Epoch 149/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5096 - accuracy: 0.7480 - val_loss: 0.5268 - val_accuracy: 0.7535\n",
            "\n",
            "Epoch 00149: val_accuracy did not improve from 0.99297\n",
            "Epoch 150/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5188 - accuracy: 0.7422 - val_loss: 0.6727 - val_accuracy: 0.7127\n",
            "\n",
            "Epoch 00150: val_accuracy did not improve from 0.99297\n",
            "Epoch 151/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5126 - accuracy: 0.7439 - val_loss: 0.5523 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00151: val_accuracy did not improve from 0.99297\n",
            "Epoch 152/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5119 - accuracy: 0.7476 - val_loss: 0.5431 - val_accuracy: 0.7405\n",
            "\n",
            "Epoch 00152: val_accuracy did not improve from 0.99297\n",
            "Epoch 153/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5098 - accuracy: 0.7465 - val_loss: 0.5282 - val_accuracy: 0.7422\n",
            "\n",
            "Epoch 00153: val_accuracy did not improve from 0.99297\n",
            "Epoch 154/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5094 - accuracy: 0.7461 - val_loss: 0.5252 - val_accuracy: 0.7517\n",
            "\n",
            "Epoch 00154: val_accuracy did not improve from 0.99297\n",
            "Epoch 155/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5111 - accuracy: 0.7467 - val_loss: 0.5441 - val_accuracy: 0.7361\n",
            "\n",
            "Epoch 00155: val_accuracy did not improve from 0.99297\n",
            "Epoch 156/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5115 - accuracy: 0.7439 - val_loss: 0.5229 - val_accuracy: 0.7517\n",
            "\n",
            "Epoch 00156: val_accuracy did not improve from 0.99297\n",
            "Epoch 157/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5135 - accuracy: 0.7459 - val_loss: 0.5534 - val_accuracy: 0.7387\n",
            "\n",
            "Epoch 00157: val_accuracy did not improve from 0.99297\n",
            "Epoch 158/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5106 - accuracy: 0.7452 - val_loss: 0.5251 - val_accuracy: 0.7552\n",
            "\n",
            "Epoch 00158: val_accuracy did not improve from 0.99297\n",
            "Epoch 159/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5090 - accuracy: 0.7474 - val_loss: 0.5307 - val_accuracy: 0.7448\n",
            "\n",
            "Epoch 00159: val_accuracy did not improve from 0.99297\n",
            "Epoch 160/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5087 - accuracy: 0.7485 - val_loss: 0.5263 - val_accuracy: 0.7413\n",
            "\n",
            "Epoch 00160: val_accuracy did not improve from 0.99297\n",
            "Epoch 161/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5097 - accuracy: 0.7457 - val_loss: 0.5323 - val_accuracy: 0.7509\n",
            "\n",
            "Epoch 00161: val_accuracy did not improve from 0.99297\n",
            "Epoch 162/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5096 - accuracy: 0.7454 - val_loss: 0.5259 - val_accuracy: 0.7552\n",
            "\n",
            "Epoch 00162: val_accuracy did not improve from 0.99297\n",
            "Epoch 163/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5096 - accuracy: 0.7465 - val_loss: 0.5300 - val_accuracy: 0.7483\n",
            "\n",
            "Epoch 00163: val_accuracy did not improve from 0.99297\n",
            "Epoch 164/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5090 - accuracy: 0.7509 - val_loss: 0.5263 - val_accuracy: 0.7413\n",
            "\n",
            "Epoch 00164: val_accuracy did not improve from 0.99297\n",
            "Epoch 165/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5095 - accuracy: 0.7470 - val_loss: 0.5227 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00165: val_accuracy did not improve from 0.99297\n",
            "Epoch 166/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5094 - accuracy: 0.7448 - val_loss: 0.5296 - val_accuracy: 0.7465\n",
            "\n",
            "Epoch 00166: val_accuracy did not improve from 0.99297\n",
            "Epoch 167/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5081 - accuracy: 0.7493 - val_loss: 0.5341 - val_accuracy: 0.7483\n",
            "\n",
            "Epoch 00167: val_accuracy did not improve from 0.99297\n",
            "Epoch 168/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5096 - accuracy: 0.7502 - val_loss: 0.5324 - val_accuracy: 0.7483\n",
            "\n",
            "Epoch 00168: val_accuracy did not improve from 0.99297\n",
            "Epoch 169/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5099 - accuracy: 0.7463 - val_loss: 0.5324 - val_accuracy: 0.7543\n",
            "\n",
            "Epoch 00169: val_accuracy did not improve from 0.99297\n",
            "Epoch 170/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5084 - accuracy: 0.7520 - val_loss: 0.5333 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00170: val_accuracy did not improve from 0.99297\n",
            "Epoch 171/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5089 - accuracy: 0.7517 - val_loss: 0.5261 - val_accuracy: 0.7526\n",
            "\n",
            "Epoch 00171: val_accuracy did not improve from 0.99297\n",
            "Epoch 172/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5094 - accuracy: 0.7441 - val_loss: 0.5277 - val_accuracy: 0.7517\n",
            "\n",
            "Epoch 00172: val_accuracy did not improve from 0.99297\n",
            "Epoch 173/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5082 - accuracy: 0.7446 - val_loss: 0.5349 - val_accuracy: 0.7535\n",
            "\n",
            "Epoch 00173: val_accuracy did not improve from 0.99297\n",
            "Epoch 174/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5159 - accuracy: 0.7448 - val_loss: 0.9536 - val_accuracy: 0.6901\n",
            "\n",
            "Epoch 00174: val_accuracy did not improve from 0.99297\n",
            "Epoch 175/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5384 - accuracy: 0.7389 - val_loss: 0.6581 - val_accuracy: 0.7057\n",
            "\n",
            "Epoch 00175: val_accuracy did not improve from 0.99297\n",
            "Epoch 176/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5137 - accuracy: 0.7463 - val_loss: 0.5799 - val_accuracy: 0.7405\n",
            "\n",
            "Epoch 00176: val_accuracy did not improve from 0.99297\n",
            "Epoch 177/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5198 - accuracy: 0.7496 - val_loss: 0.5737 - val_accuracy: 0.7274\n",
            "\n",
            "Epoch 00177: val_accuracy did not improve from 0.99297\n",
            "Epoch 178/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5121 - accuracy: 0.7452 - val_loss: 0.5522 - val_accuracy: 0.7483\n",
            "\n",
            "Epoch 00178: val_accuracy did not improve from 0.99297\n",
            "Epoch 179/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5103 - accuracy: 0.7476 - val_loss: 0.5782 - val_accuracy: 0.7370\n",
            "\n",
            "Epoch 00179: val_accuracy did not improve from 0.99297\n",
            "Epoch 180/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5101 - accuracy: 0.7461 - val_loss: 0.5224 - val_accuracy: 0.7413\n",
            "\n",
            "Epoch 00180: val_accuracy did not improve from 0.99297\n",
            "Epoch 181/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5091 - accuracy: 0.7459 - val_loss: 0.5247 - val_accuracy: 0.7569\n",
            "\n",
            "Epoch 00181: val_accuracy did not improve from 0.99297\n",
            "Epoch 182/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5092 - accuracy: 0.7474 - val_loss: 0.5277 - val_accuracy: 0.7535\n",
            "\n",
            "Epoch 00182: val_accuracy did not improve from 0.99297\n",
            "Epoch 183/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5090 - accuracy: 0.7439 - val_loss: 0.5222 - val_accuracy: 0.7465\n",
            "\n",
            "Epoch 00183: val_accuracy did not improve from 0.99297\n",
            "Epoch 184/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5080 - accuracy: 0.7502 - val_loss: 0.5223 - val_accuracy: 0.7509\n",
            "\n",
            "Epoch 00184: val_accuracy did not improve from 0.99297\n",
            "Epoch 185/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5089 - accuracy: 0.7433 - val_loss: 0.5203 - val_accuracy: 0.7509\n",
            "\n",
            "Epoch 00185: val_accuracy did not improve from 0.99297\n",
            "Epoch 186/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5090 - accuracy: 0.7433 - val_loss: 0.5342 - val_accuracy: 0.7526\n",
            "\n",
            "Epoch 00186: val_accuracy did not improve from 0.99297\n",
            "Epoch 187/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5090 - accuracy: 0.7478 - val_loss: 0.5260 - val_accuracy: 0.7422\n",
            "\n",
            "Epoch 00187: val_accuracy did not improve from 0.99297\n",
            "Epoch 188/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5090 - accuracy: 0.7454 - val_loss: 0.5386 - val_accuracy: 0.7387\n",
            "\n",
            "Epoch 00188: val_accuracy did not improve from 0.99297\n",
            "Epoch 189/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5088 - accuracy: 0.7415 - val_loss: 0.5223 - val_accuracy: 0.7517\n",
            "\n",
            "Epoch 00189: val_accuracy did not improve from 0.99297\n",
            "Epoch 190/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5084 - accuracy: 0.7476 - val_loss: 0.5208 - val_accuracy: 0.7483\n",
            "\n",
            "Epoch 00190: val_accuracy did not improve from 0.99297\n",
            "Epoch 191/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5093 - accuracy: 0.7472 - val_loss: 0.5289 - val_accuracy: 0.7465\n",
            "\n",
            "Epoch 00191: val_accuracy did not improve from 0.99297\n",
            "Epoch 192/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5083 - accuracy: 0.7485 - val_loss: 0.5252 - val_accuracy: 0.7405\n",
            "\n",
            "Epoch 00192: val_accuracy did not improve from 0.99297\n",
            "Epoch 193/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5086 - accuracy: 0.7433 - val_loss: 0.5788 - val_accuracy: 0.7387\n",
            "\n",
            "Epoch 00193: val_accuracy did not improve from 0.99297\n",
            "Epoch 194/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5086 - accuracy: 0.7480 - val_loss: 0.5238 - val_accuracy: 0.7474\n",
            "\n",
            "Epoch 00194: val_accuracy did not improve from 0.99297\n",
            "Epoch 195/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5082 - accuracy: 0.7435 - val_loss: 0.5268 - val_accuracy: 0.7509\n",
            "\n",
            "Epoch 00195: val_accuracy did not improve from 0.99297\n",
            "Epoch 196/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5084 - accuracy: 0.7504 - val_loss: 0.5610 - val_accuracy: 0.7474\n",
            "\n",
            "Epoch 00196: val_accuracy did not improve from 0.99297\n",
            "Epoch 197/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5088 - accuracy: 0.7483 - val_loss: 0.5403 - val_accuracy: 0.7474\n",
            "\n",
            "Epoch 00197: val_accuracy did not improve from 0.99297\n",
            "Epoch 198/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5080 - accuracy: 0.7493 - val_loss: 0.5317 - val_accuracy: 0.7500\n",
            "\n",
            "Epoch 00198: val_accuracy did not improve from 0.99297\n",
            "Epoch 199/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5085 - accuracy: 0.7489 - val_loss: 0.5334 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00199: val_accuracy did not improve from 0.99297\n",
            "Epoch 200/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5098 - accuracy: 0.7413 - val_loss: 0.5345 - val_accuracy: 0.7483\n",
            "\n",
            "Epoch 00200: val_accuracy did not improve from 0.99297\n",
            "Epoch 201/500\n",
            "144/144 [==============================] - 1s 8ms/step - loss: 0.5084 - accuracy: 0.7444 - val_loss: 0.5239 - val_accuracy: 0.7509\n",
            "\n",
            "Epoch 00201: val_accuracy did not improve from 0.99297\n",
            "Epoch 202/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5189 - accuracy: 0.7407 - val_loss: 0.5418 - val_accuracy: 0.7500\n",
            "\n",
            "Epoch 00202: val_accuracy did not improve from 0.99297\n",
            "Epoch 203/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5093 - accuracy: 0.7485 - val_loss: 0.5260 - val_accuracy: 0.7509\n",
            "\n",
            "Epoch 00203: val_accuracy did not improve from 0.99297\n",
            "Epoch 204/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5094 - accuracy: 0.7480 - val_loss: 0.5234 - val_accuracy: 0.7517\n",
            "\n",
            "Epoch 00204: val_accuracy did not improve from 0.99297\n",
            "Epoch 205/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5086 - accuracy: 0.7476 - val_loss: 0.5225 - val_accuracy: 0.7500\n",
            "\n",
            "Epoch 00205: val_accuracy did not improve from 0.99297\n",
            "Epoch 206/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5078 - accuracy: 0.7511 - val_loss: 0.5232 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00206: val_accuracy did not improve from 0.99297\n",
            "Epoch 207/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5082 - accuracy: 0.7496 - val_loss: 0.5321 - val_accuracy: 0.7387\n",
            "\n",
            "Epoch 00207: val_accuracy did not improve from 0.99297\n",
            "Epoch 208/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5085 - accuracy: 0.7433 - val_loss: 0.5281 - val_accuracy: 0.7422\n",
            "\n",
            "Epoch 00208: val_accuracy did not improve from 0.99297\n",
            "Epoch 209/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5086 - accuracy: 0.7433 - val_loss: 0.5250 - val_accuracy: 0.7422\n",
            "\n",
            "Epoch 00209: val_accuracy did not improve from 0.99297\n",
            "Epoch 210/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5076 - accuracy: 0.7459 - val_loss: 0.5274 - val_accuracy: 0.7500\n",
            "\n",
            "Epoch 00210: val_accuracy did not improve from 0.99297\n",
            "Epoch 211/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5082 - accuracy: 0.7496 - val_loss: 0.5287 - val_accuracy: 0.7552\n",
            "\n",
            "Epoch 00211: val_accuracy did not improve from 0.99297\n",
            "Epoch 212/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5083 - accuracy: 0.7435 - val_loss: 0.5253 - val_accuracy: 0.7526\n",
            "\n",
            "Epoch 00212: val_accuracy did not improve from 0.99297\n",
            "Epoch 213/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5080 - accuracy: 0.7476 - val_loss: 0.5256 - val_accuracy: 0.7405\n",
            "\n",
            "Epoch 00213: val_accuracy did not improve from 0.99297\n",
            "Epoch 214/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5082 - accuracy: 0.7520 - val_loss: 0.5343 - val_accuracy: 0.7491\n",
            "\n",
            "Epoch 00214: val_accuracy did not improve from 0.99297\n",
            "Epoch 215/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5093 - accuracy: 0.7446 - val_loss: 0.5526 - val_accuracy: 0.7474\n",
            "\n",
            "Epoch 00215: val_accuracy did not improve from 0.99297\n",
            "Epoch 216/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5090 - accuracy: 0.7433 - val_loss: 0.5307 - val_accuracy: 0.7526\n",
            "\n",
            "Epoch 00216: val_accuracy did not improve from 0.99297\n",
            "Epoch 217/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5087 - accuracy: 0.7474 - val_loss: 0.5279 - val_accuracy: 0.7422\n",
            "\n",
            "Epoch 00217: val_accuracy did not improve from 0.99297\n",
            "Epoch 218/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5086 - accuracy: 0.7498 - val_loss: 0.5293 - val_accuracy: 0.7413\n",
            "\n",
            "Epoch 00218: val_accuracy did not improve from 0.99297\n",
            "Epoch 219/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5081 - accuracy: 0.7478 - val_loss: 0.5288 - val_accuracy: 0.7578\n",
            "\n",
            "Epoch 00219: val_accuracy did not improve from 0.99297\n",
            "Epoch 220/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5075 - accuracy: 0.7487 - val_loss: 0.5273 - val_accuracy: 0.7413\n",
            "\n",
            "Epoch 00220: val_accuracy did not improve from 0.99297\n",
            "Epoch 221/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5079 - accuracy: 0.7496 - val_loss: 0.5411 - val_accuracy: 0.7526\n",
            "\n",
            "Epoch 00221: val_accuracy did not improve from 0.99297\n",
            "Epoch 222/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5081 - accuracy: 0.7487 - val_loss: 0.5254 - val_accuracy: 0.7509\n",
            "\n",
            "Epoch 00222: val_accuracy did not improve from 0.99297\n",
            "Epoch 223/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5082 - accuracy: 0.7515 - val_loss: 0.5251 - val_accuracy: 0.7552\n",
            "\n",
            "Epoch 00223: val_accuracy did not improve from 0.99297\n",
            "Epoch 224/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5082 - accuracy: 0.7467 - val_loss: 0.5376 - val_accuracy: 0.7526\n",
            "\n",
            "Epoch 00224: val_accuracy did not improve from 0.99297\n",
            "Epoch 225/500\n",
            "144/144 [==============================] - 1s 8ms/step - loss: 0.5080 - accuracy: 0.7487 - val_loss: 0.5263 - val_accuracy: 0.7569\n",
            "\n",
            "Epoch 00225: val_accuracy did not improve from 0.99297\n",
            "Epoch 226/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5081 - accuracy: 0.7465 - val_loss: 0.5235 - val_accuracy: 0.7483\n",
            "\n",
            "Epoch 00226: val_accuracy did not improve from 0.99297\n",
            "Epoch 227/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5075 - accuracy: 0.7500 - val_loss: 0.5277 - val_accuracy: 0.7552\n",
            "\n",
            "Epoch 00227: val_accuracy did not improve from 0.99297\n",
            "Epoch 228/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5087 - accuracy: 0.7439 - val_loss: 0.5251 - val_accuracy: 0.7491\n",
            "\n",
            "Epoch 00228: val_accuracy did not improve from 0.99297\n",
            "Epoch 229/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5082 - accuracy: 0.7472 - val_loss: 0.5329 - val_accuracy: 0.7474\n",
            "\n",
            "Epoch 00229: val_accuracy did not improve from 0.99297\n",
            "Epoch 230/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5079 - accuracy: 0.7489 - val_loss: 0.5255 - val_accuracy: 0.7474\n",
            "\n",
            "Epoch 00230: val_accuracy did not improve from 0.99297\n",
            "Epoch 231/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5086 - accuracy: 0.7454 - val_loss: 0.5273 - val_accuracy: 0.7500\n",
            "\n",
            "Epoch 00231: val_accuracy did not improve from 0.99297\n",
            "Epoch 232/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5078 - accuracy: 0.7526 - val_loss: 0.5313 - val_accuracy: 0.7500\n",
            "\n",
            "Epoch 00232: val_accuracy did not improve from 0.99297\n",
            "Epoch 233/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5082 - accuracy: 0.7426 - val_loss: 0.5278 - val_accuracy: 0.7422\n",
            "\n",
            "Epoch 00233: val_accuracy did not improve from 0.99297\n",
            "Epoch 234/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5084 - accuracy: 0.7459 - val_loss: 0.5267 - val_accuracy: 0.7569\n",
            "\n",
            "Epoch 00234: val_accuracy did not improve from 0.99297\n",
            "Epoch 235/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5082 - accuracy: 0.7435 - val_loss: 0.5310 - val_accuracy: 0.7543\n",
            "\n",
            "Epoch 00235: val_accuracy did not improve from 0.99297\n",
            "Epoch 236/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5082 - accuracy: 0.7433 - val_loss: 0.5281 - val_accuracy: 0.7535\n",
            "\n",
            "Epoch 00236: val_accuracy did not improve from 0.99297\n",
            "Epoch 237/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5085 - accuracy: 0.7487 - val_loss: 0.5296 - val_accuracy: 0.7543\n",
            "\n",
            "Epoch 00237: val_accuracy did not improve from 0.99297\n",
            "Epoch 238/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5583 - accuracy: 0.7381 - val_loss: 2.6651 - val_accuracy: 0.2483\n",
            "\n",
            "Epoch 00238: val_accuracy did not improve from 0.99297\n",
            "Epoch 239/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5447 - accuracy: 0.7359 - val_loss: 0.6262 - val_accuracy: 0.7153\n",
            "\n",
            "Epoch 00239: val_accuracy did not improve from 0.99297\n",
            "Epoch 240/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5132 - accuracy: 0.7437 - val_loss: 0.5928 - val_accuracy: 0.7422\n",
            "\n",
            "Epoch 00240: val_accuracy did not improve from 0.99297\n",
            "Epoch 241/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5108 - accuracy: 0.7491 - val_loss: 0.5551 - val_accuracy: 0.7318\n",
            "\n",
            "Epoch 00241: val_accuracy did not improve from 0.99297\n",
            "Epoch 242/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5149 - accuracy: 0.7396 - val_loss: 0.5490 - val_accuracy: 0.7465\n",
            "\n",
            "Epoch 00242: val_accuracy did not improve from 0.99297\n",
            "Epoch 243/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5116 - accuracy: 0.7437 - val_loss: 0.5315 - val_accuracy: 0.7535\n",
            "\n",
            "Epoch 00243: val_accuracy did not improve from 0.99297\n",
            "Epoch 244/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5089 - accuracy: 0.7461 - val_loss: 0.5800 - val_accuracy: 0.7344\n",
            "\n",
            "Epoch 00244: val_accuracy did not improve from 0.99297\n",
            "Epoch 245/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5170 - accuracy: 0.7441 - val_loss: 0.5537 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00245: val_accuracy did not improve from 0.99297\n",
            "Epoch 246/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5093 - accuracy: 0.7487 - val_loss: 0.5443 - val_accuracy: 0.7457\n",
            "\n",
            "Epoch 00246: val_accuracy did not improve from 0.99297\n",
            "Epoch 247/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5083 - accuracy: 0.7491 - val_loss: 0.5361 - val_accuracy: 0.7457\n",
            "\n",
            "Epoch 00247: val_accuracy did not improve from 0.99297\n",
            "Epoch 248/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5075 - accuracy: 0.7526 - val_loss: 0.5283 - val_accuracy: 0.7526\n",
            "\n",
            "Epoch 00248: val_accuracy did not improve from 0.99297\n",
            "Epoch 249/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5098 - accuracy: 0.7448 - val_loss: 0.5338 - val_accuracy: 0.7396\n",
            "\n",
            "Epoch 00249: val_accuracy did not improve from 0.99297\n",
            "Epoch 250/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5087 - accuracy: 0.7463 - val_loss: 0.5314 - val_accuracy: 0.7543\n",
            "\n",
            "Epoch 00250: val_accuracy did not improve from 0.99297\n",
            "Epoch 251/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5080 - accuracy: 0.7498 - val_loss: 0.5323 - val_accuracy: 0.7569\n",
            "\n",
            "Epoch 00251: val_accuracy did not improve from 0.99297\n",
            "Epoch 252/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5112 - accuracy: 0.7463 - val_loss: 0.5361 - val_accuracy: 0.7526\n",
            "\n",
            "Epoch 00252: val_accuracy did not improve from 0.99297\n",
            "Epoch 253/500\n",
            "144/144 [==============================] - 1s 8ms/step - loss: 0.5080 - accuracy: 0.7459 - val_loss: 0.5367 - val_accuracy: 0.7405\n",
            "\n",
            "Epoch 00253: val_accuracy did not improve from 0.99297\n",
            "Epoch 254/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5077 - accuracy: 0.7502 - val_loss: 0.5271 - val_accuracy: 0.7526\n",
            "\n",
            "Epoch 00254: val_accuracy did not improve from 0.99297\n",
            "Epoch 255/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5096 - accuracy: 0.7461 - val_loss: 0.5547 - val_accuracy: 0.7422\n",
            "\n",
            "Epoch 00255: val_accuracy did not improve from 0.99297\n",
            "Epoch 256/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5086 - accuracy: 0.7446 - val_loss: 0.5243 - val_accuracy: 0.7535\n",
            "\n",
            "Epoch 00256: val_accuracy did not improve from 0.99297\n",
            "Epoch 257/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5080 - accuracy: 0.7504 - val_loss: 0.5304 - val_accuracy: 0.7448\n",
            "\n",
            "Epoch 00257: val_accuracy did not improve from 0.99297\n",
            "Epoch 258/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5083 - accuracy: 0.7470 - val_loss: 0.5308 - val_accuracy: 0.7509\n",
            "\n",
            "Epoch 00258: val_accuracy did not improve from 0.99297\n",
            "Epoch 259/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5085 - accuracy: 0.7446 - val_loss: 0.5305 - val_accuracy: 0.7526\n",
            "\n",
            "Epoch 00259: val_accuracy did not improve from 0.99297\n",
            "Epoch 260/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5081 - accuracy: 0.7467 - val_loss: 0.5264 - val_accuracy: 0.7587\n",
            "\n",
            "Epoch 00260: val_accuracy did not improve from 0.99297\n",
            "Epoch 261/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5071 - accuracy: 0.7459 - val_loss: 0.5389 - val_accuracy: 0.7457\n",
            "\n",
            "Epoch 00261: val_accuracy did not improve from 0.99297\n",
            "Epoch 262/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5074 - accuracy: 0.7461 - val_loss: 0.5306 - val_accuracy: 0.7578\n",
            "\n",
            "Epoch 00262: val_accuracy did not improve from 0.99297\n",
            "Epoch 263/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5082 - accuracy: 0.7467 - val_loss: 0.5264 - val_accuracy: 0.7448\n",
            "\n",
            "Epoch 00263: val_accuracy did not improve from 0.99297\n",
            "Epoch 264/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5083 - accuracy: 0.7502 - val_loss: 0.5320 - val_accuracy: 0.7509\n",
            "\n",
            "Epoch 00264: val_accuracy did not improve from 0.99297\n",
            "Epoch 265/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5081 - accuracy: 0.7448 - val_loss: 0.5277 - val_accuracy: 0.7509\n",
            "\n",
            "Epoch 00265: val_accuracy did not improve from 0.99297\n",
            "Epoch 266/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5080 - accuracy: 0.7507 - val_loss: 0.5253 - val_accuracy: 0.7543\n",
            "\n",
            "Epoch 00266: val_accuracy did not improve from 0.99297\n",
            "Epoch 267/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5081 - accuracy: 0.7472 - val_loss: 0.5324 - val_accuracy: 0.7396\n",
            "\n",
            "Epoch 00267: val_accuracy did not improve from 0.99297\n",
            "Epoch 268/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5079 - accuracy: 0.7437 - val_loss: 0.5285 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00268: val_accuracy did not improve from 0.99297\n",
            "Epoch 269/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5077 - accuracy: 0.7461 - val_loss: 0.5284 - val_accuracy: 0.7500\n",
            "\n",
            "Epoch 00269: val_accuracy did not improve from 0.99297\n",
            "Epoch 270/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5080 - accuracy: 0.7485 - val_loss: 0.5284 - val_accuracy: 0.7526\n",
            "\n",
            "Epoch 00270: val_accuracy did not improve from 0.99297\n",
            "Epoch 271/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5080 - accuracy: 0.7463 - val_loss: 0.5319 - val_accuracy: 0.7422\n",
            "\n",
            "Epoch 00271: val_accuracy did not improve from 0.99297\n",
            "Epoch 272/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5079 - accuracy: 0.7513 - val_loss: 0.5275 - val_accuracy: 0.7543\n",
            "\n",
            "Epoch 00272: val_accuracy did not improve from 0.99297\n",
            "Epoch 273/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5080 - accuracy: 0.7472 - val_loss: 0.5289 - val_accuracy: 0.7587\n",
            "\n",
            "Epoch 00273: val_accuracy did not improve from 0.99297\n",
            "Epoch 274/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5080 - accuracy: 0.7450 - val_loss: 0.5281 - val_accuracy: 0.7587\n",
            "\n",
            "Epoch 00274: val_accuracy did not improve from 0.99297\n",
            "Epoch 275/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5075 - accuracy: 0.7491 - val_loss: 0.5258 - val_accuracy: 0.7500\n",
            "\n",
            "Epoch 00275: val_accuracy did not improve from 0.99297\n",
            "Epoch 276/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5076 - accuracy: 0.7465 - val_loss: 0.5334 - val_accuracy: 0.7465\n",
            "\n",
            "Epoch 00276: val_accuracy did not improve from 0.99297\n",
            "Epoch 277/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5078 - accuracy: 0.7474 - val_loss: 0.5302 - val_accuracy: 0.7431\n",
            "\n",
            "Epoch 00277: val_accuracy did not improve from 0.99297\n",
            "Epoch 278/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5073 - accuracy: 0.7491 - val_loss: 0.5302 - val_accuracy: 0.7448\n",
            "\n",
            "Epoch 00278: val_accuracy did not improve from 0.99297\n",
            "Epoch 279/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5083 - accuracy: 0.7398 - val_loss: 0.5282 - val_accuracy: 0.7552\n",
            "\n",
            "Epoch 00279: val_accuracy did not improve from 0.99297\n",
            "Epoch 280/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5076 - accuracy: 0.7500 - val_loss: 0.5276 - val_accuracy: 0.7526\n",
            "\n",
            "Epoch 00280: val_accuracy did not improve from 0.99297\n",
            "Epoch 281/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5112 - accuracy: 0.7457 - val_loss: 0.5717 - val_accuracy: 0.7405\n",
            "\n",
            "Epoch 00281: val_accuracy did not improve from 0.99297\n",
            "Epoch 282/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5091 - accuracy: 0.7437 - val_loss: 0.5288 - val_accuracy: 0.7569\n",
            "\n",
            "Epoch 00282: val_accuracy did not improve from 0.99297\n",
            "Epoch 283/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5085 - accuracy: 0.7418 - val_loss: 0.5489 - val_accuracy: 0.7448\n",
            "\n",
            "Epoch 00283: val_accuracy did not improve from 0.99297\n",
            "Epoch 284/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5081 - accuracy: 0.7461 - val_loss: 0.5375 - val_accuracy: 0.7491\n",
            "\n",
            "Epoch 00284: val_accuracy did not improve from 0.99297\n",
            "Epoch 285/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5078 - accuracy: 0.7528 - val_loss: 0.5260 - val_accuracy: 0.7517\n",
            "\n",
            "Epoch 00285: val_accuracy did not improve from 0.99297\n",
            "Epoch 286/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5087 - accuracy: 0.7485 - val_loss: 0.5307 - val_accuracy: 0.7396\n",
            "\n",
            "Epoch 00286: val_accuracy did not improve from 0.99297\n",
            "Epoch 287/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5081 - accuracy: 0.7485 - val_loss: 0.5300 - val_accuracy: 0.7569\n",
            "\n",
            "Epoch 00287: val_accuracy did not improve from 0.99297\n",
            "Epoch 288/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5080 - accuracy: 0.7454 - val_loss: 0.5283 - val_accuracy: 0.7431\n",
            "\n",
            "Epoch 00288: val_accuracy did not improve from 0.99297\n",
            "Epoch 289/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5079 - accuracy: 0.7415 - val_loss: 0.5474 - val_accuracy: 0.7457\n",
            "\n",
            "Epoch 00289: val_accuracy did not improve from 0.99297\n",
            "Epoch 290/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5079 - accuracy: 0.7500 - val_loss: 0.5266 - val_accuracy: 0.7552\n",
            "\n",
            "Epoch 00290: val_accuracy did not improve from 0.99297\n",
            "Epoch 291/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5080 - accuracy: 0.7463 - val_loss: 0.5256 - val_accuracy: 0.7587\n",
            "\n",
            "Epoch 00291: val_accuracy did not improve from 0.99297\n",
            "Epoch 292/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5073 - accuracy: 0.7524 - val_loss: 0.5303 - val_accuracy: 0.7448\n",
            "\n",
            "Epoch 00292: val_accuracy did not improve from 0.99297\n",
            "Epoch 293/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5082 - accuracy: 0.7474 - val_loss: 0.5266 - val_accuracy: 0.7543\n",
            "\n",
            "Epoch 00293: val_accuracy did not improve from 0.99297\n",
            "Epoch 294/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5080 - accuracy: 0.7402 - val_loss: 0.5312 - val_accuracy: 0.7517\n",
            "\n",
            "Epoch 00294: val_accuracy did not improve from 0.99297\n",
            "Epoch 295/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5080 - accuracy: 0.7461 - val_loss: 0.5298 - val_accuracy: 0.7483\n",
            "\n",
            "Epoch 00295: val_accuracy did not improve from 0.99297\n",
            "Epoch 296/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5079 - accuracy: 0.7461 - val_loss: 0.5260 - val_accuracy: 0.7595\n",
            "\n",
            "Epoch 00296: val_accuracy did not improve from 0.99297\n",
            "Epoch 297/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5075 - accuracy: 0.7415 - val_loss: 0.5302 - val_accuracy: 0.7552\n",
            "\n",
            "Epoch 00297: val_accuracy did not improve from 0.99297\n",
            "Epoch 298/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5077 - accuracy: 0.7472 - val_loss: 0.5299 - val_accuracy: 0.7509\n",
            "\n",
            "Epoch 00298: val_accuracy did not improve from 0.99297\n",
            "Epoch 299/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5079 - accuracy: 0.7513 - val_loss: 0.5295 - val_accuracy: 0.7526\n",
            "\n",
            "Epoch 00299: val_accuracy did not improve from 0.99297\n",
            "Epoch 300/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5077 - accuracy: 0.7459 - val_loss: 0.5300 - val_accuracy: 0.7569\n",
            "\n",
            "Epoch 00300: val_accuracy did not improve from 0.99297\n",
            "Epoch 301/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5076 - accuracy: 0.7472 - val_loss: 0.5357 - val_accuracy: 0.7500\n",
            "\n",
            "Epoch 00301: val_accuracy did not improve from 0.99297\n",
            "Epoch 302/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5083 - accuracy: 0.7470 - val_loss: 0.5331 - val_accuracy: 0.7509\n",
            "\n",
            "Epoch 00302: val_accuracy did not improve from 0.99297\n",
            "Epoch 303/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5080 - accuracy: 0.7465 - val_loss: 0.5272 - val_accuracy: 0.7552\n",
            "\n",
            "Epoch 00303: val_accuracy did not improve from 0.99297\n",
            "Epoch 304/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5074 - accuracy: 0.7480 - val_loss: 0.5268 - val_accuracy: 0.7509\n",
            "\n",
            "Epoch 00304: val_accuracy did not improve from 0.99297\n",
            "Epoch 305/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5078 - accuracy: 0.7437 - val_loss: 0.5300 - val_accuracy: 0.7500\n",
            "\n",
            "Epoch 00305: val_accuracy did not improve from 0.99297\n",
            "Epoch 306/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5082 - accuracy: 0.7413 - val_loss: 0.5261 - val_accuracy: 0.7543\n",
            "\n",
            "Epoch 00306: val_accuracy did not improve from 0.99297\n",
            "Epoch 307/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5081 - accuracy: 0.7459 - val_loss: 0.5388 - val_accuracy: 0.7491\n",
            "\n",
            "Epoch 00307: val_accuracy did not improve from 0.99297\n",
            "Epoch 308/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5079 - accuracy: 0.7441 - val_loss: 0.5335 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00308: val_accuracy did not improve from 0.99297\n",
            "Epoch 309/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5079 - accuracy: 0.7433 - val_loss: 0.5298 - val_accuracy: 0.7483\n",
            "\n",
            "Epoch 00309: val_accuracy did not improve from 0.99297\n",
            "Epoch 310/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5077 - accuracy: 0.7478 - val_loss: 0.5304 - val_accuracy: 0.7517\n",
            "\n",
            "Epoch 00310: val_accuracy did not improve from 0.99297\n",
            "Epoch 311/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5080 - accuracy: 0.7467 - val_loss: 0.5267 - val_accuracy: 0.7595\n",
            "\n",
            "Epoch 00311: val_accuracy did not improve from 0.99297\n",
            "Epoch 312/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5080 - accuracy: 0.7470 - val_loss: 0.5317 - val_accuracy: 0.7413\n",
            "\n",
            "Epoch 00312: val_accuracy did not improve from 0.99297\n",
            "Epoch 313/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5078 - accuracy: 0.7409 - val_loss: 0.5282 - val_accuracy: 0.7587\n",
            "\n",
            "Epoch 00313: val_accuracy did not improve from 0.99297\n",
            "Epoch 314/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5073 - accuracy: 0.7483 - val_loss: 0.5300 - val_accuracy: 0.7578\n",
            "\n",
            "Epoch 00314: val_accuracy did not improve from 0.99297\n",
            "Epoch 315/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5077 - accuracy: 0.7431 - val_loss: 0.5333 - val_accuracy: 0.7500\n",
            "\n",
            "Epoch 00315: val_accuracy did not improve from 0.99297\n",
            "Epoch 316/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5082 - accuracy: 0.7394 - val_loss: 0.5298 - val_accuracy: 0.7543\n",
            "\n",
            "Epoch 00316: val_accuracy did not improve from 0.99297\n",
            "Epoch 317/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5075 - accuracy: 0.7413 - val_loss: 0.5331 - val_accuracy: 0.7543\n",
            "\n",
            "Epoch 00317: val_accuracy did not improve from 0.99297\n",
            "Epoch 318/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5079 - accuracy: 0.7502 - val_loss: 0.5293 - val_accuracy: 0.7543\n",
            "\n",
            "Epoch 00318: val_accuracy did not improve from 0.99297\n",
            "Epoch 319/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5076 - accuracy: 0.7485 - val_loss: 0.5306 - val_accuracy: 0.7543\n",
            "\n",
            "Epoch 00319: val_accuracy did not improve from 0.99297\n",
            "Epoch 320/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5077 - accuracy: 0.7485 - val_loss: 0.5324 - val_accuracy: 0.7517\n",
            "\n",
            "Epoch 00320: val_accuracy did not improve from 0.99297\n",
            "Epoch 321/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5079 - accuracy: 0.7439 - val_loss: 0.5300 - val_accuracy: 0.7535\n",
            "\n",
            "Epoch 00321: val_accuracy did not improve from 0.99297\n",
            "Epoch 322/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5082 - accuracy: 0.7487 - val_loss: 0.5480 - val_accuracy: 0.7543\n",
            "\n",
            "Epoch 00322: val_accuracy did not improve from 0.99297\n",
            "Epoch 323/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5069 - accuracy: 0.7480 - val_loss: 0.5338 - val_accuracy: 0.7405\n",
            "\n",
            "Epoch 00323: val_accuracy did not improve from 0.99297\n",
            "Epoch 324/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5072 - accuracy: 0.7472 - val_loss: 0.5354 - val_accuracy: 0.7552\n",
            "\n",
            "Epoch 00324: val_accuracy did not improve from 0.99297\n",
            "Epoch 325/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5078 - accuracy: 0.7476 - val_loss: 0.5337 - val_accuracy: 0.7431\n",
            "\n",
            "Epoch 00325: val_accuracy did not improve from 0.99297\n",
            "Epoch 326/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5083 - accuracy: 0.7478 - val_loss: 0.5641 - val_accuracy: 0.7474\n",
            "\n",
            "Epoch 00326: val_accuracy did not improve from 0.99297\n",
            "Epoch 327/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5081 - accuracy: 0.7470 - val_loss: 0.5424 - val_accuracy: 0.7474\n",
            "\n",
            "Epoch 00327: val_accuracy did not improve from 0.99297\n",
            "Epoch 328/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5075 - accuracy: 0.7474 - val_loss: 0.5299 - val_accuracy: 0.7535\n",
            "\n",
            "Epoch 00328: val_accuracy did not improve from 0.99297\n",
            "Epoch 329/500\n",
            "144/144 [==============================] - 1s 8ms/step - loss: 0.5075 - accuracy: 0.7463 - val_loss: 0.5301 - val_accuracy: 0.7578\n",
            "\n",
            "Epoch 00329: val_accuracy did not improve from 0.99297\n",
            "Epoch 330/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5078 - accuracy: 0.7487 - val_loss: 0.5280 - val_accuracy: 0.7595\n",
            "\n",
            "Epoch 00330: val_accuracy did not improve from 0.99297\n",
            "Epoch 331/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5070 - accuracy: 0.7522 - val_loss: 0.5301 - val_accuracy: 0.7491\n",
            "\n",
            "Epoch 00331: val_accuracy did not improve from 0.99297\n",
            "Epoch 332/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5079 - accuracy: 0.7478 - val_loss: 0.5287 - val_accuracy: 0.7552\n",
            "\n",
            "Epoch 00332: val_accuracy did not improve from 0.99297\n",
            "Epoch 333/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5078 - accuracy: 0.7431 - val_loss: 0.5263 - val_accuracy: 0.7543\n",
            "\n",
            "Epoch 00333: val_accuracy did not improve from 0.99297\n",
            "Epoch 334/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5078 - accuracy: 0.7502 - val_loss: 0.5299 - val_accuracy: 0.7448\n",
            "\n",
            "Epoch 00334: val_accuracy did not improve from 0.99297\n",
            "Epoch 335/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5075 - accuracy: 0.7457 - val_loss: 0.5283 - val_accuracy: 0.7448\n",
            "\n",
            "Epoch 00335: val_accuracy did not improve from 0.99297\n",
            "Epoch 336/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5078 - accuracy: 0.7467 - val_loss: 0.5311 - val_accuracy: 0.7483\n",
            "\n",
            "Epoch 00336: val_accuracy did not improve from 0.99297\n",
            "Epoch 337/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5079 - accuracy: 0.7418 - val_loss: 0.5282 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00337: val_accuracy did not improve from 0.99297\n",
            "Epoch 338/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5073 - accuracy: 0.7459 - val_loss: 0.5286 - val_accuracy: 0.7500\n",
            "\n",
            "Epoch 00338: val_accuracy did not improve from 0.99297\n",
            "Epoch 339/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5070 - accuracy: 0.7517 - val_loss: 0.5350 - val_accuracy: 0.7552\n",
            "\n",
            "Epoch 00339: val_accuracy did not improve from 0.99297\n",
            "Epoch 340/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5078 - accuracy: 0.7435 - val_loss: 0.5275 - val_accuracy: 0.7500\n",
            "\n",
            "Epoch 00340: val_accuracy did not improve from 0.99297\n",
            "Epoch 341/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5074 - accuracy: 0.7476 - val_loss: 0.5293 - val_accuracy: 0.7578\n",
            "\n",
            "Epoch 00341: val_accuracy did not improve from 0.99297\n",
            "Epoch 342/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5072 - accuracy: 0.7530 - val_loss: 0.5329 - val_accuracy: 0.7500\n",
            "\n",
            "Epoch 00342: val_accuracy did not improve from 0.99297\n",
            "Epoch 343/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5078 - accuracy: 0.7428 - val_loss: 0.5302 - val_accuracy: 0.7526\n",
            "\n",
            "Epoch 00343: val_accuracy did not improve from 0.99297\n",
            "Epoch 344/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5075 - accuracy: 0.7444 - val_loss: 0.5295 - val_accuracy: 0.7587\n",
            "\n",
            "Epoch 00344: val_accuracy did not improve from 0.99297\n",
            "Epoch 345/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5078 - accuracy: 0.7428 - val_loss: 0.5335 - val_accuracy: 0.7517\n",
            "\n",
            "Epoch 00345: val_accuracy did not improve from 0.99297\n",
            "Epoch 346/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5077 - accuracy: 0.7465 - val_loss: 0.5321 - val_accuracy: 0.7431\n",
            "\n",
            "Epoch 00346: val_accuracy did not improve from 0.99297\n",
            "Epoch 347/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5076 - accuracy: 0.7461 - val_loss: 0.5313 - val_accuracy: 0.7535\n",
            "\n",
            "Epoch 00347: val_accuracy did not improve from 0.99297\n",
            "Epoch 348/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5070 - accuracy: 0.7507 - val_loss: 0.5471 - val_accuracy: 0.7474\n",
            "\n",
            "Epoch 00348: val_accuracy did not improve from 0.99297\n",
            "Epoch 349/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5075 - accuracy: 0.7541 - val_loss: 0.5306 - val_accuracy: 0.7595\n",
            "\n",
            "Epoch 00349: val_accuracy did not improve from 0.99297\n",
            "Epoch 350/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5077 - accuracy: 0.7426 - val_loss: 0.5341 - val_accuracy: 0.7413\n",
            "\n",
            "Epoch 00350: val_accuracy did not improve from 0.99297\n",
            "Epoch 351/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5079 - accuracy: 0.7444 - val_loss: 0.5301 - val_accuracy: 0.7448\n",
            "\n",
            "Epoch 00351: val_accuracy did not improve from 0.99297\n",
            "Epoch 352/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5077 - accuracy: 0.7422 - val_loss: 0.5302 - val_accuracy: 0.7595\n",
            "\n",
            "Epoch 00352: val_accuracy did not improve from 0.99297\n",
            "Epoch 353/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5076 - accuracy: 0.7409 - val_loss: 0.5306 - val_accuracy: 0.7587\n",
            "\n",
            "Epoch 00353: val_accuracy did not improve from 0.99297\n",
            "Epoch 354/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5077 - accuracy: 0.7446 - val_loss: 0.5304 - val_accuracy: 0.7500\n",
            "\n",
            "Epoch 00354: val_accuracy did not improve from 0.99297\n",
            "Epoch 355/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5077 - accuracy: 0.7457 - val_loss: 0.5327 - val_accuracy: 0.7422\n",
            "\n",
            "Epoch 00355: val_accuracy did not improve from 0.99297\n",
            "Epoch 356/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5073 - accuracy: 0.7487 - val_loss: 0.5347 - val_accuracy: 0.7543\n",
            "\n",
            "Epoch 00356: val_accuracy did not improve from 0.99297\n",
            "Epoch 357/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5076 - accuracy: 0.7507 - val_loss: 0.5314 - val_accuracy: 0.7543\n",
            "\n",
            "Epoch 00357: val_accuracy did not improve from 0.99297\n",
            "Epoch 358/500\n",
            "144/144 [==============================] - 1s 8ms/step - loss: 0.5077 - accuracy: 0.7426 - val_loss: 0.5326 - val_accuracy: 0.7422\n",
            "\n",
            "Epoch 00358: val_accuracy did not improve from 0.99297\n",
            "Epoch 359/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5075 - accuracy: 0.7474 - val_loss: 0.5371 - val_accuracy: 0.7396\n",
            "\n",
            "Epoch 00359: val_accuracy did not improve from 0.99297\n",
            "Epoch 360/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5075 - accuracy: 0.7526 - val_loss: 0.5387 - val_accuracy: 0.7387\n",
            "\n",
            "Epoch 00360: val_accuracy did not improve from 0.99297\n",
            "Epoch 361/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5079 - accuracy: 0.7459 - val_loss: 0.5290 - val_accuracy: 0.7543\n",
            "\n",
            "Epoch 00361: val_accuracy did not improve from 0.99297\n",
            "Epoch 362/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5079 - accuracy: 0.7437 - val_loss: 0.5332 - val_accuracy: 0.7474\n",
            "\n",
            "Epoch 00362: val_accuracy did not improve from 0.99297\n",
            "Epoch 363/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5079 - accuracy: 0.7446 - val_loss: 0.5307 - val_accuracy: 0.7517\n",
            "\n",
            "Epoch 00363: val_accuracy did not improve from 0.99297\n",
            "Epoch 364/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5076 - accuracy: 0.7478 - val_loss: 0.5300 - val_accuracy: 0.7535\n",
            "\n",
            "Epoch 00364: val_accuracy did not improve from 0.99297\n",
            "Epoch 365/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5073 - accuracy: 0.7441 - val_loss: 0.5322 - val_accuracy: 0.7517\n",
            "\n",
            "Epoch 00365: val_accuracy did not improve from 0.99297\n",
            "Epoch 366/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5079 - accuracy: 0.7437 - val_loss: 0.5369 - val_accuracy: 0.7422\n",
            "\n",
            "Epoch 00366: val_accuracy did not improve from 0.99297\n",
            "Epoch 367/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5072 - accuracy: 0.7476 - val_loss: 0.5341 - val_accuracy: 0.7500\n",
            "\n",
            "Epoch 00367: val_accuracy did not improve from 0.99297\n",
            "Epoch 368/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5076 - accuracy: 0.7454 - val_loss: 0.5329 - val_accuracy: 0.7543\n",
            "\n",
            "Epoch 00368: val_accuracy did not improve from 0.99297\n",
            "Epoch 369/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5077 - accuracy: 0.7483 - val_loss: 0.5388 - val_accuracy: 0.7413\n",
            "\n",
            "Epoch 00369: val_accuracy did not improve from 0.99297\n",
            "Epoch 370/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5074 - accuracy: 0.7480 - val_loss: 0.5332 - val_accuracy: 0.7448\n",
            "\n",
            "Epoch 00370: val_accuracy did not improve from 0.99297\n",
            "Epoch 371/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5071 - accuracy: 0.7478 - val_loss: 0.5347 - val_accuracy: 0.7517\n",
            "\n",
            "Epoch 00371: val_accuracy did not improve from 0.99297\n",
            "Epoch 372/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5070 - accuracy: 0.7467 - val_loss: 0.5341 - val_accuracy: 0.7500\n",
            "\n",
            "Epoch 00372: val_accuracy did not improve from 0.99297\n",
            "Epoch 373/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5078 - accuracy: 0.7489 - val_loss: 0.5336 - val_accuracy: 0.7543\n",
            "\n",
            "Epoch 00373: val_accuracy did not improve from 0.99297\n",
            "Epoch 374/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5073 - accuracy: 0.7480 - val_loss: 0.5352 - val_accuracy: 0.7578\n",
            "\n",
            "Epoch 00374: val_accuracy did not improve from 0.99297\n",
            "Epoch 375/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5075 - accuracy: 0.7448 - val_loss: 0.5348 - val_accuracy: 0.7569\n",
            "\n",
            "Epoch 00375: val_accuracy did not improve from 0.99297\n",
            "Epoch 376/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5080 - accuracy: 0.7461 - val_loss: 0.5390 - val_accuracy: 0.7405\n",
            "\n",
            "Epoch 00376: val_accuracy did not improve from 0.99297\n",
            "Epoch 377/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5077 - accuracy: 0.7420 - val_loss: 0.5332 - val_accuracy: 0.7552\n",
            "\n",
            "Epoch 00377: val_accuracy did not improve from 0.99297\n",
            "Epoch 378/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5074 - accuracy: 0.7526 - val_loss: 0.5327 - val_accuracy: 0.7552\n",
            "\n",
            "Epoch 00378: val_accuracy did not improve from 0.99297\n",
            "Epoch 379/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5078 - accuracy: 0.7478 - val_loss: 0.5459 - val_accuracy: 0.7378\n",
            "\n",
            "Epoch 00379: val_accuracy did not improve from 0.99297\n",
            "Epoch 380/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5075 - accuracy: 0.7446 - val_loss: 0.5318 - val_accuracy: 0.7500\n",
            "\n",
            "Epoch 00380: val_accuracy did not improve from 0.99297\n",
            "Epoch 381/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5077 - accuracy: 0.7446 - val_loss: 0.5396 - val_accuracy: 0.7509\n",
            "\n",
            "Epoch 00381: val_accuracy did not improve from 0.99297\n",
            "Epoch 382/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5076 - accuracy: 0.7457 - val_loss: 0.5306 - val_accuracy: 0.7509\n",
            "\n",
            "Epoch 00382: val_accuracy did not improve from 0.99297\n",
            "Epoch 383/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5074 - accuracy: 0.7452 - val_loss: 0.5317 - val_accuracy: 0.7431\n",
            "\n",
            "Epoch 00383: val_accuracy did not improve from 0.99297\n",
            "Epoch 384/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5077 - accuracy: 0.7457 - val_loss: 0.5389 - val_accuracy: 0.7569\n",
            "\n",
            "Epoch 00384: val_accuracy did not improve from 0.99297\n",
            "Epoch 385/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5077 - accuracy: 0.7461 - val_loss: 0.5325 - val_accuracy: 0.7526\n",
            "\n",
            "Epoch 00385: val_accuracy did not improve from 0.99297\n",
            "Epoch 386/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5075 - accuracy: 0.7485 - val_loss: 0.5288 - val_accuracy: 0.7431\n",
            "\n",
            "Epoch 00386: val_accuracy did not improve from 0.99297\n",
            "Epoch 387/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5068 - accuracy: 0.7498 - val_loss: 0.5369 - val_accuracy: 0.7535\n",
            "\n",
            "Epoch 00387: val_accuracy did not improve from 0.99297\n",
            "Epoch 388/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5072 - accuracy: 0.7522 - val_loss: 0.5317 - val_accuracy: 0.7543\n",
            "\n",
            "Epoch 00388: val_accuracy did not improve from 0.99297\n",
            "Epoch 389/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5076 - accuracy: 0.7448 - val_loss: 0.5312 - val_accuracy: 0.7578\n",
            "\n",
            "Epoch 00389: val_accuracy did not improve from 0.99297\n",
            "Epoch 390/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5067 - accuracy: 0.7515 - val_loss: 0.5368 - val_accuracy: 0.7535\n",
            "\n",
            "Epoch 00390: val_accuracy did not improve from 0.99297\n",
            "Epoch 391/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5074 - accuracy: 0.7474 - val_loss: 0.5906 - val_accuracy: 0.7405\n",
            "\n",
            "Epoch 00391: val_accuracy did not improve from 0.99297\n",
            "Epoch 392/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5074 - accuracy: 0.7498 - val_loss: 0.5336 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00392: val_accuracy did not improve from 0.99297\n",
            "Epoch 393/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5075 - accuracy: 0.7467 - val_loss: 0.5311 - val_accuracy: 0.7543\n",
            "\n",
            "Epoch 00393: val_accuracy did not improve from 0.99297\n",
            "Epoch 394/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5068 - accuracy: 0.7526 - val_loss: 0.5345 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00394: val_accuracy did not improve from 0.99297\n",
            "Epoch 395/500\n",
            "144/144 [==============================] - 1s 6ms/step - loss: 0.5078 - accuracy: 0.7431 - val_loss: 0.5471 - val_accuracy: 0.7448\n",
            "\n",
            "Epoch 00395: val_accuracy did not improve from 0.99297\n",
            "Epoch 396/500\n",
            "144/144 [==============================] - 1s 7ms/step - loss: 0.5068 - accuracy: 0.7498 - val_loss: 0.5304 - val_accuracy: 0.7552\n",
            "\n",
            "Epoch 00396: val_accuracy did not improve from 0.99297\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00396: early stopping\n",
            "Training/validation time was: 559.892 seconds\n",
            "\n",
            " \n",
            "EVALUATING THE MODEL:\n",
            "45/45 [==============================] - 0s 3ms/step - loss: 0.5294 - accuracy: 0.7312\n",
            "Evaluate time was 0.175841 seconds\n",
            "0.731249988079071\n",
            "\n",
            "SUMMARY: \n",
            "With 450 cases, the max val_accuracy was: 75.9549 % and the test_accuracy was: 73.1250 %. \n",
            "It is a difference of 2.8299 %.\n",
            "Fitness history is: [1, 0.987500011920929, 0.9840909242630005, 0.9840909242630005] \n",
            "Cases history is: [600, 550, 500, 450] \n",
            "acc_val history is: [0.9908854365348816, 0.9857954382896423, 0.992968738079071, 0.7595486044883728] \n",
            "acc_test history is: [0.987500011920929, 0.9840909242630005, 0.9918749928474426, 0.731249988079071] \n",
            "Times history is: [925.6832382678986, 643.7173902988434, 386.4396514892578, 559.8915147781372]\n",
            "\n",
            "HILL CLIMBING: For the configured step, was possible to reduce the dataset to 450 cases, with test_accuracy of 0.731249988079071\n",
            "\n",
            "The fitness history is: [1, 0.987500011920929, 0.9840909242630005, 0.9840909242630005] \n",
            "The cases decrease history is: [600, 550, 500, 450]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3RSl536yrnr"
      },
      "source": [
        "## 4.1 First stage simulation results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqsp-KxojciJ"
      },
      "source": [
        "**Chart comparing the history of fitness values with the reduction of cases.**\n",
        "\n",
        "* Remembering that, due to the way the optimizer was implemented (aiming at the reduction of cases - climbdown), *only the iterations in which there was a reduction in the test accuracy will be recorded in this graph*. Note that a given round of iteration, with a smaller number of cases, does not always mean that it will deliver less accuracy than the previous round (stochastic characteristic of the tool).\n",
        "\n",
        "* In order to investigate the \"real\" variation of the test accuracy during optimization process, subsequent graphs should be analyzed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0NUO52BypuD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "outputId": "a956219b-e6eb-4bb2-a667-821e14d52692"
      },
      "source": [
        "# Tamanho da figura:\n",
        "plt.figure(figsize=(10,10))\n",
        "# Configuração dos parâmetros da linha:\n",
        "plt.plot(n_cases_hist, fitness_hist, color = 'black')\n",
        "# Título do gráfico:\n",
        "plt.title('Fitness vs. cases', fontsize=20)\n",
        "# Título do eixo \"x\":\n",
        "plt.xlabel('Case quantity',fontsize=18)\n",
        "# Título do eixo \"y\":\n",
        "plt.ylabel('Fitness',fontsize=18)\n",
        "# Escala de plotagem (linear, log, symlog, etc):\n",
        "plt.yscale('linear')\n",
        "# Inversão do eixo x (n_cases_hist) para correta projeção da redução de casos:\n",
        "ax = plt.gca()\n",
        "ax.invert_xaxis()\n",
        "# Inserir grid ao gráfico:\n",
        "plt.grid()\n",
        "# Controle da resolução dos eixos (quantidade de 'thicks'):\n",
        "ax.yaxis.set_major_locator(ticker.MultipleLocator(.005)) # Eixo 'y'.\n",
        "ax.xaxis.set_major_locator(ticker.MultipleLocator(25))   # Eixo 'x'.\n",
        "# Visualização do gráfico:\n",
        "plt.show()\n",
        "\n",
        "#Para salvar no Drive...\n",
        "#plt.savefig('/content/drive/My Drive/MESTRADO - UFES/Fitness.png', transparent=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zNwdrDSiOcp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc9724e2-ace4-4c9e-81be-9df9ab55c18d"
      },
      "source": [
        "# Plotando as listas com os dados do histórico de otimização:\n",
        "# (Caso queira armazenar tabulado no Excel, para segurança)\n",
        "\n",
        "print('Fitness history is:', fitness_hist)\n",
        "print('Cases history is:', n_cases_hist)\n",
        "print('acc_val history is:', acc_val_hist)\n",
        "print('acc_test history is:', acc_test_hist)\n",
        "print('Times history is:', time_hist)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitness history is: [1, 0.987500011920929, 0.9840909242630005, 0.9840909242630005]\n",
            "Cases history is: [600, 550, 500, 450]\n",
            "acc_val history is: [0.9908854365348816, 0.9857954382896423, 0.992968738079071, 0.7595486044883728]\n",
            "acc_test history is: [0.987500011920929, 0.9840909242630005, 0.9918749928474426, 0.731249988079071]\n",
            "Times history is: [925.6832382678986, 643.7173902988434, 386.4396514892578, 559.8915147781372]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkytXx-tk0k8"
      },
      "source": [
        "**Referring to the last model that delivered test accuracy >= 95 %, the following graphs are available:**\n",
        "\n",
        "* Training accuracy vs. training validation;\n",
        "\n",
        "* Loss in training vs. loss in validation.\n",
        "\n",
        "**Referring to the optimization history, the following graphs are available:**\n",
        "\n",
        "* History of test accuracies vs. cases reduction;\n",
        "\n",
        "* History of the max. validation accuracy (found in each round) vs. test accuracy (for each round)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OOGjOZGy3NV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b6de0a5a-835f-433d-9cda-da0dd5e1e58e"
      },
      "source": [
        "# -> ACURÁCIA:\n",
        "# Tamanho da figura:\n",
        "plt.figure(figsize=(10,10))\n",
        "# Definição dos parâmetros que serão plotados:\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "# Título do gráfico:\n",
        "plt.title('Best model: train_acc vs. val_acc', fontsize=20)\n",
        "# Títulos dos labels:\n",
        "plt.xlabel('Epoch',fontsize=18)\n",
        "plt.ylabel('Accuracy',fontsize=18)\n",
        "# Título da legenda:\n",
        "plt.legend(['Train', 'Validation'], loc='best', fontsize=18)\n",
        "# Visualização do gráfico:\n",
        "plt.show()\n",
        "#Para salvar no Drive...\n",
        "#plt.savefig('/content/drive/My Drive/MESTRADO - UFES/acctrainvsval.png', transparent=True)\n",
        "\n",
        "# -> PERDA (LOSS):\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Best model: loss vs. epochs', fontsize=20)\n",
        "plt.ylabel('Loss',fontsize=18)\n",
        "plt.xlabel('Epoch',fontsize=18)\n",
        "plt.legend(['Train', 'Validation'], loc='best', fontsize=20)\n",
        "plt.show()\n",
        "#Para salvar no Drive...\n",
        "#plt.savefig('/content/drive/My Drive/MESTRADO - UFES/losstrainvsval.png', transparent=True)\n",
        "\n",
        "# -> TESTE vs. REDUÇÃO DE CASOS:\n",
        "plt.figure(figsize=(10,10))\n",
        "line, = plt.plot(n_cases_hist, acc_test_hist)\n",
        "plt.title('Optimization: evaluate_acc vs. cases', fontsize=20)\n",
        "plt.xlabel('Cases quantity',fontsize=18)\n",
        "plt.ylabel('evaluate_acc',fontsize=18)\n",
        "plt.yscale('linear')\n",
        "ax = plt.gca() # Inversão do eixo 'x'.\n",
        "ax.invert_xaxis() # Inversão do eixo 'x'.\n",
        "plt.show()\n",
        "#Para salvar no Drive...\n",
        "#plt.savefig('/content/drive/My Drive/MESTRADO - UFES/testvscasehist.png', transparent=True)\n",
        "\n",
        "# -> HIST MÁX ACC VAL vs. HIST TESTE:\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(n_cases_hist, acc_val_hist)\n",
        "plt.plot(n_cases_hist, acc_test_hist)\n",
        "plt.title('Optimization: val_acc vs. evaluate_acc', fontsize=20)\n",
        "plt.xlabel('Cases quantity',fontsize=18)\n",
        "plt.ylabel('Accuracy',fontsize=18)\n",
        "plt.legend(['max_val_acc', 'max_evaluate_acc'], loc='best', fontsize=20)\n",
        "plt.yscale('linear')\n",
        "ax = plt.gca()\n",
        "ax.invert_xaxis()\n",
        "plt.show()\n",
        "#Para salvar no Drive...\n",
        "#plt.savefig('/content/drive/My Drive/MESTRADO - UFES/maxaccvalvstest.png', transparent=True)\n",
        "\n",
        "# SUMÁRIO DOS RESULTADOS DO 1º ESTÁGIO:\n",
        "print('OPTIMIZATION SUMMARY:')\n",
        "# Acurácia configurada:\n",
        "print('-> The target accuracy configurated was:',\n",
        "      \"%.2f\" % round((acc_target*100),2),'%')\n",
        "\n",
        "# Minimização de casos:\n",
        "print('-> The minimum number of cases computed above target accuracy was:',\n",
        "      n_cases_hist[-1],'cases')\n",
        "\n",
        "# Porcentagem de redução do dataset:\n",
        "print('-> (It represents',\"%.2f\" % round((((n_cases_hist[-1])/600)*100),2),\n",
        "      '% from total dataset)')\n",
        "\n",
        "print('\\nBEST MODEL SUMMARY:')\n",
        "\n",
        "# Melhor acurácia de validação encontrada no melhor modelo:\n",
        "print('-> BEST MODEL: Best validation accuracy found was:',\n",
        "      \"%.4f\" % round((acc_val_hist[-1]*100),4),'%')\n",
        "\n",
        "# Melhor acurácia de teste encontrada no melhor modelo:\n",
        "print('-> BEST MODEL: Test accuracy found was:',\n",
        "      \"%.4f\" % round((acc_test_hist[-1])*100,4),'%')\n",
        "\n",
        "# Diferença entre val_accuracy e test_accuracy:\n",
        "print('-> BEST MODEL: It is a difference of',\n",
        "      \"%.4f\" % round(((acc_val_hist[-1]*100)-(acc_test_hist[-1]*100)),4),'%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm8AAAJqCAYAAAB9+6c7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXxcV3338c9vtFqSLduyvEnyFu+eOHEsZyGBOCEJ2b0odkhLm9AWytPSwtOnLVAoBCiFrtA+LRTa0sDTUmLHcchKdidNIMHyEsdr7HjfZcuWrX07zx/3jjwazYw00kijkb7v1+u+xrrnzNwzo5nR1+fcc4855xARERGR9BBIdQNEREREpOcU3kRERETSiMKbiIiISBpReBMRERFJIwpvIiIiImlE4U1EREQkjSi8yZBjZhvMrF+vgWNmj5iZM7Np/XmcnjKzh/z2PJTqtgwGZvaw/3osTXVbZOANts+nSLIpvEnSmFm5mf2Hme03swYzu2Bm75rZ35hZSRKPM+y+mM1sqf+cH051W0QkNv9zuiHV7ZChTeFN+sw8fwVsBD4G7Ab+Efh3oB74Y+A9M7tvgJr0m8C8fj7GF/xjHOvn4/TUerz2rE91Q0REpH9lproBMiT8OfCnwEHgbufcjvBCM6sA/hP4qZnd6px7tT8b45w73J+P7x/jBHCiv4/TU865GqAm1e0QEZH+p5436RN/6PLPgRbg3sjgBuCcWwf8byAD+J6ZBcLu33GulpndZWa/MLM6MztnZo+Z2ayI4zngQf/HA/59nZkdDKvT5Zy38GFHf3j352ZW4x9nnZmV+fVmmNlPzazKH/p91cyuiPK8uwzdmtnBsPZE2x4JqzvbzL5lZpX+sZrM7JCZ/cDMSiOPBYQC71ciHnNp5OsYpa2L/ed4Ouw43zWzSfGel5n9rj/s3Whmp/y2FUbeJ1FmttvMms1sXIzyz/lt+HTYvpv84+/0h+MbzGy7mX3FzHL72qYobUj4eGaWYWafMrM3/fdWg5ntM7N/i/I+7nHdKMe51n99Yvaymtku/3c91v/ZzOxB//NV5f9Oj5jZ82Z2f29eoyjH/IiZPWtmZ/xjv2/eKROjw+rkmtl5/70YtfPAzL7nP7+7w/YtN7P/NLP3zPt+qDOzTWb2hxb2fdJN++KeeuB/fg9G7Cs0sz8xs1fM7Kj/vq0ysyfN7LqIug/Zpe+dGyM+pw9H1L3GvO+3k/5jHjGz75vZ5J48lxjt7/F3SsT9bjOzp+zS98MRM/uZmd3Sl7rSv9TzJn31cbz30Rrn3Ltx6v0b8GVgDnAjl8JIyErgDrxhvw3AlUAFcJOZfcA5t8ev91VgOXAF8A/AeX//eXpmCfA54DXgX4HL/WMHzWwZ8AbesO+Pgal+2YtmNsM5V9vNY38HGB1l/z3AVXhDyOHP91N4r8MvgGZgAfA7wD1mVu6cCw3JPuHfPui3e0PY4xyM1yD/D+A6wIDHgEPAYuB/AcvM7Abn3IEod/1r4CPAU8ALwE3AJ4CZwM0Rx3gI+A/gR865h+K1x/cj4C+BB4D/G6X8QbzX4ydh+z4HzMV7rZ4BcoHrgYeBpWZ2i3OurQfH7qmEjmdm2cDTwK3AEb/tF4BpwAq899XeROtG45x7y8z2AHeaWZFz7mx4uZld7bd9nXOu2t/9Dbyh/gPAGrxe2kl4n4dVwKOJvTydmdlX8F6bav+5nQYW4p0ycaeZXeecu+CcazSzR4FP4n3en4p4nBzgfuAU8POwom8B7cDbeKcqFOK9D//Bfw6/0Zf2xzEP77V7He99cA6YAtwL3GFm9zjnQu3civf99BW8z9kjYY+zIfQPM/st4AdAE/Ak3ntgFpc++9f2cvQgke+UUFu+ive9XIv3PXMEmAx8AO8UmJd6U1cGgHNOm7Zeb8DLgAM+0YO6/+XX/VLYvof8fQ5vyDW8/mf8/S9H7H/E3z8txnE2eG/tTvuWhh3n1yPK/t3fXw18MaLsz/2yzyTShrB6t+L1Su4FxoXtLwFyotS/DWgDvhej/Q/HOE7odXwobF8BcNZ/vA9G1P+cX/+FGM/rMDAlbH8m3h8wB1wd49iP9PA9U+q3qTJK2RL/sdZF7J8BWJT6X/fr3x+x/2F//9Jevq8TPd5f+vufjPy9AjlAcW/qxmnfF/zH+HSUsn/2y+4J23cWOArkRak/rrvjddOWm/zj/QIYHeO98e2wfdf5+x6L8lir/LK/i9h/WZS6Abz/CDjgmhjv42lh+7r7DB0EDkbsK4z2+vjv4ePArihlDtgQ4xiz8ULVPqAkouzD/udifS9/D4l+p9zmt3V/ZFtCz7E3dbUNzJbyBmhL7w3Y6X+ob+9B3W/5db8bti/05f5ylPoZ/pecA6aG7e/yxRxxvw3EDm//E6X+h/yyA0BGRNlUv+w/IvbHbYNfJ4jXw3EGmJXAa7oN2B+j/Q/HuE/odXwobN+v+/t+EqV+pv98HZ1DWuh5/U6U+3ycKIHB/wM3F5iUwHN8wX+sBRH7/8nff28PH2esX/+HEfsfpg/hLZHj+e/T83g9q5O7uX+P63bzOKEAvDFifzZeUDsFZIbtP+v/vrv8cU/Ca7I+2u8yrHwLcDpi3x68nqexEfuf9h9rYQ+PfZVf/8sR+7t8PnvwGTpIRHjr5tj/GPn58ffHC2/f9svvivNatgIjk/w7ivad8pTflhU9uH+P62obmE3DpjJYvBa5wznXZmZvAJcBi/CGIvqqMsq+4/7tVtd16C00zBDznJFozDuf7Bm8npS7nHN7I8oNL1w9hDcEPAbvD3tIcyLHi+Eq//aVyALnXKuZvY43VLcIr6ctXLTX6Yh/OybisXozWeIRvF7JB/Emu4SGEx/AG3J7NryymeXj9cSuwOu9GIk3FByStEvR9OJ4c/EC7NvOuePEl0jdmJxzR83sZeBWM5vvnNvpF92DFzC/7ZxrDbvLfwF/AOw0szV4n7df+r+7vroOr3d5lZmtilKeDRRHDPH+CG848qPAdwHMbALeUP0W59y28AcwsyLgT4A78XpF8yOOkdTff8Sxr8d7L1wHjMd7PpHH7ukwZ+g8uRvNbEmU8vF43wOzgU0JtjPR75Rr8QLZz+leInVlACi8SV+dxDsvpKwHdUN1ov3ROhXn8cH7g5cM0f5YtcYq80MOQFZPD+D/4X8a7/n+unPujSjV/h74LN6M1efxQmKDX/YQXo9fX4Ves1izYkP7o52nF+0cwtDrlBGlLFHr8c7z+piZfcEPzXfjBY/vhAcPM8vCC6BXA9vxzs+qwgsM4J1jlJOENvX2eKHXryeXjUmkbnce4VIA/py/70H/9kcRdf833pDXx4HP+1urmT0L/B/n3L4+tKMI72/JV7qpFxrGB++c0q/77f2uv+/X/cfp1HZ/wsNGYDrwK/++1Xjvx9F4wSppv/+IY6/AO1e0EXgReB+owzv/bine+buJHLvIv/2TbuoVJNRQT6LfKaOBc865BrqXSF0ZAApv0ldv4J3zcgveBICozCwD78sO4M0oVSbEuOtE/zYtLoPhP8+f4vV6fdE5999R6owH/hAvGHzAOXcxovyBJDUn9JpNjFE+KaLegHHONfg9QL+DF0B+TuzgsQwvSD3inPt4eIHfw9ldaEhUoscLBd2e9P4kUrc74QH4z/CCwR3AO865d8Ir+uH4O8B3/PffDXi9XquABWa2wDnX1Mt21AAB59zYnt7B7zl8BbjFzOY653bj/f5b6DxRBbz3yHTgq865h8ML/Bmfn+nhYdv921h/90bT9T8tX8frsSp3zu2KOPb38cJbIkKftULn3IUE7xtTL79TzgNFZjaiB6EskboyAHSpEOmrR/DOvVlhZgvi1PstvJlJe4gyREqUL0E/CN3g/7glrCg0tJmMHqBk+w5eD9IPnXN/GaPODLzP3gtRvmRL/fJIvXnOoddsaWSBeZdp+KD/4+YEHjOZHvFvHzSzYrzgsc05tzWi3kz/9vEoj5HoH8+eSPR4u/H+uC3swaUeEqkbl/9HdA3e5+oW4NeI0nMV5X6nnXOPO+dW4/UwXoZ3fmZvvQWM6ebzH80j/u2DZnYl3uzU55xzVRH1Qr+PdVEeI5Hf/zn/tssogZnNJHrv/kxgZ5TgFuDSd1OkdmJ/Tt/ybz8Yo7y3evOd8hbeqQC39+DxE6krA0DhTfrEObcfb/ZcFvCkmc2PrGNmy/Gm9LcB/8s51x5ZB7jZwq7r5Ps03h+WV51z4ee7hYZepvS1/clkZp/Fa/NLeFP2Yzno397gB9TQ/Qvwei+j9Qz05jk/gTe89ICZXRtR9lm83oyXXB8vamzetbDmWpTrxsXjnHsTbxbuMrzXK4vOl1cIOejfLo047gzgrxJsbk8kdDy/V+u7wAjgX/zLXYTfL9sPpwnV7aFH/Nvf9LdWvPPbwh8zxz9vi4j9WXjD1BB2GRszm+T/Pnt6qsK3/dt/jRZIzSw/yvsPvHB8Ae8yEw9FPJ9wB/3bpRGPuwhv1m1P7faPt8zvqQo9zgi8yQfRHARmhT8v/9yyh4Eu33W+s8Q+jeSf8HoXv21msyML/d9/b4LdQf82ke+U0GV6/s6iLF8YsS+RujIANGwqyfAw3gnEfwS8Y2bPAzvw/hh/ALgG79yLB1zs1RWeAtabd+HRfXjXebsDL3z8XkTdl/HOGflXM1sHXATOO+f+KZlPKhFmNhH4O7yTercDX/TPlQu31Tn3hHPupJn9FG/YaquZvYD3v/5b8c6t2Yr3/MPtwTuH5aNm1oI3ecMB/y8i2HZwztX615RaC7xmZmvxTqxejDf1/yTwu3175oB3Uv9/4PX4PJTgfUPnPv05UYKH7ym898QfmdnleD2KU/B6OJ8h+SG+N8f7Kt77/B68peCexntfluG91n/CpWCSSN24nHNvmtk+vOHPLOAp59zpiGojgDf8epvw3ju5eO+3ecCTET1L38Qbwvx4T9rhnHvZzD7v32+vfx7dAbzztqbi9Y69QUSvjT90vhb4bbzP+Fm81zfSj/Fek++Y2U14gX8W3u/jcbzrwnXLOddiZv+A917b4n/XZOK9DseJfi7ut4F/8euvwwte1+MFt6fwfoeRXsb7nD6F16vdArzunHvdObfb/0z+ENhhZj8H3sP73U3B65GrwpvY0mO9+U5xzr1gZn8BfAnYZWaha7dNwOtVfAv/85xIXRkgqZ7uqm3obHjnCf0I74u7Ae9ijtuBvyXGdYAIu8QF3pfxL/FOCD6PN0wyO8b9/gjYhXe5AUfYFH/iXyrk4SiPNY041ykjytR/Ii5FEPYY8bZHwu6fhzfbbh/el+sRvOtzFUVrv3+fJXh/GGrwhmY6LoVBlEuFRNxvPd4fhWa8APc9olyqIvJ59eQ1JMHrvEXcdwpej6zDCx6x6pXhBbvQSdg78GapZsb4/Twc/vr0ol0JHc+/TyZez+uv8N77dXhB4wfAzN7W7UFbvxT2HquIUp7lt/05/3ff6L8X3sLr8cyO8R7o8l7qph034A3jHvffZ1V4oeHv8c4Zi3WfUNv/b5zHno93XbzT/mu1Ce9cuGnR3nux3sd4Q3+fx5t4EPos/DXe5/EgUS4V4r+/t/rHPYP3Wbo81nsMb8boT/AmYYXe25Gfmcv9Nh7C+w6rxvuu/D5wcy/fswl/p/j3uxPvnNNqvy1H/OfYpR2J1NXWv5v5vxCRlLBLV+f/uHPukdS2RkREZPDTOW8iIiIiaUThTURERCSNaMKCiAx5ZvZwD6s+4bpeqkRkwPmnlEzrQdWtzrkn+rc1MtjonDcRGfLMrKdfdDr3UgYFM9tAz65j9yPn3EP92xoZbBTeRERERNLIsBk2HTdunJs2bVqqmyEiIiLSrU2bNp1xzkW9aPewCW/Tpk2jsrIy1c0QERER6ZaZRb0AO2i2qYiIiEhaUXgTERERSSMKbyIiIiJpROFNREREJI0ovImIiIikEYU3ERERkTSSsvBmZj80s9Nmtj1GuZnZP5rZPjPbZmZXhZU9aGZ7/e3BgWu1iIiISGqlsuftEeD2OOV3ALP87ZPA9wDMbCzwFeAa4GrgK2Y2pl9bKiIiIjJIpCy8OedeB6rjVFkG/Nh53gJGm9kk4CPAi865aufcOeBF4odAERERkSFjMJ/zVgIcCfv5qL8v1n4RERGRIW8wh7c+M7NPmlmlmVVWVVWlujkiIiIifTaYw9sxoCzs51J/X6z9XTjnfuCcK3fOlRcXR13bVURERCStDObw9iTwm/6s02uBGufcCeB54DYzG+NPVLjN3yciIiIy5GWm6sBm9t/AUmCcmR3Fm0GaBeCc+xfgWeBOYB9QD3zcL6s2s68DG/2H+ppzLt7EBxEREZEhI2XhzTn3QDflDvj9GGU/BH7YH+0SERERGcwG87CpiIiIiERQeBMRERFJIwpvIiIiImlE4U1EREQkjSi8iYiIiKQRhTcRERGRNKLwJiIiIpJGUnadNxERGT5a29o539BCdV1zzO1cvXd76/wJfPaW2alussigpfAmIiIJcc5R39x2KXjVN1Nd64Wvs3XNnKu7dBsqr2lowbnojzcyJ5Mx+dmMzc+mobmN7214n49fP53CEVkD+8RE0oTCm4jIMNfW7jhX3zl0dbqt79pL1tTaHvWxMgPmBbE8L4zNmzSKsfnZjMnPpig/m3E5rUzIuEiRq2EM58lvPUdWw1moq4K6KmqrT/D35y7jqXfm87Frpw7wKyGSHhTeRESGmPrm1rhDk2cjesnOx+kVK8jJ7AhfE0blMnfiKIoKshmT54WxMSMyGJ9ZRxE1jHY15LdUY/WHoPa0H8jOQFUVHDzt/bulPvqBskdCQTH5LQ38n6yt/ObGCoU3kRgU3pJo6d+8Smu7oyAnk1G5WRTkZjIyN5OCnExG5mYx0v/Z25fVURZeNytDc0hE5JK2dsf5+uihq0vvWK03RNnYEr1XLCNgl0JXfhbzJo5iTH4WY/NzGJuXxdiCHMbmZVOU3co4q6Gw/RzZjdVQd7ijZ4y6Ki+M1fr/rj8LREl+lgH5xf42DsbOgILx3r879oeVZ43w7rbvJfL/s4KxJ15n98nFzJ04qh9fXZH0pPCWRDfPncD5+mYuNLZS29TCqQuNvF/VysXGVi42ttDSFuO/tmFyMgOMzM1iVG5ml/DnBb3Q/ix/f+dgWJCTSX52JoGADcAzFpFENTS3cbauiXN1Ld5tWCiL1lPWXa9YKHwVF+QwZ8IoxobCWPjtiEyKrJaC1moCDWegzu8VC/WOVZ3xg1k3vWM5oy6Fr6LLYMq1nQNYwfhLP+eOhkAv/jM6/UbaRxSxrP2XrNm4ii/fMz/xxxAZ4hTekijel4xzjqbWdi42tlLb5IW52sZWP+h5P1+M+Hfo56qLddSGfm5ujflFHmIGBdl+mAsLd516AHM6h8BRoX+HhcDcrIwkv0IiQ0t7u4uYQdlEdV1L59t67zYU1uL3imV5Q5R52cyZOLKjl+zSOWM5jMnPoig/h9GZzeQ2nfXCVkevmB++zlTBoQR7xwr8QBYKYvnFkB/eU3apd6xfZWQRWLCMWzf9hG9s3sfn75hLdqZGJETCKbwNEDMjNyuD3KwMikfm9Ppx2tsddc2tMYNerFBYXdfMobP1Hb2AsU42DpedEYjo/fPDX07nYHiprGtQLMjJJEO9gJImGlvaog9JRpk9WV3XzPn6Ztpj/GcqPzvj0kn6BTnMnjDSO4m/4NLJ/OHbqOwAgcbqsCB27FLP2JkqOBTWY9aj3rHxnXvHog1Z9rZ3rL8FK8ip/CGLm3/Fy7uWcMflk1LdIpFBReEtzQQC5gekvk2hb25tp7ap1Q96nXv9vNtLw72hn2sbWzlSXd+pbqw/XOHyszO6BL1RYf8uiBMKQz2CuVkBzBQCpefa2x01DS0d54PFG5oMbQ0tbVEfK2B09IiNzc9m1viCLuErVB46mT83KwOa6/wAdgbqjl8KX2fPwOGqsLIe9I4V+IGraGb0c8YKxkPeOMjK7d8XdiBMuQ43chL31b7NjyrvVXgTiaDwNkxlZwYYm+n9wekt5xwNLW1Rg17UHsGwUHj8fEPHz/XN0f9ghssMGAXhQ7+dgl6UcwJzLg0BjwxNDtGEkLTW2NIWO3iFnaxf7feQnYvTK5aXndERuIoKLoWxUE9Z5O2o3CzvPNL2Ni9kdfSOnfECWGiYsi6R3rHizueOdZwvFjFkOVh7x/pTIANbsIIb3v5XPvPeIU7WLGRi4RAIpSJJovCWTK/9Dbh277yQTlued5sZ8XNoy8z1TlRLM2ZGXnYmedmZTOjDhLDWtnbqmtq4EN7L19QSFgq7/nyxsYWTFxq5ePpSOOzJhJDcrEDX8Bce9OL0AIZ+zsvK0ISQPgr1ilXH6AkLn0EZ6jGLFfIDRkeP2Bi/V6wjfIX1hIX3kHU6n7Op9lLYqjvoha+zVXD4TNhJ/d30jgUyOwevaL1joZ6zodI71t+CFWS+9V1usUrWbV7E7980M9UtEhk0FN6S6a1/hoZzvbtvVp4X4iKDXdTwF9oXXj90/27KAoNvEkJmRoDCvACFeb0fCg6fEBJ36DfiPMGLjS1UXWzqGBbu8YSQnOhDvx1BL+wcwWg9gCNzM8nJHHy/i95qbGnr8dBkdQ96xcJD18zigo6r73fZ8rIpHJHVOUx36h074Z20f7bKG6asi9zi9Y4Vdp5ZOfW6zsOU+WE9ZcOxd6y/lSyG0VP4jYZKPlN5O7+39DKdOiHiU3hLps8dhLZWaG2Alkbvj0JLg3fbGv5z+Obva43c59dvroe6s2HlofqNvWtjRk6MYBgR9rIigmSsXsNO4TDs/hkD+9bqzwkhF/xgF69H8NKEEO/nnk4I6bjES5eh3ojgF6NHsD8mhLS3Oy40RlmDMsrQZOgk/roYvWIW1is2Ni+by4oLKJ92aQZltHPGRmRHCbUdvWOnwnrGwoYtO3rOEukdm9X5BP7wE/rVO5Z6ZhCsYOGb/8iFmpP86kA118woSnWrRAYFhbdky8iEjJGQM7J/j9Pe7gfCeOEwLOiFB8mWiLqhYFh70t8XUT/aH8LuBDK7CX/RegZ70euYkZ3UIedkTwiJd+7fBX9mcHhZbyeEdMzyjQx6kSEwJ5PmtvaYQ5Ner1gLbTEOPCLr0rliY/KzmVFcEHNocmy+1ysWNVy2tUJD2MzKmD1jCfSOjZsZ0TsWcUL/iDFpeXrCsBasIPDGt1mes4lHK+cpvIn4FN7SVSAA2XneRj9+oTkHbc097DWsjx7+IoNl/dmIsgZvVp7rfuJCFxboYfjrbc9i2P0T+MOfrAkh9c1tMUNgtJ9rm1qpaWjh2LlLITDWuWKhXrExed51w6aPy2fx1LFdL/IadnmLqL1iXmO932FdFdQdgerTcCTipP6Ee8fGd+4d63JCfzFk9r6XVdLAhCCMm83HGiq5690P89V7F/T5P1YiQ4HCm8Rn5v2BzMzxei76U1tLz4eTI8NftF7HxgteaIjsdWxr6l37MiODYYwh46xcovca9rDX0T8v0czIz8kkPyeTCaN6P4QXPiHkYmMr2ZnG2Pyc2L1iIR29Y0fh7OlLJ/CHXww29HNtlfe7iibUO1YwPn7vWIF/3TH1jkmIP3Q6Y8O3GNVylqe3neCBq6ekulUiKafwJoNHRpa35fbzWobtbV2HjGP1LEYdco5S1lDduSz02L2Rkd2HiSpdew0zM0dQmOVtjBzhta06YmiyNmKYsu401FcTv3esuPO5Y+FLI3Wc0D9OvWPSNwtWYhu+yUOjt7CmcrrCmwgKbzIcBTIgp8Db+pNzET2DSZjEUnsqevB03U+QiCun8NKlLDp6xyKuyB86oV+9YzKQimfDxMupqH+bvz58M3tPXWTWhH4+p1hkkFN4E+kvZpd6wRjbf8fpOC8x3kSVsLKsEV1P5lfvmAxmwQomvPQw0wJVrKk8whfv0mL1MrwpvImku07nJY5OdWtEkm/BSnjpYf5w4rt8Y3MJf3r7XK2WIsOa3v0iIjK4jZkKpUu4tf1NztY188ru06lukUhKKbyJiMjgF6xg5PldlBecYW3lkVS3RiSlFN5ERGTwm78cMD4z8V1e3VPF6Qu9XGVGZAhQeBMRkcFv1CSYdgPX1L1KW3s76zYfS3WLRFJG4U1ERNJDcCXZ5/ZRUVLD2sojONeLpftEhgCFNxERSQ/zloFl8DtjNrP/TB2bDp1LdYtEUkLhTURE0kN+EVx2E3POvEh+doA1mrggw5TCm4iIpI9gBYHzh/jUzBqe3naC2qbWVLdIZMApvImISPqYexdkZLN6xK+ob27j2W0nUt0ikQGn8CYiIukjtxBm3sr4w89y2bgRGjqVYUnhTURE0ktwJXbxBJ+ZdYbKQ+d4v6o21S0SGVAKbyIikl7m3AFZedzS/iYZAWNt5dFUt0hkQCm8iYhIesnOh9m3k7f3KT48u4h1m4/S2tae6laJDBiFNxERST/BCqg/y++WHaHqYhMb9lSlukUiA0bhTURE0s/MWyBnFIsuvMK4ghxNXJBhReFNRETST1YuzL2bwO6nWX1lMa/sPk3VxaZUt0pkQCi8iYhIegpWQFMNHyveR2u744ktWqxehgeFNxERSU8zboQRY5l85FmumjKaR7VYvQwTCm8iIpKeMrJg/jLY8ywPXFnEvtO1bDlyPtWtEul3Cm8iIpK+ghXQUs89edsZkZXBWk1ckGFA4U1ERNLX1A9AwURyd6/nroWTeOqdE9Q3a7F6GdoU3kREJH0FMmDBCtj7Ig8sLKS2qZVn3z2Z6laJ9CuFNxERSW/BCmhr4qqGXzCtKE/XfJMhT+FNRETSW2k5FE7Btj/OqvIyfnWgmgNn6lLdKpF+o/AmIiLpzQyCK2H/q6yaP4KAwWOb1PsmQ5fCm4iIpL9gBbS3Mv7ICyydM57HNh2lrV3XfJOhSeFNRETS38TLoWgWbF/H6vJSTl1o4vX3tFi9DE0KbyIikv7MvN63g29wc4mjKD9bExdkyFJ4ExGRoSG4EnBk73mSFYtKeGnXKc7WarF6GXoU3kREZGgongMTLocd3qzTljbHE1uPp7pVIkmn8CYiIkNHcCUceZs5uee4omw0a7VYvQxBCm8iIjJ0BFd6tzvWs7q8lN0nL7LtaE1q2ySSZApvIvNVk9UAACAASURBVCIydIyZBiXlsH0d91wxmdysgCYuyJCj8CYiIkNLsAJOvMOo2kPcGZzEk1uP09DclupWiSSNwpuIiAwtC5YD1jFx4WJTK8/v0GL1MnQovImIyNAyajJM/QC8+xjXTBvDlLF5PLpRQ6cydCi8iYjI0BNcCWf2EDizi1WLS/nl/rMcPluf6laJJIXCm4iIDD3zloFlwPZ1VCwuxbRYvQwhCm8iIjL0FBTDjBth+zomF+byoVnFWqxehgyFNxERGZqCFXDuIBzfzOryMo7XNPLmvjOpbpVInym8iYjI0DT3bghkwfbHuWX+eEbnZfGorvkmQ4DCm4iIDE0jRsOsW2H74+QEjOVXlvDijlOcq2tOdctE+kThTUREhq5gBVw8DkfeYnV5Gc1t7fxs67FUt0qkTxTeRERk6Jp9O2SOgO3rmD95FJeXFLKm8miqWyXSJwpvIiIydOUUwJzbYccT0NbK6vJSdp64wPZjWqxe0pfCm4iIDG3BCqg/Awdf594rSsjO1GL1kt4U3kREZGibeStkj4Tt6yjMy+L2BRN5YssxGlu0WL2kJ4U3EREZ2rJyYd7dsOspaG3i/iVlXGhs5YWdp1LdMpFeUXgTEZGhL1gBjTXw/itcN6OIktEjWKuhU0lTCm8iIjL0zVgKI8bA9nUEAsaq8lLe2HeGo+e0WL2kH4U3EREZ+jKyYP4y2P0sNNdz3+JSAB7bpMuGSPpReBMRkeEhWAEtdbD3eUrH5HH9ZeNYW3mUdi1WL2lG4U1ERIaHqddDwQTYvg6A1UvKOHa+gV/uP5vihokkRuFNRESGh0AGLFgB770AjRe4bf4ERuVm6ppvknYU3kREZPgIVkBbE+x5ltysDJYvKuG57SepqW9JdctEekzhTUREho/SJVA45dLQaXkZza3tPPmOFquX9KHwJiIiw4cZBFfA+69AfTXBkkLmTxqlxeolrSi8iYjI8BKsgPZWb8UFYHV5Ke8eq2Hn8QspbphIz6Q0vJnZ7Wa2x8z2mdnno5RPNbOXzWybmW0ws9KwsjYz2+pvTw5sy0VEJG1NXAhFMzuGTpddWUJ2RoC1mzRxQdJDysKbmWUA/wzcAcwHHjCz+RHV/hb4sXNuIfA14JthZQ3OuSv97d4BabSIiKQ/M6/37eD/wMVTjMnP5tYFE1i/5RhNrVqsXga/VPa8XQ3sc87td841Az8FlkXUmQ+84v/71SjlIiIiiVuwElw77PwZ4E1cOF/fwks7T6e4YSLdS2V4KwHC+6iP+vvCvQOs9P+9AhhpZkX+z7lmVmlmb5nZ8v5tqoiIDCnj58L4BR1DpzfMHMfkwlxd803SwmCfsPDHwI1mtgW4ETgGhPq0pzrnyoFfA75jZpdF3tnMPukHvMqqqqoBa7SIiKSB4Eo48hacP0JGwLhvcSmv763i+PmGVLdMJK5UhrdjQFnYz6X+vg7OuePOuZXOuUXAF/195/3bY/7tfmADsCjyAM65Hzjnyp1z5cXFxf3yJEREJE0F/YGdHesBuG9xGc7B45t12RAZ3FIZ3jYCs8xsupllAx8FOs0aNbNxZhZq4xeAH/r7x5hZTqgOcD2wc8BaLiIi6W/sDJh8VcfQ6ZSiPK6bUcQaLVYvg1zKwptzrhX4NPA8sAtY45zbYWZfM7PQ7NGlwB4zew+YAHzD3z8PqDSzd/AmMnzLOafwJiIiiQlWwImtcPZ9AFYvKeVwdT1vH6hOccNEYjPnhsf/LsrLy11lZWWqmyEiIoNJzTH49ny46Utw45/Q2NLGkm+8xK3zJvD391+Z6tbJMGZmm/xz+7sY7BMWRERE+k9hCUz5QMfQaW5WBvdeMZlnt5/gQqMWq5fBSeFNRESGt+BKqNoFp7yzb1aXl9HY0s5T7xxPccNEolN4ExGR4W3+crBAR+/bwtJC5kwYqcXqZdBSeBMRkeGtoBim3+iFN+cwM1YvKeOdI+fZc/Jiqlsn0oXCm4iISLACzh2A41sAWH7lZLIyjLVacUEGIYU3ERGReXdDIKtj6LSoIIdb5nmL1Te3tqe4cSKdKbyJiIiMGAMzb/FWW2j3wtrq8jLO1jXzyu5TKW6cSGcKbyIiIuANnV44BkfeBuCDs8YxYVSOJi7IoKPwJiIiAjDnDsgc0TF0mpkR4L7FpWzYc5pTFxpT3DiRSxTeREREAHIKYPZHYOcT0NYKwKrFZbQ7WKfF6mUQUXgTEREJCVZAXRUc/B8Apo3L5+rpY1lbeZThspykDH4KbyIiIiGzboXskR1Dp+BNXDhwpo6NB8+lsGEilyi8iYiIhGSNgLl3wa4nobUZgDsvn0hBTiZrdM03GSQU3kRERMIFK6CxBt5/BYC87EzuuWISz2w7QW1Ta4obJ6LwJiIi0tmMpd5138KGTleVl9HQ0sYz27RYvaSewpuIiEi4zGyYdy/seRZaGgBYVDaameMLeHSjhk4l9RTeREREIgUroLkW9r4A4C1WX17K5sPn2Xdai9VLaim8iYiIRJp2A+SP7zR0umJRKZkBY61WXJAUU3gTERGJFMiABSvgveehyetpKx6Zw81zx7Nu8zFa2rRYvaSOwpuIiEg0wQpobYQ9z3XsWl1expnaJjbsqUphw2S4U3gTERGJpnQJjCrtNHS6dE4xxSNzNHFBUkrhTUREJJpAAIIrYN/LUF8NeIvVr7yqhFf3nOb0RS1WL6mh8CYiIhJLsALaW2D30x27VpeX0dbuWL/5WAobJsOZwpuIiEgsk66EsTM6DZ1eVlxA+dQxrKk8osXqJSUU3kRERGIx83rfDrwOtac7dq8uL+P9qjo2Hz6fwsbJcKXwJiIiEk+wAlw77PxZx647F04iLzuDNZq4ICmg8CYiIhLP+Hkwfn6nodOCnEzuunwST287Tp0Wq5cBpvAmIiLSneBKOPxLqLm0usL9S8qoa27j2XdPpLBhMhwpvImIiHRnwUrvdsf6jl2Lp45hxrh8LZclA07hTUREpDtFl8HkRZ2GTs2MVeVl/OpgNfuralPYOBluFN5ERER6IlgBx7fA2fc7dlVcVUJGwFi7Sb1vMnAU3kRERHpiwQrvdsfjHbvGj8rlpjnFrNt0lFYtVi8DROFNRESkJwpLYcp1sP3xTrtXlZdx+mITr+/VYvUyMBTeREREeipYAad3wqmdHbtunjuecQXZrNmooVMZGApvIiIiPTV/GVig09BpVkaAFYtKeGnXKc7UNqWwcTJcKLyJiIj0VMF4mP4hb9Zp2Lqmq8vLaG13PLFFi9VL/1N4ExERSUSwAqr3w4mtHbtmTRjJoimjeXSjFquX/qfwJiIikoi5d0Mgq9M138Drfdt7upZ3jtakqGEyXCi8iYiIJCJvLMz8MGxfD+2XLg9y98JJ5GYFWFOpxeqlfym8iYiIJCpYAReOwtFfdewamZvFnZdP4qmtx2lobkth42SoU3gTERFJ1Jw7IDM36tDpxaZWntuuxeql/yi8iYiIJCpnJMz+COx4Atov9bJdM30s04ryNHQq/UrhTUREpDeCFVB3Gg6+0bErtFj9W/urOXS2LoWNk6FM4U1ERKQ3Zt0G2QVdhk5XXlVCwOAxLVYv/UThTUREpDeyRsDcu2DXk9Da3LF7UuEIPjS7mMc2HaWtXdd8k+RTeBMREemtYAU0nIP9Gzrtvr+8jBM1jfyPFquXfqDwJiIi0lszboLc0V2GTj88bwJj87NZW6mhU0k+hTcREZHeysyGeffA7megpaFjd3ZmgOVXlvDCzpNU1zXHeQCRxCm8iYiI9EWwApovwt4XO+1evaSUljYtVi/Jp/AmIiLSF9M+CPnFXYZO504cxcLSQtZUarF6SS6FNxERkb7IyIT5y+G956HpYqei1eVl7D55ke3HLqSocTIUKbyJiIj0VbACWhtgz8877b7nisnkZGqxekkuhTcREZG+KrsGRpV0GTotHJHFHcGJ/GzrMRpbtFi9JIfCm4iISF8FArBgBex7ybvuW5jV5WVcaGzl+R0nU9Q4GWoU3kRERJIhWAHtLbDr6U67r51RRNnYERo6laRReBMREUmGyYtgzPQuQ6eBgLFqcRlv7jvLker6FDVOhhKFNxERkWQw83rfDrwGtZ2XxapYXIppsXpJEoU3ERGRZAlWgGuHnU902l0yegQ3zBzHY5uO0q7F6qWPFN5ERESSZcJ8KJ4H2x/vUrS6vIxj5xt48/0zKWiYDCUKbyIiIskUrIDDv4Cazsti3bZgAqPzslijxeqljxTeREREkim40rvdsb7T7pzMDJZfWcLzO05yvl6L1UvvKbyJiIgkU9FlMOnKLrNOAVaVl9Lc2s6T7xxPQcNkqFB4ExERSbZgBRzfDNX7O+1eMLmQBZNH8ehGXfNNek/hTUREJNkWrPBuY0xc2HH8AtuP1Qxwo2SoUHgTERFJttFlUHZt1PC27MrJZGcGdM036TWFNxERkf4QrIDTO+D0rk67R+dl85EFE1m/RYvVS+8ovImIiPSH+cvAAjGGTkupaWjhpV2nUtAwSXcKbyIiIv1h5ASY9kFv1qnrvKrC9ZeNo2T0CE1ckF5ReBMREekvwQqofh9Obuu0OxAw7ltcyhv7znDsfEOKGifpSuFNRESkv8y7BwKZUa/5dt/iUpyDdZq4IAlSeBMREekveWPhsg97571FDJ2Wjc3j+plFrN10RIvVS0IU3kRERPpTsAJqjsDRjV2KVpeXcaS6gbcOnE1BwyRdKbyJiIj0pzl3QEZO1KHTjyyYyMjcTNZo4oIkQOFNRESkP+WOgtm3eQvVt3e+rltulrdY/XPbT1LT0JKiBkq6UXgTERHpb8EKqD0Fh97sUrS6vIym1nae0mL10kMKbyIiIv1t1kcgKz/q0GmwZBRzJ45kbaWGTqVnFN5ERET6W3YezL0Tdv4M2joPj5oZq8vLeOdoDbtPXkhRAyWdKLyJiIgMhGAFNJyD/Ru6FC1fVEJWhrFmo675Jt1TeBMRERkIl90MuYVRh07H5mdz2/yJrN9ylObW9hQ0TtKJwpuIiMhAyMzxVlzY9TS0NHYpXlVeyrn6Fl7WYvXSDYU3ERGRgRKsgOaLsO/FLkUfnFXMxFG5rNHEBemGwpuIiMhAmfYhyBsXdeg0w1+s/rX3qjhZ07VnTiRE4U1ERGSgZGTCguWw5+fQVNuleFV5Ke0O1m3WxAWJTeFNRERkIAUroLUB3vt5l6KpRflcO2MsayqP4JwWq5foFN5EREQGUtm1MHJy1KFT8FZcOHS2nl8dqB7ghkm6SGl4M7PbzWyPme0zs89HKZ9qZi+b2TYz22BmpWFlD5rZXn97cGBbLiIi0kuBAARXwt4Xveu+RbgjOImCnEzWVGroVKJLWXgzswzgn4E7gPnAA2Y2P6La3wI/ds4tBL4GfNO/71jgK8A1wNXAV8xszEC1XUREpE+CK6G9BXY/06VoRHYG91wxmWffPcHFRi1WL12lsuftamCfc26/c64Z+CmwLKLOfOAV/9+vhpV/BHjROVftnDsHvAjcPgBtFhER6bvJV8GYaTGHTu9fUkZDSxtPbzsxsO2StJDK8FYChF/M5qi/L9w7wEr/3yuAkWZW1MP7ioiIDE5m3sSF/a9BbVWX4itKC5k9oUDXfJOoBvuEhT8GbjSzLcCNwDGgrad3NrNPmlmlmVVWVXX9cIiIiKRMsAJcG+z6WZei0GL1Ww6fZ++piylonAxmqQxvx4CysJ9L/X0dnHPHnXMrnXOLgC/6+8735L5+3R8458qdc+XFxcXJbr+IiEjvjZ8PxXNh++NRi5cvKiEzYKzdpIkL0lkqw9tGYJaZTTezbOCjwJPhFcxsnJmF2vgF4If+v58HbjOzMf5Ehdv8fSIiIukhNHR66BdQ06X/gXEFOXx43nge33yUljYtVi+XpCy8OedagU/jha5dwBrn3A4z+5qZ3etXWwrsMbP3gAnAN/z7VgNfxwuAG4Gv+ftERETSx4KVgIOdT0Qtvn9JGWdqm3ll9+mBbZcMajZcruBcXl7uKisrU90MERGRzr7/IQhkwide6VLU2tbOB771CgtLC/m3B5ekoHGSKma2yTlXHq1ssE9YEBERGdqCFXBsE1Qf6FKUmRGgYnEpr+6p4vQFLVYvHoU3ERGRVFqwwrvdsT5q8arFpbS1O9Zt7npenAxPCm8iIiKpNHoKlF0Tc9bpjOICrp42lrVarF58Cm8iIiKpFqyAU+9C1Z6oxavKS9l/po5Nh7quhSrDj8KbiIhIqs1fDhaI2ft25+WTyM/O0IoLAii8iYiIpN7ICTD1em+t0yhDo/k5mdy9cDJPbztBXVNrChoog4nCm4iIyGAQrICze+Hku1GLVy8ppb65jWe0WP2wp/AmIiIyGMy717ve2/Z1UYuvmjKGy4rzNXQqCm8iIiKDQn4RzLjJO+8tytBpaLH6ykPneL+qNgUNlMFC4U1ERGSwCFZAzWE4Gn1FoBVXlZARMNZWarH64UzhTUREZLCYeydk5MQcOh0/Mpeb5oxn3eajtGqx+mFL4U1ERGSwyC2EWbd6qy20t0Wtsrq8lKqLTWzYUzXAjZPBQuFNRERkMAlWQO1JOPSLqMU3zR3PuIIcTVwYxhTeREREBpPZH4Gs/JhDp1kZASquKuGV3aeputg0wI2TwUDhTUREZDDJzoc5d8DOn0FbS9Qqq8pLaW13PLFFi9UPRwpvIiIig02wAhqqYf9rUYtnjh/JVVNG86gWqx+WFN5EREQGm5kfhpzCmEOnAPcvKWPf6Vq2HDk/gA2TwUDhTUREZLDJzIF598Dup6GlMWqVuxZOZkRWBms1cWHYUXgTEREZjIIroekC7HspanFBTiZ3LZzEU++coL5Zi9UPJwpvIiIig9H0GyGvKO7Q6eryMmqbWnnu3ZMD2DBJNYU3ERGRwSgjE+Yvh/d+Ds11UassmTaG6ePyeVRDp8OKwpuIiMhgFayAlnrY81zUYjNjVXkpvzpQzYEz0QOeDD0KbyIiIoPVlOtg5CTY/njMKhVXlRIweGyTet+GC4U3ERGRwSoQgAUrYd+L0BD9kiATRuWydM54Htt0lLZ2XfNtOFB4ExERGcyCFdDWDLufiVlldXkppy408fpeLVY/HCi8iYiIDGYlV8HoqXFnnd48dwJF+dms2aih0+FA4U1ERGQwM/N63/ZvgLozUatkZwZYsaiEl3ad4mytFqsf6hTeREREBrtgBbg2b7H6GFaVl9HS5nhi6/EBbJikQo/Dm5l9ycwm9WdjREREJIoJC2DcHNixPmaVORNHckXZaNZqsfohL5Get68Bh83sKTNbbmYZ/dUoERERCRMaOj34Blw4EbPa6vJSdp+8yLajNQPYOBloiYS3a4B/Bz4IrAOOmtm3zGx2v7RMRERELgmuBBzsfCJmlXuumExuVoA1WnFhSOtxeHPObXTOfQqYBHwceA/4U2CXmb1uZr9hZiP6qZ0iIiLD27hZMPHyuLNOR+VmcWdwEk9uPU5Dc9sANk4GUsITFpxzDc65HzvnbgTmAH8NXAY8Apwws++a2ZXJbaaIiIgQrICjG+HcwZhVVpWXcbGpled3aLH6oaqvs00PAJuAXYABBcAngE1m9owmOIiIiCTRgpXebZyJC9dMH8uUsXkaOh3CehXezGyBmf09cBx4FJgL/AUwAygDvgHcBPwwSe0UERGRMVOhdEncodNAwFi1uJRfvH+Ww2frB7BxMlASuVRIgZl9wszeArYBfwC8DSwHpjjnvuycO+ycO+mc+zLwMPCh/mi0iIjIsBWsgJPvQtV7MavcV16KabH6ISuRnrdTwL/gTVj4GjDdOXePc+4p51x7lPqHAE1gEBERSab5ywGDHY/HrDKpcAQfmlWsxeqHqETC24vAvXih7avOuaPxKjvnHnXOaQUHERGRZBo1Cabd4A2dxrkY7+ryMo7XNPLmvuhLakn6SuRSIcudc8/E6GUTERGRgRJcCWfeg1PbY1a5Zf54RudlaeLCEJTIOW8fNrNvxin/ppndlJxmiYiISEzzloFlxJ24kJOZwfIrS3hhxynO1TUPYOOkvyUyrPk5YGac8ul+HREREelP+UVw2U09GjptbmvnZ1uPDWDjpL8lEt6uAN6KU/62X0dERET6W7ACzh+GY5tiVpk/eRSXlxSypjLuaeqSZhIJb4VAXZzyBmBM35ojIiIiPTL3LsjIjjt0Ct5i9TtPXGD7MS1WP1QkEt6OAYvjlC8GtBaHiIjIQMgthFm3wfbHoT32Oqb3XlFCdmaAtZq4MGQkEt6eAR40s1siC8zsw8CDwLPJapiIiIh0I7gSak/C4V/GrFKYl8UdwYk8sfU4jS1arH4oSCS8fQOoAp43s6fN7C/87WngBb/s6/3RSBEREYli9u2QldeDodMyahpaeGHnqQFqmPSnRK7zdgr4APA8cAfwZ/52B/AccL1z7kR/NFJERESiyM6HOXfAzp9BW0vMatfNKKJk9AgNnQ4RCa2A4Jw75Jy7ExgHXONv45xzdzvnDvZD+0RERCSeYAXUn4UDr8WsEggYq8pLeWPfGY6e02L16a5Xy1c558455zb627lkN0pERER6aOYtkFPoTVyI477FpQA8tkmXDUl3vQpvZlZgZqVmNiVyS3YDRUREJI7MHJh3N+x6ClqbYlYrHZPHDTPHsbbyKO1arD6tJRTezOyjZrYdqAEOAQeibCIiIjKQgiuh6QLseylutVXlZRw738Av958doIZJf0hkbdPlwE+ATOD7gAH/DawFWoBNwNf6oY0iIiISz/QbIa+o21mnt82fwKjcTC1Wn+YS6Xn7Y2AXcCXwZX/fD51zHwXKgTnA1uQ2T0RERLqVkQXzl8Ge56A59mJIuVkZLF9UwnPbT1JTH3t2qgxuiYS3hcCPnHONQLu/LwPAObcd+AHwheQ2T0RERHokWAEt9fDez+NWW11eRnNrO0++o8Xq01Ui4S0DCA2SN/i3hWHle4BgMholIiIiCZpyHYyc1O2s02BJIfMnjdJi9WkskfB2FJgK4JxrAE7Tea3TOcRfuF5ERET6SyADFqyAvS9CY/xF6FeXl/LusRp2Hr8wQI2TZEokvP0CCF/X9Engs2b2ZTN7GPh9YEPymiYiIiIJCVZAWxPsjr/U+LIrS8jOCLB2kyYupKNEwtt3gQ1mNsL/+Yt4Q6UP401geB9vUoOIiIikQsliGD2l21mnY/KzuXXBBJ7YcoymVi1Wn24SWdt0o3Puz/whU5xzVc65K/Fmn14OXOGcU4QXERFJFTNYsBL2vwp18a/ldn95GefqW3hp5+kBapwkS4/Cm5nl+8OjH4ksc85tc87tcM61R7uviIiIDKBgBbS3wq4n41a7fuY4Jhfm6ppvaahH4c05Vwf8GVDWv80RERGRPpl4ORTN6nboNCNg3Le4lNf3VnH8fEPcujK4JHLO2/vAxP5qiIiIiCSBmdf7dvANuHgybtX7FpfhHDy+WZcNSSeJTlj4hJkV9VdjREREJAmCKwEHO56IW21KUR7XzShijRarTyuZCdS9CFQDe8zsR8BeoD6yknPux0lqm4iIiPRG8RyYcLk3dHrtp+JWvX9JGZ99dCtvH6jmusvUP5MOEglvj4T9+3/HqOMAhTcREZFUC66El78K5w7BmKkxq90enMjIn2WytvKIwluaSGTY9KYebDcnu4EiIiLSC8GV3u2O9XGr5WZlcO8Vk3l2+wkuNGqx+nTQ454359xr/dkQERERSaIx06Ck3Bs6veGzcauuLi/jv94+zNPvnODXrpkyMO2TXkuk501ERETSSbACTm6DM3vjVltYWsicCSN5VNd8Sws97nkzsy/3oJpzzn29D+0RERGRZFmwHJ7/M9j+OCz9XMxqZsbqJWV8/emd7Dl5kTkTRw5gIyVRiUxYeDhOmQPMv1V4ExERGQxGTYap18P2x+DGP/WuARfD8isn863ndrG28ghfunv+ADZSEpXIsOn0KNss4HbgBeAtYG6yGygiIiJ9EFwJZ96DUzviVisqyOGWeRNYv+UYza1a8XIwS2Rh+kNRtvedcy8AdwJtwMf7raUiIiKSuPnLwDK6XS4LvIkLZ+uaeWW3FqsfzJIyYcE554DHgN9MxuOJiIhIkuSPgxlLvfDm4q+i8KHZxUwcpcXqB7tkzjbNBnR1PxERkcEmWAHnD8GxzXGrZQSMisUlbNhzmlMXGgeocZKopIQ3MysHPgPsSsbjiYiISBLNvQsysns0dLpqcRntDtZpsfpBq8fhzcz2x9jOA28DxcCf9VtLRUREpHdGjIaZt8KOx6E9/mSEaePyuXr6WNZWHsV1M8wqqZFIz9th4FDEdhB4CfgrYI5z7ufJbqCIiIgkQXAlXDwBh3/ZbdXV5WUcOFPHxoPnBqBhkqhElsda2o/tEBERkf405w7IyvOGTqddH7fqnZdP5OEnd7Cm8ghXTx87QA2UntLyWCIiIsNBdj7Mvh12PgFtrXGr5mVncs8Vk3hm2wlqm+LXlYGXyDlv95vZj+OU/8jM7ktOs0RERCTpghVQfxYOvNZt1VXlZTS0tPHMtuMD0DBJRCI9b58G4p3l2Ab8Qd+aIyIiIv1m5i2QM8pb67Qbi8pGM3N8AWsqNet0sEkkvM0DtsQp3wJoMTQREZHBKisX5t4Nu5+C1qa4Vc2M1eWlbDp0jn2nLw5QA6UnEglv+Xi9a7E4YGTfmiMiIiL9KlgBjTXw/ivdVl2xqJTMgLFWvW+DSiLh7QBwQ5zyG/AuJyIiIiKD1YwbYcTYHl2wt3hkDjfPHc+6zcdoadNi9YNFIuFtPbDKzH47ssDMfgtYBXQ/iC4iIiKpk5EF8++F3c9Cc3231VeXl3GmtokNe6oGoHHSE4mEt2/hLX/1AzPbbmb/6W/vAv8K7AH+sj8aKSIiIkkUrICWOtj7fLdVl84ppnhkjharH0R6HN6ccxeB64HvA5OAX/O3ycD3gA845y4kcnAzu93M9pjZPjP7fJTyUAglvQAAIABJREFUKWb2qpltMbNtZnanv3+amTWY2VZ/+5dEjisiIjKsTb0eCib0aOg0MyNAxVWlvLL7NKcvarH6wSChi/Q652qcc78HjAMm+Ns459ynnXPnE3ksM8sA/hm4A2+W6gNmFjlb9UvAGufcIuCjwHfDyt53zl3pb59K5NgiIiLDWiADFqyA916Axu77XVaVl9LW7li/+dgANE6606sVFpynyt96u2rt1cA+59x+51wz8FNgWeShgFH+vwsBXSlQREQkGYIV0NYEe57ttuplxQWUTx3DmsojWqx+EEhkhYXfN7OX4pS/YGa/m8CxS4DwAfSj/r5wDwMfM7OjwLN0vgjwdH849TUz+2ACxxUREZHSJVA4pUdDp+BNXHi/qo7NhxMaaJN+kEjP20PA3jjl7wG/1afWdPUA8IhzrhS4E/h/ZhYATgBT/OHUPwJ+YmajIu9sZp80s0ozq6yq0iwZERGRDmYQXOFd762+utvqdy6cRF52Bms1cSHlEglvs4B345Tv8Ov01DGgLOznUn9fuN8G1gA4534J5OKdY9fknDvr798EvA/MjjyAc+4Hzrly51x5cXFxAk0TEREZBoIV0N4Ku57stmpBTiZ3L5zEU+8cp06L1adUIuEtCy88xZLbTXmkjcAsM5tuZtl4ExIi3z2HgQ8DmNk8//GrzKzYn/CAmc3AC437Ezi2iIiITFwIRTMTGjqta27j2XdP9HPDJJ5Ewtt7wK1xym/D6wHrEedcK95i98/jXT9ujXNuh5l9zcz+P3t3Hh91de9//PXJzpKEhDUkgYiyJgpIXK5YQRFEVLYgqG0vVKx2uy22avXeayvqbW39tffa67231lqXti6gAqKlgBbXimyyI4sS9i1EdsjG+f3xnaRZJmRmskwmeT8fj3nM5Ps95/v9zDfD5MM533POWF+xHwHfNLM1wEvANN8AiauAtWa2GngV+JZzru42XxEREfkHM6/1bfsHcHx/ncWH9EyhV6d2Wi4rzIJJ3l4CRpnZI76WMgDMLNbMZuIlby8Gc3Ln3F+cc32cc+c75/7Dt+0nzrk3fK83OueGOucG+qYEWeTb/ppzLtu37WLn3PxgzisiIiI+2RMBBxvn1VnUzLg5N5Nl+YV8cehE48cmfgWTvP0n8D7wb8BeM/vQzD7EGzzwIPAh8KuGD1FEREQaTZd+0DUn4K7TvIvTiY4yZq9U61u4BLPCQgle69r9eNN6DPY9dgH34d2bZo0Qo4iIiDSmnImw6xM4srPOol2SEri6b2deW7mbUi1WHxbBrrBQ4pz7pa+7sp3vMRhYAvwGTaIrIiISebInes8b5gRU/ObcTA4eL+L9rZqGKxxCWmEBwMxSzez7vkEDy4BvAfotioiIRJrU8yB9SMBdp9f060Kn9nHMWq6u03AIOnkzs+vM7BW8Odn+E4gHZgIXOuf6NXB8IiIi0hRy8mDfGijYVmfR2OgoJgxO5+1NBzh8oqgJgpPKAkrezCzLN4XHDrxlqobjTdEB8G/OuYedcxsaKUYRERFpbNkTAIMNrwdUfHJuJqVnHXM+1WL1Te2cyZuZfdXM3gG2AT8GVgAT8NYgfQgNUBAREWkZkrpDzytg3asQwOLzvbsmMrhHB15ZrsXqm1pdLW9/BHoCM4Duzrk859wbvgl2RUREpCXJmQgFm+HgxoCKT87NZOvBE6zZfbSRA5PK6kreioAsYBww2szaNHpEIiIiEh79x4FFBzxw4caL0kiIjWKWFqtvUnUlb2l4rW4d8Vrh9pvZM2Z2FeoyFRERaVnad4Zew7zkLYCu0MSEWMZcmMb81Xs5XVzWBAEK1JG8OeeOOOeedM5dDOQCf8K7520J3ooKDkhu9ChFRESkaeTkwZf5sHdVQMWn5GZyvKiUBeu1WH1TCWaFhVXOue/itcZ9HSgfXfp7M1ttZv9uZtmNEaSIiIg0kX43QlQsrA9s1Oml56WS1bGtuk6bUNDzvDnnipxzLzrnRgDnA/8BpAAPA2saOD4RERFpSm06QO+R3moLZ+te/qp8sfqlXxSy4/DJJghQQl5hAcA5l++c+wneoIYxQGBpuoiIiDRfOXlwbI+33mkAJl6cTpTBq1qsvknUK3kr5zx/dc5NbojjiYiISBj1GQ0xbQIedZqW3Iar+nTm1ZW7KTurOd8aW4MkbyIiItKCxLeHPtfBxrlQFtjUrlNyM9l39AwfaLH6RqfkTURERGrKyYOThyD/g4CKj+jfldR2ccxeoa7TxqbkTURERGrqPRLiEgPuOo2LiWL8oHQWbdxP4cniRg6udVPyJiIiIjXFtoF+N8CmN6A0sGRs8iUZlJQ55q3WYvWNScmbiIiI+JeTB2eOwud/C6h4v25JDMxI1mL1jUzJm4iIiPjXazi0SQm46xTg5txMPtt/nPV7jjVaWK2dkjcRERHxLyYO+o+FzX+B4lMBVblpYHfiY7RYfWNS8iYiIiK1y8mD4hOwdVFAxZPbxHJ9Tjfmrd7DmRItVt8YlLyJiIhI7bKuhHZdguo6nZybybEzpSzcsL8RA2u9lLyJiIhI7aKiIXuC1/J2JrD72C7v1ZHM1DbqOm0kSt5ERETk3HLyoPQMbF4QUPGoKOPmIZl8tO0wuwoDu1dOAqfkTURERM4t4xJIzgyq6zRvSAamxeobhZI3ERERObeoKK/r9PN34FRhQFXSO7Thygs68erK3ZzVYvUNSsmbiIiI1C0nD86Wwqb5AVeZnJvJniOn+ejzgkYMrPVR8iYiIiJ1SxsIqecH1XU6KrsrHdrGMkuL1TcoJW8iIiJSNzOv9S3/Azh+IKAq8THRjB+UzsIN+zlySovVNxQlbyIiIhKYnDxwZ2HjvICr3JybQXHpWd5Ys7cRA2tdlLyJiIhIYLr0gy7ZQXWdZndPJrt7kuZ8a0BK3kRERCRwORNh11I4EngyNuWSTNbvOcaGvUcbMbDWQ8mbiIiIBC5nove8YU7AVcYO7E5cTBSzNXChQSh5ExERkcCl9oLuFwfVddqhbRzXZXdjzqdarL4hKHkTERGR4OTkwb7VcPjzgKtMzs3g6OkS3t4U2EhVqZ2SNxEREQlO9gTvef3rAVcZen4n0ju00ZxvDUDJm4iIiAQnOR16XBFU12lUlDFpSAYfbD3EniOnGzG4lk/Jm4iIiAQvZyIc2gQHNgZcZdKQDJyD17RYfb0oeRMREZHgDRgPFgUbAu86zUxty9ALOjJ75S4tVl8PSt5EREQkeO07w3lXeV2nLvBEbHJuJrsKT7N0++FGDK5lU/ImIiIiocnJg8IvvJGnAbouuxuJCTGa860elLyJiIhIaPrdCFGxQQ1cSIj1Fqv/y7p9HD1d0ojBtVxK3kRERCQ0bVPhghGwfg6cPRtwtcm5mRSVnmW+FqsPiZI3ERERCV1OHhzbDbuXBV4lPYl+3RKZrcXqQ6LkTURERELX93qISQiq69TMmJybyZrdR/ls/7FGDK5lUvImIiIioYtPhD7XeQvVl5UGXG3C4HTioqOYtVwDF4Kl5E1ERETqJycPTh6CHR8GXCWlXRwjB3Rlzqe7KS4N/H45UfImIiIi9dV7FMS1D6rrFODm3Ay+PFXCO1qsPihK3kRERKR+YttAvxtg4xtQWhxwta/07ky3pARmaeBCUJS8iYiISP3l5MGZI/DFkoCrRPsWq39vyyH2Hz3TiMG1LEreREREpP56XQ0JHULqOj3r4LVVGrgQKCVvIiIiUn8xcTBgLHz2FpScDrhaz47tuLxXKrNW7MIFsUZqa6bkTURERBpGTh4Un4Cti4KqNjk3kx2HT7Fse2EjBdayKHkTERGRhpH1FWjXJeiu0+tz0kiMj2GWFqsPiJI3ERERaRhR0ZA9HrYshKLjAVdrExfNTYO685d1+zh+RovV10XJm4iIiDScnDwoPQObFwRVbXJuJqdLynhz7b5GCqzlUPImIiIiDSfjUkjKCLrrdGBGMn26ttecbwFQ8iYiIiINJyoKcibAtnfgVOADEMoXq/905xG2Hgi8y7U1UvImIiIiDSsnD86WwGdvBlVt/OB0YqKM2Ss1cOFclLyJiIhIw0obBKm9gu467dQ+nmv7d+X1VbspKdNi9bVR8iYiIiINy8xrfdv+Ppw4GFTVyZdkUHCimL99Fly91kTJm4iIiDS8nDxwZ2HjvKCqXdW7M10S45mtgQu1UvImIiIiDa9Lf+gyIOiu05joKPKGZLBk8yEOHtNi9f4oeRMREZHGkTMRdn4MR/cEVe3mIRmUnXW8/mlw9VoLJW8iIiLSOLInes8b5gRVrVfn9lyalcqs5Vqs3h8lbyIiItI4Op4P3QcH3XUKcHNuBl8UnGTlji8bIbDIpuRNREREGk/2RNi7Cgq/CKramAvTaBcXrRUX/FDyJiIiIo0ne4L3vP71oKq1i4/hxou68+bafZwsKm2EwCKXkjcRERFpPB0yIfPyoJM3gMmXZHKquIy31mmx+sqUvImIiEjjysmDgxvg4Kagql3cowPnd27HrOXqOq1MyZuIiIg0rgHjwKKCbn0rX6x+xY4v+fzQiUYKLvIoeRMREZHGldgVsr7ijToNcuqPCRenEx1lzF6hxerLKXkTERGRxpeTB4Wfw741QVXrkpjA1X278Nqq3ZRqsXpAyZuIiIg0hf43QVRMSHO+Tbkkk0PHi3h386FGCCzyKHkTERGRxtc2Fc4f4a22cDa4FrThfTvTqX285nzzUfImIiIiTSMnD47ugt3Lg6oWGx1F3sXp/O2zgxw6XtRIwUUOJW8iIiLSNPpeDzEJIS+XVXrWMVeL1St5ExERkSaSkAS9R/m6TsuCqnpBl0Qu7tGBWSu0WL2SNxEREWk6OXlw8iDkfxh01SmXZLL14Ak+3XWkEQKLHEreREREpOn0HgVx7UPqOr3hou60iY1mdisfuKDkTURERJpOXFvoOwY2vQGlxUFVbR8fww0XpTF/zT5OFbfexeqVvImIiEjTysmD01/CF+8GXXVybiYnikpZsG5/w8cVIZS8iYiISNM6/xpISA6p6/SSrBTO69SuVc/5FtbkzcxGm9lmM9tmZvf72d/DzJaY2admttbMxlTa94Cv3mYzu65pIxcREZGQxcRB/7Hw2VtQcjqoqmbGzbkZfLK9kPyCk40UYPMWtuTNzKKB/wGuBwYAt5rZgGrF/h2Y5ZwbDNwC/K+v7gDfz9nAaOB/fccTERGRSJCTB8XHYevioKvmXZxBlMHsla2z9S2cLW+XAtucc18454qBl4Fx1co4IMn3OhnY63s9DnjZOVfknNsObPMdT0RERCJB1legXeeQuk67JiUwvG8XXl25m7KzrW/Ot3Amb+lA5ZR5t29bZQ8BXzOz3cBfgH8Joq6IiIg0V9ExMGA8bFkIRceDrj45N4MDx4p4f2vrW6y+uQ9YuBV4zjmXAYwB/mhmAcdsZnea2QozW3HoUOv75YqIiDRrOXlQeho2/zXoqtf060rHdnGtcs63cCZve4DMSj9n+LZVNh2YBeCc+xhIADoFWBfn3O+cc7nOudzOnTs3YOgiIiJSb5mXQVJ6SF2ncTFRTBiczuKNBzh8onUtVh/O5G050NvMzjOzOLwBCG9UK7MTGAFgZv3xkrdDvnK3mFm8mZ0H9AaWNVnkIiIiUn9RUZA9Aba97c37FqSbczMpKXPMXb237sItSNiSN+dcKfA9YCGwCW9U6QYze9jMxvqK/Qj4ppmtAV4CpjnPBrwWuY3AX4HvOueCW+FWREREwi8nD86WeNOGBKlvt0QGZnZgditbrN5ay5vNzc11K1asCHcYIiIiUplz8JvBkHoefH1O0NX//MkO/m3Oet743lAuyujQCAGGh5mtdM7l+tvX3AcsiIiISEtmBjkT4Yv34ETwgwtvGtidhNgoXlneegYuKHkTERGR8MrJA1cGm+YFXTUpIZYxOWm8sXovp4tbxx1USt5EREQkvLoMgM79YP3rIVW/OTeT40WlLNzQOharV/ImIiIi4WXmtb7t+DscrTHzV50uOy+VHqltW81i9UreREREJPyyJwIONs4NumpUlDE5N4O/f36YXYWnGj62ZkbJm4iIiIRfpwsgbWBIE/YC5A3JwIxWseKCkjcRERFpHnLyYM9KKNwedNW05DZc1btzq1isXsmbiIiINA/ZE7znDaENXJicm8neo2f4aFtBAwbV/Ch5ExERkeahQw9vvdMQR51eO6ALKW1jW/zABSVvIiIi0nzk5MGB9XDws6CrxsdEM35wOos2HODIqeJGCK55UPImIiIizceA8WBRIXed3jwkk+Kys8z9NPgpRyKFkjcRERFpPhK7QtaV3qjTENZfH9A9iQvTk5m1YncjBNc8KHkTERGR5iUnDw5vg/1rQ6o+OTeDjfuOsX7P0QYOrHlQ8iYiIiLNS/+xEBUT8pxvYwemExcT1WLnfFPyJiIiIs1L21Q4/xpv1GkIXafJbWO5Pqcbc1fv5UxJy1usXsmbiIiIND85eXB0F+xeHlL1ybmZHD1dwqKNBxo4sPBT8iYiIiLNT98xEB0fctfpP/XqSHqHNi2y61TJm4iIiDQ/CUnQZxRsmANng+/6jIoybs7N4MNtBez+smUtVq/kTURERJqnnDw4cQB2fBRS9UlDMgB4bWXLmvNNyZuIiIg0T72vg9h2IXedZqS05coLOjF75S7OtqDF6pW8iYiISPMU1xb6jYGN86CsJKRD3Jybye4vT/PxF4cbOLjwUfImIiIizVdOHpz+Er54N6TqowZ0JSkhpkUtVq/kTURERJqv86+BhOSQu04TYr3F6hes38/RU6G13jU3St5ERESk+YqJh/43waY3oeRMSIeYnJtJcelZ3li7t4GDCw8lbyIiItK85eRB8XHYtji06unJDEhLYtbyltF1quRNREREmresq6BtJ2+5rBBNzs1g3Z6jbNx7rAEDCw8lbyIiItK8RcfAgHGw5a9QfDKkQ4wblE5cdBSzV0Z+65uSNxEREWn+cvKg5BRsXhBS9ZR2cYzM7srcT/dQVBrZi9UreRMREZHmr8c/QWJavbpOp+Rm8uWpEt7eeLABA2t6St5ERESk+YuKguyJ3qCF00dCOsTQCzrRPTkh4ud8U/ImIiIikSEnD8qK4bO3QqoeHWVMGpLB+1sPsffI6QYOrukoeRMREZHIkH4xdOgZ8oS9AJOGZOIcvL5qdwMG1rSUvImIiEhkMPNa3754F04WhHSIHh3bcsX5HZm1YnfELlav5E1EREQiR04euDJvsfoQTc7NZGfhKT7ZXtiAgTUdJW8iIiISObpmQ6e+9Rp1OjqnG4kJMcyO0IELSt5EREQkcpR3ne74CI6FtlZpQmw0Ywd25y/r93HsTOQtVq/kTURERCJLzkTAwYa5IR9icm4mZ0rO8uaafQ0XVxNR8iYiIiKRpVNv6HZRvUadXpSRTL9uiRE555uSNxEREYk8OXmwZwV8mR9SdTPj5txMVu86wpYDxxs2tkam5E1EREQiT/YE77keAxfGD+pObLQxa3lktb4peRMREZHIk9ITMi6tV/LWsX081/bvypxP91BcerYBg2tcSt5EREQkMuXkwYF1cGhzyIeYnJvJ4ZPF/O2zyFmsXsmbiIiIRKbs8YDVq/Xtqj6d6ZYUWYvVK3kTERGRyJTYDbKu9EadutCWuoqOMvKGpPPu5oMcOHamgQNsHEreREREJHLl5MHhrbB/XciHuHlIJmcdvBYhi9UreRMREZHI1X8sRMXUa863rE7tuPS8VGav2I0LsQWvKSl5ExERkcjVriP0utq7760eideU3Ey2F5xkxY4vGzC4xqHkTURERCJbTh4c3Qm7V4R8iOsv7Eb7+BheiYA535S8iYiISGTrNwai4+vVddo2LoabBqbx1tp9nCgqbcDgGp6SNxEREYlsCcnQeyRsmANny0I+zM25mZwuKeOttXsbMLiGp+RNREREIl9OHpzYDzv+HvIhBmd24IIu7Zm1onmPOlXyJiIiIpGvz3UQ2w42hD5hr5kxJTeTlTu+ZNvBEw0YXMNS8iYiIiKRL64d9B0NG+dBWUnIhxk/OJ2YKGN2M15xQcmbiIiItAw5eXDqMGx/L+RDdE6M55p+XXht1R5KyprnYvVK3kRERKRluOBaiE+u11qn4C1WX3CiiHc3H2qgwBqWkjcRERFpGWLiof+NsGk+lBaFfJjhfTvTOTG+2S5Wr+RNREREWo6ciVB0DLa9HfIhYqKjyLs4g799dpCDx5vfYvVK3kRERKTlOG8YtO1Yrwl7AW7OzaDsrGPOqj0NFFjDUfImIiIiLUd0LAwYB5sXQPHJkA9zfuf25PZMYdaKXc1usXolbyIiItKy5ORBySnY8td6HWZybiafHzrJqp1HGiiwhqHkTURERFqWHv8EiWn1HnV6w0VptI2LbnZzvil5ExERkZYlKhqyJ8DWRXDmaMiHaRcfw40XpTF/zV5ONqPF6pW8iYiISMuTkwdlxfDZW/U6zOTcTE4Wl/GXdfsaKLD6U/ImIiIiLU/6EOjQo96jTof0TKFXp3bMbkaL1St5ExERkZbHzGt9+3wJnDxcj8MYN+dmsiy/kC8ONY/F6pW8iYiISMuUkweuDDbNq9dh8i5OJzrKeHVl82h9U/ImIiIiLVPXHOjUp96jTrskJXB13868unI3pc1gsXolbyIiItIylXed5n8Ix+o34ODm3EwOHi/i/a3hX6xeyZuIiIi0XNkTAQcb59brMNf060Kn9nHMWh7+rlMlbyIiItJyde4D3S6s96jT2OgoJgxO5+1NBzh8oqiBgguNkjcRERFp2XLyYPdy+DK/XoeZnJtJ6VnHnE/Du1i9kjcRERFp2bInes8b5tTrML27JjK4R4ewL1YfE7Yzi4iIiDSFlJ6QcYnXdXrl3fU61A9H9qGoJLwjTtXyJiIiIi1fTh7sXweHttTrMF/p3ZlrB3TFzBoosOCp5c2PoqIiCgsLOX78OGVlZeEOR0SaQHR0NImJiaSmphIfHx/ucESkoQ0YD399ADa8DsPvD3c09aLkrZqioiJ27txJSkoKWVlZxMbGhjW7FpHG55yjpKSEY8eOsXPnTnr06KEETqSlSUqDrCu9rtNhP/bmgItQ6jatprCwkJSUFDp16kRcXJwSN5FWwMyIi4ujU6dOpKSkUFhYGO6QRKQx5EyEgi1wYH24I6kXJW/VHD9+nKSkpHCHISJhkpSUxPHjx8Mdhog0hv7jwKLrPedbuCl5q6asrIzY2NhwhyEiYRIbG6t7XUVaqnYd4fyrvbVOwzjVR30pefNDXaUirZf+/Yu0cNkT4cgO2LMq3JGETMmbiIiItB79boDouIjuOlXyJiIiIq1Hmw5wwUhvypCz4Z1sN1RK3kQaUFZWFllZWeEOQ0REziVnIhzfBzs/DnckIVHyJiIiIq1L3+shtm3Edp0qeRMREZHWJa4d9BkNG+dCWWm4owmakjcRERFpfXLy4NRh2P5euCMJWliTNzMbbWabzWybmdVYaMzM/tPMVvseW8zsSKV9ZZX2vdG0kbd8+fn5mBnTpk3j888/Z9KkSXTs2JHExERGjRrF+vXe7NSHDh3izjvvJC0tjYSEBC655BKWLFlS5Vh79+7l4YcfZujQoXTr1o24uDi6d+/ObbfdxsaNG2uce/z48ZgZv/nNb2rse/DBBzEzpk+fHtT7Wbp0KWbGhAkTai3Tv39/4uPjK2bXLy4u5sknn2TMmDH07NmT+Ph4UlNTufbaa1mwYEFQ5w9EqOfbvXs33//+9+nduzdt2rQhNTWVSy+9lEceeaReZUVEWrQLroX4JG/OtwhjLkyT1JlZNLAFGAnsBpYDtzrnav4198r/CzDYOXe77+cTzrn2gZ4vNzfXrVixos5ymzZton///oEetsXKz8/nvPPOY9iwYaxfv57+/ftz6aWXkp+fz5w5c0hNTeXjjz9m9OjRJCUlMWzYMAoLC3n55ZeJiopiy5Yt9OjRA4CXX36Z22+/nauvvpqsrCzat2/P1q1befPNN4mLi+Ojjz5i4MCBFecuLCxk8ODBHDhwgI8//pjBgwcD8M477zBq1Cj69evH8uXLadu2bVDvqV+/fmzfvp29e/fSsWPHKvuWLVvGZZddRl5eHq+++ioA+/fvJz09nSuuuIK+ffvSuXNn9u3bx/z58yksLOTpp5/mjjvuqHKc8sEK+fn5QcUW6vlWrFjBddddR2FhIVdddRWXX345p06dYuPGjbz77rtVJpsNpmxrp+8BkVZizrfhs7fg3q0Q07zWMzazlc65XL87nXNheQD/BCys9PMDwAPnKP93YGSln08Ec74hQ4a4QGzcuDGgci3d9u3bHeAA9+ijj1bZ9/DDDzvApaSkuLvuusuVlZVV7HvhhRcc4GbMmFGx7cCBA+7YsWM1zrF69WrXrl07N3r06Br7PvroIxcTE+N69+7tjh8/7vbv3++6devm2rRp49avXx/Se/rZz37mAPff//3fNfZ95zvfcYB74403KradOXPG7dq1q0bZI0eOuOzsbJeSkuJOnTpVZV/Pnj1dz549Q4ov2PMVFRW5rKwsB7g///nPNepVPlYwZUXfAyKtxpbFzv00yblNb4U7khqAFa6WnCacLW+TgNHOuTt8P38duMw59z0/ZXsCS4EM51yZb1spsBooBR5zzs091/kaouVt5vwNbNx7rM5jhNOA7kn89Kbseh+nvOUtKyuLbdu2ER0dXbFv586d9OzZk7Zt27J//34SExMr9pWVlZGQkMCVV15Zo/vUn7Fjx7Jo0SKOHz9eY1myxx57jAceeIDbbruNQ4cOsXjxYr+tT4HavXs3PXv25OKLL2b58uUV24uLi0lLSyMmJoY9e/YQExNT57F+/etf86Mf/Yj33nuPq666qmJ7fVregj3fa6+9xqRJkxg7dizz5s07Z/1gyopa3kRajbIS+H994PxrYNIz4Y6minO1vNX9V6p5uAV4tTyohTWVAAAgAElEQVRx8+npnNtjZr2Av5nZOufc55UrmdmdwJ1ARReeBGfQoEFVEjeA7t27A9CnT58qiRtAdHQ0Xbt2Zffu3VW2v/XWW/z2t79lxYoVFBQUUFpadXRPQUEBaWlpVbb9+Mc/ZsmSJbz44osA3HrrrSEnbgAZGRmMGDGCxYsXs3HjRgYMGABQ0S15991310jcNmzYwOOPP87777/Pvn37OHPmTJX9e/bsCTkef4I539KlSwG4/vrr6zxuMGVFRFqN6FgYMA7WvgLFJ71RqBEgnMnbHiCz0s8Zvm3+3AJ8t/IG59we3/MXZvYuMBj4vFqZ3wG/A6/lrb4BN0SLVqRJTk6usa08wfG3r3x/SUlJxc9PPPEEM2bMICUlhZEjR9KjRw/atm2LmTF37lzWrFlDUVFRjeOYGRMnTmTRokUAzJgxo97vZ9q0aSxevJjnn3+eX/ziFwA8//zzAEydOrVK2aVLl3LNNddQWlrKiBEjGDt2LElJSURFRbF69WrmzZvnN+5QBXu+I0e88Tvp6el1HjuYsiIirUpOHqx8FrYs9CbvjQDhTN6WA73N7Dy8pO0W4LbqhcysH5ACfFxpWwpwyjlXZGadgKHAL5skaglKaWkpDz30EN26dWPVqlU1Wtc+/rj22a23bt3KPffcQ0pKCkePHuWOO+5g2bJlJCQkhBzPhAkTSEpK4k9/+hM/+9nPOHz4MAsWLGDgwIFVBk0APProo5w+fZolS5YwfPjwKvt+/vOfN3j3Y7Dn69ChAxBY618wZUVEWpWeV0D7bt6EvRGSvIVtqhDnXCnwPWAhsAmY5ZzbYGYPm9nYSkVvAV52VW/O6w+sMLM1wBK8e978jlKV8CooKODIkSNcccUVNRK3EydOsGrVKr/1ioqKmDJlCidPnuSVV17hgQceYN26dfVufWvTpg2TJ09m7969vP3227z44ouUlpbWaHUD2LZtG6mpqTUSKYD33mv4eYGCPd/ll18OENC0JcGUFRFpVaKiIXsCbF0MZ46GO5qAhHWeN+fcX5xzfZxz5zvn/sO37SfOuTcqlXnIOXd/tXp/d85d6Jwb6HtuXncZSoUuXbrQtm1bVq5cyYkTJyq2l5SU8IMf/ICCggK/9e655x4+/fRT7rvvPkaOHMnMmTMZOnQoTz31FLNnz65XTNOmTQPghRde4IUXXiAmJoavfvWrNcplZWVRWFjI2rVrq2x/5plnWLhwYb1i8CfY8910001kZWXxxhtv8NJLL9XYX/m+w2DKioi0Ojl5UFYEn/0l3JEEJFIGLEiEioqK4vvf/z6PPfYYF154IePGjaO4uJglS5ZQWFjI1VdfXWNU6pw5c3jyySe57LLLePTRRwFvIMRLL73EoEGDuOOOOxgyZAi9evUKKaahQ4dywQUXMHv2bEpKSrjpppvo0qVLjXIzZsxg4cKFXHnllUyePJnk5GRWrFjBhx9+yKRJkyrmg2sowZ4vLi6O2bNnM2rUKG677TaeeuopLr/8cs6cOcOmTZt45513KgaGBFNWRKTVyciF5B5e1+mgW8MdTZ20PJY0ukceeYRf/epXtGnThqeeeorXX3+d3Nxcli1bVmMU8M6dO5k+fTrJycm8/PLLVUZ/ZmZm8oc//IFjx45xyy23UFxcHHJMU6dOrRhU4a/LFGD06NHMnz+fAQMG8Morr/DMM88QHx/PkiVLuOGGG0I+d21COV9ubi6rV6/m29/+Njt27ODXv/41f/zjHzly5AgPP/xwyGVFRFoVM+9+ty+WwMnD4Y6mTmGb562paYUFEQmUvgdEWqF9a+Gpr8CN/wW53wh3NOec500tbyIiIiLdLoSOvb2u02ZOyZuIiIiImTdwIf9DOL4/3NGckwYsSMTKz8/nueeeC6jsjBkzKuY6ayqrV69m7txzrtpW4aGHHmrcYEREpG45E+G9x2DDXLj8W+GOplZK3iRi5efnM3PmzIDKTps2LSzJW6DxKXkTEWkGOveFrhd6XafNOHlTt6lErOHDh+OcC+hRvmB8U5o2bVrA8YmISDORMxF2L4Mvd4Q7klopeRMREREpV75E1oY54Y3jHJS8iYiIiJRLyYL0XNjwergjqZWSNxEREZHKcibCvjVQsC3ckfil5E1ERESksuwJgDXb1jclbyIiIiKVJXWHnlfAulehGQ4qU/ImIiIiUl3ORCjYDAc3hjuSGpS8iYiIiFTXfxxYdLNcLkvJm4iIiEh17TtDr2Fe8tbMuk6VvImIiIj4k5MHX+bD3lXhjqQKJW8iYfTQQw9hZrz77rvhDkVERKrrdyNExcL65jXqVMmbSCuVlZUVlmXDREQiRpsO0Hukl7ydPRvuaCooeRMRERGpTU4eHN8Lu5aGO5IKSt5EREREatNnNMS0aVajTpW8iV/5+fmYGdOmTePzzz9n0qRJdOzYkcTEREaNGsX69esBOHToEHfeeSdpaWkkJCRwySWXsGTJkirH2rt3Lw8//DBDhw6lW7duxMXF0b17d2677TY2bqw5f8748eMxM37zm9/U2Pfggw9iZkyfPj3k9/bJJ58wadKkilgyMzO566672Lt3b5Vy/fr1Iy4ujoKCAr/H+cUvfoGZ8eSTT1ZsW7JkCXfeeScDBgwgKSmJNm3akJOTw8yZMzlz5kxA8VW+9v4MHz4cM6uyrbi4mCeffJIxY8bQs2dP4uPjSU1N5dprr2XBggVVyr777ruYGTt27GDHjh2YWcWj+jk/++wzpk2bRmZmJnFxcXTt2pXbbruNzZs3B/Re/An281Bu2bJlTJkyhfT0dOLj40lLS2PUqFHMmjWrXmVFRM4pvj30HQ0b5kJZabij8TjnWsVjyJAhLhAbN24MqFxLt337dge4YcOGuY4dO7orr7zS/fCHP3QTJ050ZuY6duzotmzZ4nr16uUGDRrkfvCDH7ivf/3rLjY21sXHx7sdO3ZUHOull15ybdq0cWPGjHHf+c533H333ecmTJjgYmNjXbt27dzq1aurnPvw4cOuR48eLj4+3q1atapi+9tvv+2ioqLcgAED3MmTJ0N6X88884yLjo52bdu2dbfccou799573fjx411UVJRLS0urEvfPfvYzB7jf/OY3fo/Vv39/FxcX5w4fPlyx7brrrnM9e/Z0t956q7vnnnvc9773PTd48GAHuOHDh7vS0tIqx/jpT3/qALdkyZIa137q1Kl+zzts2DDn/dP9h3379rmoqCh35ZVXuunTp7v777/fTZ061aWmpjrAPf3001WO/9Of/tQlJye75ORk99Of/rTiMWfOnIpyCxYscG3atHExMTFuwoQJ7t5773W33nqri4+Pd0lJSW7lypV1Xm9/gv08OOfc7373OxcdHe3i4uLcpEmT3AMPPOCmT5/uBg4c6IYNGxZy2droe0BEqtj4hnM/TXJu2ztNdkpghaslpwl7UtVUDyVvwSlPIAD36KOPVtn38MMPO8ClpKS4u+66y5WVlVXse+GFFxzgZsyYUbHtwIED7tixYzXOsXr1ateuXTs3evToGvs++ugjFxMT43r37u2OHz/u9u/f77p16+batGnj1q9fH9J72rx5s4uNjXXnn3++2717d5V95Ynh+PHjK7bt2rXLRUVFOX+fnWXLljnATZw4scr2zz//3J09e7ZG+X//9393gHv55ZerbG+o5O3MmTNu165dNcoeOXLEZWdnu5SUFHfq1Kkq+3r27Ol69uzp9xyFhYWuQ4cOrmPHjm7Dhg1V9q1bt861a9fODR482G/dugT7ediwYYOLiYlxKSkpfn/3ld93MGXPRd8DIlJF8Wnn/iPdubnfabJTnit5i2nqlr6ItuB+2L8u3FGcW7cL4frHGuxwWVlZ3H///VW2TZ06lZ/85CcUFRXx+OOPExX1j9732267jdtvv53Vq1dXbOvSpYvfYw8cOJBrrrmGRYsWUVJSQmxsbMW+K664gkceeYQHHniAu+66i0OHDrF//36efvppsrOzQ3ov//d//0dJSQlPPPEE6enpVfaNGDGCsWPHMn/+fI4fP05iYiIZGRmMGDGCxYsXs2HDhirnff755yuuRWW9evXye+67776bRx99lIULFzJlypSQ4j+X+Ph4MjIyamxPTk7m9ttv50c/+hHLly/nqquuCuh4L7zwAkeOHOHJJ59kwIABVfbl5OTwzW9+k//6r/9i48aNNfbXJdjPw//93/9RWlrKgw8+6Pd3X/l9B1NWRCRgsQnQ/0bYNB9u+DXExIc1HCVvck6DBg0iOjq6yrbu3bsD0KdPHxITE6vsi46OpmvXruzevbvK9rfeeovf/va3rFixgoKCAkpLq943UFBQQFpaWpVtP/7xj1myZAkvvvgiALfeeit33HFHyO/l448/BuC9995j+fLlNfYfPHiQsrIytmzZwpAhQwCYNm0aixcv5vnnn+eXv/wl4N1f9tJLL9GlSxfGjBlT5RgnT57kiSeeYM6cOWzZsoXjx497Tdw+e/bsCTn+umzYsIHHH3+c999/n3379tW4xy6Yc5dfqzVr1vDQQw/V2L9lyxYANm3aFHTyBsF9HpYu9UZ4XX/99XUeN5iyIiJBycmDNS/B53+DvuH9jlHyFowGbNGKFMnJyTW2xcTE1LqvfH9JSUnFz0888QQzZswgJSWFkSNH0qNHD9q2bYuZMXfuXNasWUNRUVGN45gZEydOZNGiRQDMmDGjXu/l8OHDADz++OPnLHfixImK1xMmTCApKYk//elP/PznPyc6Opo333yTwsJCZsyYUXEtAEpKSrjmmmtYtmwZOTk5TJkyhc6dO1e0IM2cOdPv+2wIS5cu5ZprrqG0tLSiFTEpKYmoqChWr17NvHnzgjp3+bV6+umnz1mu8rUKVLCfhyNHjgDUaC31J5iyIiJB6TUc2qR4o06VvElLVlpaykMPPUS3bt1YtWpVjda18hYef7Zu3co999xDSkoKR48e5Y477mDZsmUkJCSEFEt5snn06FGSkpICqtOmTRsmT57M73//exYvXszo0aNr7TKdN28ey5YtY9q0aTz77LNV9u3bt4+ZM2cGdM7ybujqrVHlyhOUyh599FFOnz7NkiVLGD58eJV9P//5z5k3b15A5y5Xfq3WrFnDRRddFFTdcwnl89ChQwfAazns16/fOY8fTFkRkaBEx8KAcbB2NhSfgri2YQtFU4VIoyooKODIkSNcccUVNf5QnzhxglWr/K8XV1RUxJQpUzh58iSvvPIKDzzwAOvWratX69vll18OwAcffBBUvfLpM55//nkOHTrEggULuOiiixg0aFCVctu2bQNg4sSJNY7x3nvvBXy+lJQUAHbt2lVj37Fjxyq6LKufOzU1tUbidq5zR0dHU1ZW5ndfqNeqLqF8HspjqT7liT/BlBURCVpOHpSchK0LwxqGkjdpVF26dKFt27asXLmyShdbSUkJP/jBD2qdQ+2ee+7h008/5b777mPkyJHMnDmToUOH8tRTTzF79uyQYvne975HbGwsd999t98EqLi42G+yMnToUHr37s28efP47W9/S0lJid852MqXmqq+TukXX3zBj3/844DjTExMpF+/fnz00UdV5j0rKyvjhz/8IadPn/Z77sLCQtauXVtl+zPPPMPChf6/ZDp27MihQ4f8Hu8b3/gGHTp0YObMmSxbtqzG/rNnz4a0Hmson4dvf/vbxMTE8Mgjj/idB67y/ZXBlBURCVrPodC+a9gn7FW3qTSqqKgovv/97/PYY49x4YUXMm7cOIqLi1myZAmFhYVcffXVNSb1nTNnDk8++SSXXXYZjz76KOC1Er300ksMGjSIO+64gyFDhtQ6srM2/fr14w9/+AO333472dnZjB49mj59+lBSUsLOnTv54IMP6Ny5M5999lmNuv/8z//Mgw8+yCOPPEJMTAxf/epXa5S56aabuOCCC/j1r3/NunXrGDx4MDt37uTNN9/khhtuYOfOnQHHeu+99zJ9+nSGDh3KzTffTEJCAkuWLKGkpISBAweyZs2aKuVnzJjBwoULufLKK5k8eTLJycmsWLGCDz/8kEmTJvHqq6/WOMeIESNYvnw5o0eP5qqrriI+Pp6BAwdy00030bFjR1599VUmTJjA5ZdfzogRI8jOzsbM2LVrFx9//DGHDx8OeOLhcqF8HgYMGMD//u//8q1vfYvBgwczbtw4evfuzeHDh1m+fDlJSUkVdYIpKyIStKhor/Xt8DZwDqpNmN5kaptDpKU9NM9bcOqaawzfBL7+VJ8/rKSkxP3qV79y/fv3dwkJCa5r167ua1/7msvPz3dTp051gNu+fbtzzrkdO3a4lJQUl5ycXLGtsrlz5zrAXXLJJa6oqCik97Z27Vo3depU16NHDxcXF+dSUlJcdna2u/POO9077/ifgHHHjh0uKirKAe7GG2+s9dg7d+50t912m+vevbtLSEhwAwYMcL/4xS9cSUmJ32vmb563cr///e/dgAEDXFxcnOvatau78847XUFBgd953pxzbv78+e6yyy5z7du3d8nJyW7kyJHuvffec88++6wD3LPPPlul/IkTJ9y3vvUtl56e7qKjo/3+vrdv3+6++93vugsuuMDFx8e7xMRE17dvX/e1r32tyoS+wQjm81DZ3//+dzdx4kTXuXNnFxsb69LS0tx1113nZs+eXa+y/uh7QERq5Wcuz8bAOeZ5M1dpGoOWLDc3161YsaLOcps2baJ///5NEJGINFf6HhCRcDOzlc65XH/7dM+biIiISARR8iYiIiISQTRgQSJWfn4+zz33XEBlZ8yYUTEHmDSsuXPnVlkOrTZZWVl+R+mKiEhwlLxJxMrPzw944ttp06YpeWskc+fOrZi4+FyGDRum5E1EpAEoeZOINXz4cFrLgJvm7Lnnngu4BVREROpP97yJiIiIRBAlbyIiIiIRRMmbiIiISARR8uaH7qMSab30719Emjslb9VER0dTUlIS7jBEJExKSkqIjo4OdxgiIrVS8lZNYmIix44dC3cYIhImx44dIzExMdxhiIjUSslbNampqXz55ZcUFBRQXFysLhSRVsA5R3FxMQUFBXz55ZekpqaGOyQRkVppnrdq4uPj6dGjB4WFheTn51NWVhbukESkCURHR5OYmEiPHj2Ij48PdzgiIrVS8uZHfHw8aWlppKWlhTsUERERkSrUbSoiIiISQZS8iYiIiEQQJW8iIiIiEUTJm4iIiEgEUfImIiIiEkGUvImIiIhEECVvIiIiIhFEyZuIiIhIBLHWsvyTmR0CdjTBqToBBU1wnkiga1GVrkdVuh7/oGtRla5HVboe/9CarkVP51xnfztaTfLWVMxshXMuN9xxNAe6FlXpelSl6/EPuhZV6XpUpevxD7oWHnWbioiIiEQQJW8iIiIiEUTJW8P7XbgDaEZ0LarS9ahK1+MfdC2q0vWoStfjH3Qt0D1vIiIiIhFFLW8iIiIiEUTJW5DMrIOZvWpmn5nZJjP7JzNLNbPFZrbV95ziK2tm9hsz22Zma83s4nDH39DMLN/M1pnZajNb4ds2yMyWlm8zs0t921v09fB3LXzb/8X3edlgZr+stP0B37XYbGbXhSfqxlPb9fDt+5GZOTPr5Pu5RX82oNZ/K4/7PhtrzWyOmXWoVL7Ffj5quRat9nsUwMyizexTM3vT9/MIM1vlu0YfmtkFvu3xZvaK73p8YmZZ4Yy7Mfi5FmZm/2FmW3x/d79faXuL/2z45ZzTI4gH8Dxwh+91HNAB+CVwv2/b/cAvfK/HAAsAAy4HPgl3/I1wPfKBTtW2LQKur3QN3m0N16OWa3E18DYQ7/u5i+95ALAGiAfOAz4HosP9Hhr7evi2ZwIL8eZd7NQaPhvn+HyMAmJ8r39R6bujRX8+arkWrfZ71Pc+fwi8CLzp+3kL0N/3+jvAc5Ve/9b3+hbglXDH3gTX4hvAC0CU7+fy79FW8dnw91DLWxDMLBm4CngGwDlX7Jw7AozDS+rwPY/3vR4HvOA8S4EOZpbWxGGHgwOSfK+Tgb2+163xenwbeMw5VwTgnDvo2z4OeNk5V+Sc2w5sAy4NU4xN7T+B+/A+J+Va42cD59wi51yp78elQIbvdWv8fLTa71EzywBuAH5fafO5vkfLr9OrwAgzs6aIsynUci2+DTzsnDsLNb5HW/RnozZK3oJzHnAIeNbXpPt7M2sHdHXO7fOV2Q909b1OB3ZVqr/bt60lccAiM1tpZnf6ts0AHjezXcD/Ax7wbW/p18PftegDfMXXvfGemV3i297SrwX4uR5mNg7Y45xbU61sq7we1dyO14oALf96+LsWrfl79L/w/kNzttK2O4C/mNlu4OvAY77tFdfDl/gfBTo2XaiNzt+1OB+YYt5tOAvMrLdve2v4bPgVE+4AIkwMcDHwL865T8zsCbzm/QrOOWdmrWkI75XOuT1m1gVYbGafAZOAu51zr5nZZLyWymvDGmXT8HctYoBUvCb9S4BZZtYrnEE2IX/X41/xugpboxrXwzn3PoCZ/RtQCvw5rBE2HX+fjQqt6XvUzG4EDjrnVprZ8Eq77gbG+P7W3Av8Gi+ha7HOcS3igTPOuVwzmwj8AfhKOGJsLtTyFpzdwG7n3Ce+n1/FS+YOlDfV+p7Lm3T34N3fUy7Dt63FcM7t8T0fBObgde1MBV73FZnNP7p7WvT1qOVa7AZe9zXrL8P732QnWvi1AL/XYxhe6/UaM8vHe8+rzKwbrfN6lA/kmQbcCHzVOVeesLTo61HLtWit36NDgbG+fxMvA9eY2VvAwEp/a14BrvC9rrgeZhaD16V6uEkjbjz+rsWf8H2P+srMAS7yvW7pn41aKXkLgnNuP7DLzPr6No0ANgJv4CUs+J7n+V6/Afyzb0TM5cDRSt0CEc/M2plZYvlrvBaV9Xj3ZgzzFbsG2Op73WKvxzmuxVy8QQuYWR+8QS4FeNfiFt/IsfOA3sCycMTeGGq5Hsudc12cc1nOuSy8L+SLff+uWuxnA2r/fJjZaLwuorHOuVOVqrTYz8c5/q20yu9R59wDzrkM37+JW4C/4d3Llez7zgAYCWzyva58nSYBf6uU9Ec0f9fCOfc1Kn2P4v1t2eJ73aI/G+eibtPg/QvwZzOLA77AGwUThdcdNh1vBN1kX9m/4I2G2Qac8pVtSboCc3z3ysYALzrn/mpmJ4AnfP8rPAOU39PSkq9HbdciDviDma0HioGpvi/aDWY2Cy/5LwW+65wrC1PsjcHv9ThH+Zb82YDaPx/b8LqEFvv2LXXOfcs515I/H7Vdi+W0zu/RGpxzpWb2TeA1MzsLfIl3TyR4t6H80ffZKcRLclq6x/D+7t4NnOAf3cet7rNRTissiIiIiEQQdZuKiIiIRBAlbyIiIiIRRMmbiIiISARR8iYiIiISQZS8iYiIiEQQJW8iIq2Qmb3rmwxVRCKMkjcRqRcza2tmM8zsAzMrNLMSMztgZn8xs2m++f4kDHy/l2lBlJ9mZjMaMSQRaQCa501EQmZmFwBvAX2At4FFeCtIdMFbz/Za4HHn3H1hC7IV87Ws5TvnhvvZF4f3N6Co0rZ3gfIVMESkmdL/iEUkJGbWBngT6AXkOeder1bkF2Z2CXBJkwcndXLOFYc7BhEJjbpNRSRUdwB9gV/5SdwAcM4td879b/nPZjbKzF4xsy/M7LSZHTGzRWY2rHpdM8s2s9lmtsfMisxsv5ktMbMbqpWLN7N/NbMNZnbGd8z5Zja4WrkoXzfiWjM7bmbHzGyzmT1jZrF1vVkzSzCzx81sry/2Zb7385yZuWpl832tWNWPMdzMXOWuTDNLNLNHzewTMyvwvddtZvaYmbWtrb6ZfcP3novMbIeZ3VetrAN6AsN8dcofWb79Ve55870eBvSsVn64mc0zs1NmluTnPV3iK/eTuq6hiDQMtbyJSKgm+Z5/F0SdaUAq8ALewvTpeEngO2Z2tXPuAwAz64i3QDfAb/HWuuwE5AKX4XXV4ku6/gpcAfwReBJIBr4JfGRmVznnVviO82/Aw8B83zHLgPOAsXjri5bUEftLwHhf/YXA+cDrwPYg3r8/5dfgNeBFvLVMh+EtWD8YuM5PnW/hrRH6DHAE+BpeS+du59yLvjJfB/4Trxv7PyrVPVRLHDOAn+Nd57srbd8EPI13nW4FnqpWbzpwFvhDHe9TRBqI7nkTkZCY2WEgxjmXHESdds65k9W2dQU2AMucc2N828YC84ApzrlZ5zje3cCvgdHOuYWVticB64Evyu/3MrNVQIJzbkCg8VY63ii8hO1559y0StvHA3MAnHNWaXs+fu41M7PhwBLgG86553zb4rzqrqRa2UeAfwcuc84tq1Z/H9DfOXfUt70tXoK7zTn3T3XF4dv3LtXub6vtnjczi8ZLUvc75y6ttL2tL5aPyn93ItL41G0qIqFKAo4HU6Fy4mZm7X0tbGXAJ3gtauWO+p6v99dVV8nXgM+AlWbWqfwBxAGLgSt99+aVHzPdzK4MJmaf8b7nx6u9n7nA5hCOV/kYxeWJm5nFmFmK7z287StymZ9qz5Ynbr5jnAKWAr3rE8s5YizDa1m7xMwurLRrEt7n4JnGOK+I+KfkTURCdQxIDKaCmZ1vZi+b2Zd4iV8BXjfeGCClvJxz7j28rtVpQIGZfWRmM82seqtZf6Cf7xjVH7cD0XjdgAD/CpwBPvDdR/dnM7vN1/JVl154XYNb/OzbFED9czKz75jZWqAIKPTF/65vd4qfKl/42XYY6FjfWM7hGbxEe3qlbdOBg8AbjXheEalGyZuIhGo9kGRmvQIpbGbtgfeB0cATeK021wEj8e5vs8rlnXNTgQvx7lU7DPwIWGtm36t8WGCd7xi1PQ75jvcx3n1qk/C6OgcBfwZWm1lqcG+9TrXdj1LjPmMz+yHwP3jdj3cBN/jinuYr4u97uqz+IQbHObcL7/7Cr5lZnJn1Bq4CXqje5SsijUsDFkQkVK/h/fG+A69Vq4r6PEUAAANISURBVC4jgO7A7c65ZyvvMLNH/VVwzq3HSxIfN7MOeN2rj5nZ/zjvht2tQGfgb865s3UF4Jw74Yv7Nd95v4OXOE2nWpdoNV/gJVF98O7Pq6y/n/KFeAMzqvOX6H4dyAeur/wezGz0OeIJVLA3NddV/nd4yeV4vMEUoC5TkSanljcRCdXv8e73usfMxvkrYGZDfAkS/KO1yKqVGUW1+7rMLNXMqnw/OeeO4N003xZI8G1+AegG/LCW83et9LqTnyKrfM91tbzN8z3fW+344/GmS6luC9DPzNIrlY0HvuunbBle0lR5wEMMcH8dMQXiBHW/t+rlU8zMatn/FrAXr4VwKt5Ahc/qF6KIBEstbyISEufcKTO7Ee8P+lwzW4Q3SOAwXmvY1Xjdor/0VfkQ2A/8yjfX2G68rsuv43V9Vr4R/p+Bu81sDrANbxqPYb7jzXLOnfaVewKvi/FxM7sGr/v1GNADr6XvjC8OgE1mthSv9W4vkAbcCRQDL9fxXhea2Xxgqq+L9a94XbB34bUM5lSr8iRwC/C2mf0WbwDF14FTfg7/Kt4UHQvM7HW8AQC3UffUJYFYCkz3jVzdhHff3vzqI36rlb8ReNLM/o6XWP7NOXcQvIELZvYHvFGwEFiLq4g0NOecHnrooUfID7yWsLvxkrMv8ZKOA3hJ3deB6EplL8JLfMoHLLwLfAV4zvs6qig3CHgeL3E7iZeQrcG77y2+2vljgO8Dy31lT+J1p/4ZGFWp3P1499wdxBsYsAuYDVwc4PtsA/wKLwE9DSwDRlWPvVL5qXgtk8V4LYb3AdfgtbJNq1QuGnjA916L8Kb8+CVed6wDHqpUdnj1+pX21YgDb5my1/C6cc/66mb59r2LN41I9d/lM77fX3mL4PBqZXr69h0D2oX786eHHq3xoXneRETqwcyeA6a6SvO8tWRmloaX+D7jnLsr3PGItEa6501ERILxbbzWwmBW1hCRBqR73kREpE5mdgvevYT3AgudcyvDHJJIq6XkTUREAvESvkmOqTpRr4g0Md3zJiIiIhJBdM+biIiISARR8iYiIiISQZS8iYiIiEQQJW8iIiL/v906IAEAAAAQ9P91OwJdIYzIGwDAiLwBAIwE+dN03OrlD3oAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "OPTIMIZATION SUMMARY:\n",
            "-> The target accuracy configurated was: 95.00 %\n",
            "-> The minimum number of cases computed above target accuracy was: 450 cases\n",
            "-> (It represents 75.00 % from total dataset)\n",
            "\n",
            "BEST MODEL SUMMARY:\n",
            "-> BEST MODEL: Best validation accuracy found was: 75.9549 %\n",
            "-> BEST MODEL: Test accuracy found was: 73.1250 %\n",
            "-> BEST MODEL: It is a difference of 2.8299 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQfhsuf3tCCK"
      },
      "source": [
        "#5.Independent test with the number of found cases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3Fx90pJnWEU"
      },
      "source": [
        "**This section is dedicated to the execution and analysis (duplicate test) of the solution (number of cases) found by the optimizer.**\n",
        "\n",
        "* Observe that the experiment is based on a stochastic process, which hinders \"perfect\" reproducibility;\n",
        "* Depending on the adjusted Reduction Step, the optimizer may offer a better \"local\" rather than better \"global\" solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZDDuV9KiTa_"
      },
      "source": [
        "'''\n",
        "Subset - lembrar de usar a mesma função de subset usada na otimização (\"subset\" ou \"subsetB\"):\n",
        "\"subset(Xt, Y, cases)\" deve ser usado se o pré-processamento foi feito antes do subset;\n",
        "\"subsetB(cases)\" faz o pré-processamento depois do subset.\n",
        "'''\n",
        "\n",
        "X_train, Y_train, X_val, Y_val, X_test, Y_test, n_cases = subsetB(400)\n",
        "\n",
        "# Quando for rodar uma segunda vez, habilite o código abaixo:\n",
        "#del modelx\n",
        "#del modely\n",
        "K.clear_session()\n",
        "tf.keras.backend.clear_session()\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "# Carregando o modelo padrão:\n",
        "modelx = load_model(save_path)\n",
        "\n",
        "# Treino e validação:\n",
        "modely, history, acc_max, train_time = fit_model(model, X_train, Y_train, X_val, Y_val, batch_size, epochs)\n",
        "\n",
        "# Teste:\n",
        "test_accuracy = test_model(X_test, Y_test, modely)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhUBkA_gWraT"
      },
      "source": [
        "# Acurácia:\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Train_acc vs. val_acc', fontsize=20)\n",
        "plt.xlabel('Epoch',fontsize=18)\n",
        "plt.ylabel('Accuracy',fontsize=18)\n",
        "plt.legend(['Train', 'Validation'], loc='best', fontsize=18)\n",
        "plt.show()\n",
        "#Para salvar no Drive...\n",
        "#plt.savefig('/content/drive/My Drive/MESTRADO - UFES/trainacc_vs_valacc.png', transparent=True)\n",
        "\n",
        "# Perda (loss):\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Loss vs. epochs', fontsize=20)\n",
        "plt.ylabel('Loss',fontsize=18)\n",
        "plt.xlabel('Epoch',fontsize=18)\n",
        "plt.legend(['Train', 'Validation'], loc='best', fontsize=20)\n",
        "plt.show()\n",
        "#Para salvar no Drive...\n",
        "#plt.savefig('/content/drive/My Drive/MESTRADO - UFES/trainloss_vs_valloss.png', transparent=True)\n",
        "\n",
        "print('Max val_acc was:',acc_max)\n",
        "print('Eval_acc was:',test_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXhEUYWdi1CR"
      },
      "source": [
        "# 6.Second stage simulation (CNN hyperparameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bga5471pz73f"
      },
      "source": [
        "**Briefly, the steps that make up the 2nd optimization stage are:**\r\n",
        "\r\n",
        "* 6.1 Reduction of the database according to the 1st stage of optimization;\r\n",
        "* 6.2 Definition of hyperparameters for adjustment and search space size;\r\n",
        "* 6.3 CNN implementation with architecture that allows automatic adjustment;\r\n",
        "* 6.4 Definition of fitness function;\r\n",
        "* 6.5 Implementation of the Bayesian optimizer;\r\n",
        "* 6.6 Plot the graphs with the optimization history.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4746jxTL6E4A"
      },
      "source": [
        "## 6.1.Dataset reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHOOa_O88Uel"
      },
      "source": [
        "Reduction will be the same as the identified on 1st stage:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKMqKRaJp0RR"
      },
      "source": [
        "# Redução do nº de casos na base de dados (conforme resultado do 1º estágio):\r\n",
        "X_train2, Y_train2, X_val2, Y_val2, X_test2, Y_test2, n_cases2 = subsetB(400)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8b30PZF6PsY"
      },
      "source": [
        "## 6.2.Hyperparameters definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoqttrfP8_I_"
      },
      "source": [
        "Definition of the hyperparameters to adjust, the search space and the standard adjustment (required by the optimizer)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Qv31BM2q6zT"
      },
      "source": [
        "# Definição do range de variação dos hiperparâmetros selecionados:\r\n",
        "dim_num_conv_layers = Integer(low=2, high=4, name='num_conv_layers')\r\n",
        "dim_num_conv_nodes = Integer(low=32, high=256, name='num_conv_nodes')\r\n",
        "dim_num_dense_layers = Integer(low=1, high=3, name='num_dense_layers') # Variando até 3 pois a última é de classificação, com ativador \"Softmax\".\r\n",
        "dim_num_dense_nodes = Integer(low=5, high=256, name='num_dense_nodes')\r\n",
        "\r\n",
        "# Definição do vetor que representa o espaço de busca do otimizador:\r\n",
        "hp = [dim_num_conv_layers, dim_num_conv_nodes, dim_num_dense_layers, dim_num_dense_nodes]\r\n",
        "\r\n",
        "# Definição do conjunto de hiperparâmetros padrão (requerido pelo otimizador):\r\n",
        "default_parameters = [4, 128, 3, 256]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dryt48K_616G"
      },
      "source": [
        "## 6.3.Model function (automodel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xGrrZnl9O5V"
      },
      "source": [
        "Definition of new Callbacks parameters for the 2nd stage.\r\n",
        "\r\n",
        "(EarlyStopping was configurated to stop CNN after 50 epochs without accuracy improvements)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asYx7f2Mzmzx"
      },
      "source": [
        "# EarlyStopping (O modelo pára o treinamento caso não perceba melhoria):\r\n",
        "earlystopping2 = EarlyStopping(monitor=\"val_accuracy\", min_delta=0, patience=100,\r\n",
        "                              verbose=1, mode=\"auto\", baseline=None,\r\n",
        "                              restore_best_weights=True)\r\n",
        "\r\n",
        "# TensorDash (acompanhamento das métricas do modelo pelo app Android):\r\n",
        "histories2 = Tensordash(ModelName = 'AutoML2', email = 'viniciuswv@gmail.com',\r\n",
        "                       password = 'admin1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Jx-ZGf96Pn"
      },
      "source": [
        "Definition of the CNN function, with **dynamic architecture**.\r\n",
        "\r\n",
        "(It's necessary to pass the selected hyperparameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX3lNFvwiyQ8"
      },
      "source": [
        "# Batch size (número de exemplos de treinamento usados em uma iteração/época):\r\n",
        "batch_size = 32\r\n",
        "\r\n",
        "# Épocas (quantidade de ciclos de treinamento da rede neural):\r\n",
        "epochs = 500\r\n",
        "\r\n",
        "def automodel (num_conv_layers, num_conv_nodes, num_dense_layers, num_dense_nodes):\r\n",
        "  '''\r\n",
        "  Hyperparameters:\r\n",
        "  num_conv_layers: Number of convolutional layers.\r\n",
        "  num_conv_nodes: Number of neurons in each convolutional layer,\r\n",
        "  num_dense_layers: Number of dense layers.\r\n",
        "  num_dense_nodes: Number of neurons in each dense layer.\r\n",
        "  '''\r\n",
        "\r\n",
        "  # Criação das camadas com apoio do recurso 'keras.sequential':\r\n",
        "  model = Sequential() # Empilha linearmente as camadas da rede, conforme abaixo:\r\n",
        "  \r\n",
        "  # Camada de alimentação do dataset:\r\n",
        "  model.add(InputLayer(input_shape=(1666,1))) # O tensor de entrada tem o shape (1666, 1).\r\n",
        "\r\n",
        "  # Controle de camadas convolucionais (e MaxPooling/BatchNormalization):\r\n",
        "  for i in range(num_conv_layers):\r\n",
        "    name = 'layer_conv_{0}'.format(i+1) # Nome da camada.\r\n",
        "    \r\n",
        "    model.add(Conv1D(num_conv_nodes,\r\n",
        "                     kernel_size=4,\r\n",
        "                     strides=1,\r\n",
        "                     activation='relu',\r\n",
        "                     padding='same',\r\n",
        "                     name=name))\r\n",
        "    \r\n",
        "    model.add(MaxPooling1D(pool_size=3, strides=None, padding='valid'))\r\n",
        "    \r\n",
        "    model.add(BatchNormalization())\r\n",
        "  \r\n",
        "  # Camada de achatamento (flatten):\r\n",
        "  model.add(Flatten())\r\n",
        " \r\n",
        "  # Controle de camadas densas (fully connected):\r\n",
        "  for i in range(num_dense_layers):\r\n",
        "    name = 'layer_dense_{0}'.format(i+1) # Nome da camada.\r\n",
        "    model.add(Dense(num_dense_nodes, activation='relu', name=name))\r\n",
        "\r\n",
        "  # Última camada densa (necessária para a classificação das features):\r\n",
        "  model.add(Dense(16, activation='softmax'))\r\n",
        "\r\n",
        "  # Otimizador da rede:\r\n",
        "  optimizer2 = tf.keras.optimizers.SGD(learning_rate=0.01, name=\"SGD\")\r\n",
        "\r\n",
        "  model.compile(loss= 'categorical_crossentropy',\r\n",
        "                optimizer= optimizer2,\r\n",
        "                metrics=['accuracy'])\r\n",
        " \r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moLrGBBi7AaY"
      },
      "source": [
        "##6.4.Fitness function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drymEB4y-4Qv"
      },
      "source": [
        "* The fitness function will call the dynamic CNN, that will train with a certain set of hyperparameters.\r\n",
        "\r\n",
        "* The validation accuracy will be stored after each training season, and will be used as an evaluation criterion by the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKEV4IBxmlR0"
      },
      "source": [
        "# Método padrão para implementação da fitness function:\r\n",
        "@use_named_args(dimensions=hp)\r\n",
        "def fitness(num_conv_layers, num_conv_nodes, num_dense_layers, num_dense_nodes):\r\n",
        "\r\n",
        "    # Visualização dos hiperparâmetros durante o processo de otimização:\r\n",
        "    print('NEXT SET OF HYPERPARAMETERS IS:',\r\n",
        "          '\\n num_conv_layers:', num_conv_layers,\r\n",
        "          '\\n num_conv_nodes:', num_conv_nodes,   \r\n",
        "          '\\n num_dense_layers:', num_dense_layers,\r\n",
        "          '\\n num_dense_nodes:', num_dense_nodes,\r\n",
        "          '\\n')\r\n",
        "    \r\n",
        "    # Criação da CNN com os hiperparâmetros selecionados:\r\n",
        "    model_opt = automodel(num_conv_layers=num_conv_layers,\r\n",
        "                          num_conv_nodes=num_conv_nodes,\r\n",
        "                          num_dense_layers=num_dense_layers,\r\n",
        "                          num_dense_nodes=num_dense_nodes)\r\n",
        "\r\n",
        "    # Aplicação do método \"fit\" para treinamento do modelo (CNN):\r\n",
        "    history = model_opt.fit(X_train2,\r\n",
        "                            Y_train2,\r\n",
        "                            batch_size = batch_size,\r\n",
        "                            epochs = epochs,\r\n",
        "                            verbose = 1,\r\n",
        "                            validation_data = (X_val2, Y_val2),\r\n",
        "                            callbacks = [histories2, earlystopping2])\r\n",
        "\r\n",
        "    # Armazenamento das métricas de validação após cada época:\r\n",
        "    # (Pode escolher qual métrica será passada para o otimizador)\r\n",
        "    # Se for a acurácia, fazer o 'return' com o valor negativo, pois o\r\n",
        "    # otimizador utilizado só consegue minimizar o custo)\r\n",
        "    accuracy = history.history['val_accuracy'][-1]\r\n",
        "    loss = history.history['val_loss'][-1] # Custo a ser minimizado!\r\n",
        "\r\n",
        "    # Visualização da acurácia durante o processo de otimização:\r\n",
        "    print('\\nAccuracy: {0:.2%}'.format(accuracy))\r\n",
        "    \r\n",
        "    # Visualização da perda (loss) durante o processo de otimização:\r\n",
        "    print('Loss:', loss, '\\n')\r\n",
        "    \r\n",
        "    # Exclusão dos dados do modelo Keras (com os hiperparâmetros) da memória:\r\n",
        "    # (Evitar que a rede \"aprenda\" de uma rodada para outra)\r\n",
        "    del model_opt\r\n",
        "    K.clear_session()\r\n",
        "    tf.keras.backend.clear_session()\r\n",
        "    tf.compat.v1.reset_default_graph()\r\n",
        "    \r\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDLUCiNi7T-G"
      },
      "source": [
        "## 6.5.Bayesian optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-m97u15AbjU"
      },
      "source": [
        "Implementation of Bayesian optimization (from Scikit-optimize) using Gaussian Processes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QraF8-aGphyj",
        "outputId": "706835f2-2518-491f-9745-65e27a7270f3"
      },
      "source": [
        "# Implementação de um callback para monitorar tempo:\r\n",
        "TimerCallback = skopt.callbacks.TimerCallback()\r\n",
        "\r\n",
        "# Implementação de um callback para salvar cada iteração:\r\n",
        "# (Consultar a documentação caso necessário carregar o processo \"load\")\r\n",
        "checkpoint_saver = CheckpointSaver(\"./checkpoint.pkl\")\r\n",
        "\r\n",
        "# Instanciamento do otimizador:\r\n",
        "result = gp_minimize(func= fitness,\r\n",
        "                     dimensions= hp,\r\n",
        "                     acq_func='EI', # Função de minimização (consultar a documentação).\r\n",
        "                     n_calls= 100,    # Número de iterações do otimizador!\r\n",
        "                     x0= default_parameters,\r\n",
        "                     callback= [TimerCallback, checkpoint_saver])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mA saída de streaming foi truncada nas últimas 5000 linhas.\u001b[0m\n",
            "Epoch 281/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0018 - accuracy: 0.9999 - val_loss: 0.0767 - val_accuracy: 0.9756\n",
            "Epoch 282/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0023 - accuracy: 0.9999 - val_loss: 0.0759 - val_accuracy: 0.9795\n",
            "Epoch 283/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0764 - val_accuracy: 0.9795\n",
            "Epoch 284/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0738 - val_accuracy: 0.9775\n",
            "Epoch 285/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0841 - val_accuracy: 0.9814\n",
            "Epoch 286/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0031 - accuracy: 0.9997 - val_loss: 0.0826 - val_accuracy: 0.9746\n",
            "Epoch 287/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0799 - val_accuracy: 0.9805\n",
            "Epoch 288/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0727 - val_accuracy: 0.9795\n",
            "Epoch 289/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0680 - val_accuracy: 0.9824\n",
            "Epoch 290/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0771 - val_accuracy: 0.9805\n",
            "Epoch 291/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0740 - val_accuracy: 0.9814\n",
            "Epoch 292/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 0.0674 - val_accuracy: 0.9824\n",
            "Epoch 293/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0698 - val_accuracy: 0.9834\n",
            "Epoch 294/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0807 - val_accuracy: 0.9785\n",
            "Epoch 295/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0783 - val_accuracy: 0.9775\n",
            "Epoch 296/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0687 - val_accuracy: 0.9814\n",
            "Epoch 297/500\n",
            "128/128 [==============================] - 1s 9ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0729 - val_accuracy: 0.9795\n",
            "Epoch 298/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0728 - val_accuracy: 0.9814\n",
            "Epoch 299/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0783 - val_accuracy: 0.9785\n",
            "Epoch 300/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0710 - val_accuracy: 0.9785\n",
            "Epoch 301/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0740 - val_accuracy: 0.9775\n",
            "Epoch 302/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0656 - val_accuracy: 0.9785\n",
            "Epoch 303/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0698 - val_accuracy: 0.9805\n",
            "Epoch 304/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 9.6342e-04 - accuracy: 1.0000 - val_loss: 0.0687 - val_accuracy: 0.9805\n",
            "Epoch 305/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0798 - val_accuracy: 0.9795\n",
            "Epoch 306/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0762 - val_accuracy: 0.9834\n",
            "Epoch 307/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 9.9890e-04 - accuracy: 1.0000 - val_loss: 0.0713 - val_accuracy: 0.9795\n",
            "Epoch 308/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0684 - val_accuracy: 0.9814\n",
            "Epoch 309/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0590 - val_accuracy: 0.9824\n",
            "Epoch 310/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0727 - val_accuracy: 0.9795\n",
            "Epoch 311/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0682 - val_accuracy: 0.9814\n",
            "Epoch 312/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 8.0458e-04 - accuracy: 1.0000 - val_loss: 0.0721 - val_accuracy: 0.9795\n",
            "Epoch 313/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0715 - val_accuracy: 0.9805\n",
            "Epoch 314/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 9.8030e-04 - accuracy: 1.0000 - val_loss: 0.0715 - val_accuracy: 0.9795\n",
            "Epoch 315/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 9.9687e-04 - accuracy: 1.0000 - val_loss: 0.0713 - val_accuracy: 0.9805\n",
            "Epoch 316/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0702 - val_accuracy: 0.9814\n",
            "Epoch 317/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 8.8585e-04 - accuracy: 1.0000 - val_loss: 0.0711 - val_accuracy: 0.9805\n",
            "Epoch 318/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0696 - val_accuracy: 0.9854\n",
            "Epoch 319/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0696 - val_accuracy: 0.9814\n",
            "Epoch 320/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 8.5064e-04 - accuracy: 1.0000 - val_loss: 0.1701 - val_accuracy: 0.9551\n",
            "Epoch 321/500\n",
            "128/128 [==============================] - 1s 9ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0681 - val_accuracy: 0.9834\n",
            "Epoch 322/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 8.7556e-04 - accuracy: 1.0000 - val_loss: 0.0699 - val_accuracy: 0.9824\n",
            "Epoch 323/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 8.2889e-04 - accuracy: 1.0000 - val_loss: 0.0740 - val_accuracy: 0.9785\n",
            "Epoch 324/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 8.7250e-04 - accuracy: 1.0000 - val_loss: 0.0659 - val_accuracy: 0.9795\n",
            "Epoch 325/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 7.5901e-04 - accuracy: 1.0000 - val_loss: 0.0715 - val_accuracy: 0.9795\n",
            "Epoch 326/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.0783 - val_accuracy: 0.9775\n",
            "Epoch 327/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0738 - val_accuracy: 0.9814\n",
            "Epoch 328/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0237 - accuracy: 0.9946 - val_loss: 1.4496 - val_accuracy: 0.7637\n",
            "Epoch 329/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0601 - accuracy: 0.9809 - val_loss: 0.3822 - val_accuracy: 0.9033\n",
            "Epoch 330/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0136 - accuracy: 0.9962 - val_loss: 0.4584 - val_accuracy: 0.9092\n",
            "Epoch 331/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0788 - val_accuracy: 0.9775\n",
            "Epoch 332/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.0648 - val_accuracy: 0.9824\n",
            "Epoch 333/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0901 - val_accuracy: 0.9717\n",
            "Epoch 334/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0140 - accuracy: 0.9949 - val_loss: 0.1879 - val_accuracy: 0.9277\n",
            "Epoch 335/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0641 - val_accuracy: 0.9854\n",
            "Epoch 336/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0694 - val_accuracy: 0.9814\n",
            "Epoch 337/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0697 - val_accuracy: 0.9805\n",
            "Epoch 338/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0570 - val_accuracy: 0.9844\n",
            "Epoch 339/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.0847 - val_accuracy: 0.9756\n",
            "Epoch 340/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 0.0857 - val_accuracy: 0.9775\n",
            "Epoch 341/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0712 - val_accuracy: 0.9785\n",
            "Epoch 342/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0627 - val_accuracy: 0.9824\n",
            "Epoch 343/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0667 - val_accuracy: 0.9785\n",
            "Epoch 344/500\n",
            "128/128 [==============================] - 1s 9ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0653 - val_accuracy: 0.9795\n",
            "Epoch 345/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 9.5497e-04 - accuracy: 1.0000 - val_loss: 0.0774 - val_accuracy: 0.9775\n",
            "Epoch 346/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0744 - val_accuracy: 0.9785\n",
            "Epoch 347/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0629 - val_accuracy: 0.9824\n",
            "Epoch 348/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 9.7136e-04 - accuracy: 1.0000 - val_loss: 0.0665 - val_accuracy: 0.9824\n",
            "Epoch 349/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.0779 - val_accuracy: 0.9785\n",
            "Epoch 350/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 9.1433e-04 - accuracy: 1.0000 - val_loss: 0.0808 - val_accuracy: 0.9766\n",
            "Epoch 351/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 9.2797e-04 - accuracy: 1.0000 - val_loss: 0.0719 - val_accuracy: 0.9814\n",
            "Epoch 352/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0687 - val_accuracy: 0.9834\n",
            "Epoch 353/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 9.0559e-04 - accuracy: 1.0000 - val_loss: 0.0767 - val_accuracy: 0.9785\n",
            "Epoch 354/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 8.3919e-04 - accuracy: 1.0000 - val_loss: 0.0770 - val_accuracy: 0.9785\n",
            "Epoch 355/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 7.0654e-04 - accuracy: 1.0000 - val_loss: 0.0717 - val_accuracy: 0.9805\n",
            "Epoch 356/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 7.1891e-04 - accuracy: 1.0000 - val_loss: 0.0826 - val_accuracy: 0.9756\n",
            "Epoch 357/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 8.3900e-04 - accuracy: 1.0000 - val_loss: 0.0707 - val_accuracy: 0.9805\n",
            "Epoch 358/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.1301 - val_accuracy: 0.9697\n",
            "Epoch 359/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 9.4324e-04 - accuracy: 1.0000 - val_loss: 0.0702 - val_accuracy: 0.9795\n",
            "Epoch 360/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 6.9870e-04 - accuracy: 1.0000 - val_loss: 0.0708 - val_accuracy: 0.9795\n",
            "Epoch 361/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 7.3357e-04 - accuracy: 1.0000 - val_loss: 0.0675 - val_accuracy: 0.9805\n",
            "Epoch 362/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 7.1638e-04 - accuracy: 1.0000 - val_loss: 0.0649 - val_accuracy: 0.9805\n",
            "Epoch 363/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 9.5474e-04 - accuracy: 1.0000 - val_loss: 0.0737 - val_accuracy: 0.9795\n",
            "Epoch 364/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 6.9037e-04 - accuracy: 1.0000 - val_loss: 0.0739 - val_accuracy: 0.9785\n",
            "Epoch 365/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 8.6867e-04 - accuracy: 1.0000 - val_loss: 0.0732 - val_accuracy: 0.9805\n",
            "Epoch 366/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 6.4867e-04 - accuracy: 1.0000 - val_loss: 0.0679 - val_accuracy: 0.9795\n",
            "Epoch 367/500\n",
            "128/128 [==============================] - 1s 9ms/step - loss: 5.7317e-04 - accuracy: 1.0000 - val_loss: 0.0761 - val_accuracy: 0.9785\n",
            "Epoch 368/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 7.1640e-04 - accuracy: 1.0000 - val_loss: 0.0693 - val_accuracy: 0.9805\n",
            "Epoch 369/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 6.8658e-04 - accuracy: 1.0000 - val_loss: 0.0689 - val_accuracy: 0.9805\n",
            "Epoch 370/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 8.0214e-04 - accuracy: 1.0000 - val_loss: 0.0739 - val_accuracy: 0.9785\n",
            "Epoch 371/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.0729 - val_accuracy: 0.9785\n",
            "Epoch 372/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 7.3567e-04 - accuracy: 1.0000 - val_loss: 0.0747 - val_accuracy: 0.9775\n",
            "Epoch 373/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0736 - val_accuracy: 0.9785\n",
            "Epoch 374/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 6.7228e-04 - accuracy: 1.0000 - val_loss: 0.0679 - val_accuracy: 0.9805\n",
            "Epoch 375/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 6.7930e-04 - accuracy: 1.0000 - val_loss: 0.0646 - val_accuracy: 0.9814\n",
            "Epoch 376/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 8.0973e-04 - accuracy: 1.0000 - val_loss: 0.0643 - val_accuracy: 0.9814\n",
            "Epoch 377/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 6.9739e-04 - accuracy: 1.0000 - val_loss: 0.1886 - val_accuracy: 0.9727\n",
            "Epoch 378/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0030 - accuracy: 0.9988 - val_loss: 0.0743 - val_accuracy: 0.9795\n",
            "Epoch 379/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 6.9380e-04 - accuracy: 1.0000 - val_loss: 0.0644 - val_accuracy: 0.9805\n",
            "Epoch 380/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 6.4188e-04 - accuracy: 1.0000 - val_loss: 0.0699 - val_accuracy: 0.9785\n",
            "Epoch 381/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 5.5849e-04 - accuracy: 1.0000 - val_loss: 0.0651 - val_accuracy: 0.9814\n",
            "Epoch 382/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0694 - val_accuracy: 0.9814\n",
            "Epoch 383/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 8.4691e-04 - accuracy: 1.0000 - val_loss: 0.0670 - val_accuracy: 0.9814\n",
            "Epoch 384/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 6.8537e-04 - accuracy: 1.0000 - val_loss: 0.0679 - val_accuracy: 0.9805\n",
            "Epoch 385/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 6.0742e-04 - accuracy: 1.0000 - val_loss: 0.0708 - val_accuracy: 0.9785\n",
            "Epoch 386/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 7.1497e-04 - accuracy: 1.0000 - val_loss: 0.0650 - val_accuracy: 0.9814\n",
            "Epoch 387/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 6.0016e-04 - accuracy: 1.0000 - val_loss: 0.0718 - val_accuracy: 0.9785\n",
            "Epoch 388/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 5.8223e-04 - accuracy: 1.0000 - val_loss: 0.1567 - val_accuracy: 0.9346\n",
            "Epoch 389/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 9.3183e-04 - accuracy: 1.0000 - val_loss: 0.0693 - val_accuracy: 0.9785\n",
            "Epoch 390/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 6.2555e-04 - accuracy: 1.0000 - val_loss: 0.0695 - val_accuracy: 0.9805\n",
            "Epoch 391/500\n",
            "128/128 [==============================] - 1s 9ms/step - loss: 6.5305e-04 - accuracy: 1.0000 - val_loss: 0.0689 - val_accuracy: 0.9795\n",
            "Epoch 392/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 5.7457e-04 - accuracy: 1.0000 - val_loss: 0.0689 - val_accuracy: 0.9805\n",
            "Epoch 393/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 6.1917e-04 - accuracy: 1.0000 - val_loss: 0.0712 - val_accuracy: 0.9805\n",
            "Epoch 394/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 5.2427e-04 - accuracy: 1.0000 - val_loss: 0.0701 - val_accuracy: 0.9805\n",
            "Epoch 395/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 5.0226e-04 - accuracy: 1.0000 - val_loss: 0.0698 - val_accuracy: 0.9805\n",
            "Epoch 396/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 5.0044e-04 - accuracy: 1.0000 - val_loss: 0.0718 - val_accuracy: 0.9785\n",
            "Epoch 397/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 6.5430e-04 - accuracy: 1.0000 - val_loss: 0.0692 - val_accuracy: 0.9805\n",
            "Epoch 398/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 5.7915e-04 - accuracy: 1.0000 - val_loss: 0.0716 - val_accuracy: 0.9795\n",
            "Epoch 399/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 6.0743e-04 - accuracy: 1.0000 - val_loss: 0.1328 - val_accuracy: 0.9600\n",
            "Epoch 400/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 5.2164e-04 - accuracy: 1.0000 - val_loss: 0.0735 - val_accuracy: 0.9795\n",
            "Epoch 401/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.1767 - accuracy: 0.9682 - val_loss: 7.6057 - val_accuracy: 0.2949\n",
            "Epoch 402/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.8734 - accuracy: 0.6885 - val_loss: 5.8416 - val_accuracy: 0.3936\n",
            "Epoch 403/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.6758 - accuracy: 0.6962 - val_loss: 25.2747 - val_accuracy: 0.1787\n",
            "Epoch 404/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.6265 - accuracy: 0.7176 - val_loss: 0.6367 - val_accuracy: 0.7139\n",
            "Epoch 405/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.5725 - accuracy: 0.7294 - val_loss: 0.7213 - val_accuracy: 0.6719\n",
            "Epoch 406/500\n",
            "128/128 [==============================] - 1s 9ms/step - loss: 0.5451 - accuracy: 0.7316 - val_loss: 0.6556 - val_accuracy: 0.6934\n",
            "Epoch 407/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.5617 - accuracy: 0.7323 - val_loss: 0.7086 - val_accuracy: 0.6865\n",
            "Epoch 408/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.5165 - accuracy: 0.7546 - val_loss: 0.6143 - val_accuracy: 0.7197\n",
            "Epoch 409/500\n",
            "128/128 [==============================] - 1s 9ms/step - loss: 0.5280 - accuracy: 0.7438 - val_loss: 0.5791 - val_accuracy: 0.7236\n",
            "Epoch 410/500\n",
            "128/128 [==============================] - 1s 9ms/step - loss: 0.5156 - accuracy: 0.7528 - val_loss: 0.6149 - val_accuracy: 0.7090\n",
            "Epoch 411/500\n",
            "128/128 [==============================] - 1s 9ms/step - loss: 0.5249 - accuracy: 0.7383 - val_loss: 0.5921 - val_accuracy: 0.7197\n",
            "Epoch 412/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.5121 - accuracy: 0.7508 - val_loss: 0.5773 - val_accuracy: 0.7344\n",
            "Epoch 413/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.5179 - accuracy: 0.7556 - val_loss: 0.5828 - val_accuracy: 0.7314\n",
            "Epoch 414/500\n",
            "128/128 [==============================] - 1s 9ms/step - loss: 0.5045 - accuracy: 0.7523 - val_loss: 0.6291 - val_accuracy: 0.7051\n",
            "Epoch 415/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.5070 - accuracy: 0.7484 - val_loss: 0.5739 - val_accuracy: 0.7305\n",
            "Epoch 416/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.5014 - accuracy: 0.7473 - val_loss: 0.6625 - val_accuracy: 0.6973\n",
            "Epoch 417/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.5208 - accuracy: 0.7494 - val_loss: 0.6040 - val_accuracy: 0.7129\n",
            "Epoch 418/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.5254 - accuracy: 0.7470 - val_loss: 0.5809 - val_accuracy: 0.7266\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00418: early stopping\n",
            "\n",
            "Accuracy: 72.66%\n",
            "Loss: 0.5809475779533386 \n",
            "\n",
            "NEXT SET OF HYPERPARAMETERS IS: \n",
            " num_conv_layers: 4 \n",
            " num_conv_nodes: 253 \n",
            " num_dense_layers: 3 \n",
            " num_dense_nodes: 189 \n",
            "\n",
            "Epoch 1/500\n",
            "  6/128 [>.............................] - ETA: 1s - loss: 2.8252 - accuracy: 0.1368  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0046s vs `on_train_batch_end` time: 0.0072s). Check your callbacks.\n",
            "128/128 [==============================] - 3s 15ms/step - loss: 1.5585 - accuracy: 0.4319 - val_loss: 2.7195 - val_accuracy: 0.0586\n",
            "Epoch 2/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.8527 - accuracy: 0.6181 - val_loss: 2.7125 - val_accuracy: 0.1152\n",
            "Epoch 3/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.7910 - accuracy: 0.6457 - val_loss: 2.5923 - val_accuracy: 0.1758\n",
            "Epoch 4/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.7214 - accuracy: 0.6963 - val_loss: 2.2679 - val_accuracy: 0.2383\n",
            "Epoch 5/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.7312 - accuracy: 0.6888 - val_loss: 1.2309 - val_accuracy: 0.4404\n",
            "Epoch 6/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.6871 - accuracy: 0.7070 - val_loss: 0.8067 - val_accuracy: 0.6514\n",
            "Epoch 7/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.6643 - accuracy: 0.7106 - val_loss: 0.8208 - val_accuracy: 0.6787\n",
            "Epoch 8/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.6095 - accuracy: 0.7425 - val_loss: 1.1160 - val_accuracy: 0.5596\n",
            "Epoch 9/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.7137 - accuracy: 0.6846 - val_loss: 1.6255 - val_accuracy: 0.3936\n",
            "Epoch 10/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.7061 - accuracy: 0.6908 - val_loss: 1.2500 - val_accuracy: 0.4863\n",
            "Epoch 11/500\n",
            "128/128 [==============================] - 2s 14ms/step - loss: 0.6385 - accuracy: 0.7364 - val_loss: 3.0760 - val_accuracy: 0.2500\n",
            "Epoch 12/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.6681 - accuracy: 0.7169 - val_loss: 4.4660 - val_accuracy: 0.2637\n",
            "Epoch 13/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.6994 - accuracy: 0.6992 - val_loss: 1.7096 - val_accuracy: 0.4805\n",
            "Epoch 14/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.7238 - accuracy: 0.6764 - val_loss: 1.5373 - val_accuracy: 0.3242\n",
            "Epoch 15/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.7274 - accuracy: 0.6718 - val_loss: 2.5025 - val_accuracy: 0.2988\n",
            "Epoch 16/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.7032 - accuracy: 0.6910 - val_loss: 2.3527 - val_accuracy: 0.2402\n",
            "Epoch 17/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.5419 - accuracy: 0.7898 - val_loss: 4.5915 - val_accuracy: 0.2568\n",
            "Epoch 18/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.6360 - accuracy: 0.7240 - val_loss: 2.0965 - val_accuracy: 0.2920\n",
            "Epoch 19/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.5683 - accuracy: 0.7631 - val_loss: 1.4268 - val_accuracy: 0.4668\n",
            "Epoch 20/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.6366 - accuracy: 0.7238 - val_loss: 3.3540 - val_accuracy: 0.2363\n",
            "Epoch 21/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.3725 - accuracy: 0.8512 - val_loss: 4.1268 - val_accuracy: 0.1533\n",
            "Epoch 22/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.3633 - accuracy: 0.8533 - val_loss: 1.1390 - val_accuracy: 0.4248\n",
            "Epoch 23/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.3843 - accuracy: 0.8378 - val_loss: 10.7403 - val_accuracy: 0.2246\n",
            "Epoch 24/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.4634 - accuracy: 0.8051 - val_loss: 1.9292 - val_accuracy: 0.4346\n",
            "Epoch 25/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.2699 - accuracy: 0.8934 - val_loss: 0.3295 - val_accuracy: 0.8857\n",
            "Epoch 26/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.2381 - accuracy: 0.9020 - val_loss: 0.2946 - val_accuracy: 0.8896\n",
            "Epoch 27/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.2255 - accuracy: 0.9005 - val_loss: 0.4310 - val_accuracy: 0.7695\n",
            "Epoch 28/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.2631 - accuracy: 0.8871 - val_loss: 0.9519 - val_accuracy: 0.6230\n",
            "Epoch 29/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.2140 - accuracy: 0.9040 - val_loss: 0.4937 - val_accuracy: 0.8418\n",
            "Epoch 30/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.2159 - accuracy: 0.8915 - val_loss: 0.3563 - val_accuracy: 0.8242\n",
            "Epoch 31/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.2080 - accuracy: 0.9011 - val_loss: 0.3589 - val_accuracy: 0.8438\n",
            "Epoch 32/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1943 - accuracy: 0.9140 - val_loss: 0.2092 - val_accuracy: 0.9219\n",
            "Epoch 33/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.2125 - accuracy: 0.9010 - val_loss: 0.2234 - val_accuracy: 0.8945\n",
            "Epoch 34/500\n",
            "128/128 [==============================] - 2s 14ms/step - loss: 0.1887 - accuracy: 0.9135 - val_loss: 0.2423 - val_accuracy: 0.9062\n",
            "Epoch 35/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.2085 - accuracy: 0.9072 - val_loss: 0.2022 - val_accuracy: 0.9180\n",
            "Epoch 36/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1983 - accuracy: 0.9069 - val_loss: 0.2227 - val_accuracy: 0.9170\n",
            "Epoch 37/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.2153 - accuracy: 0.9011 - val_loss: 0.2192 - val_accuracy: 0.9141\n",
            "Epoch 38/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.2029 - accuracy: 0.9063 - val_loss: 0.3384 - val_accuracy: 0.8633\n",
            "Epoch 39/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.2031 - accuracy: 0.9097 - val_loss: 0.2085 - val_accuracy: 0.9307\n",
            "Epoch 40/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1892 - accuracy: 0.9129 - val_loss: 0.2648 - val_accuracy: 0.8809\n",
            "Epoch 41/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.2348 - accuracy: 0.8987 - val_loss: 0.6247 - val_accuracy: 0.6436\n",
            "Epoch 42/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.2565 - accuracy: 0.8931 - val_loss: 0.6847 - val_accuracy: 0.6826\n",
            "Epoch 43/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.2053 - accuracy: 0.9125 - val_loss: 0.5322 - val_accuracy: 0.8604\n",
            "Epoch 44/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1940 - accuracy: 0.9111 - val_loss: 0.4081 - val_accuracy: 0.8584\n",
            "Epoch 45/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1952 - accuracy: 0.9145 - val_loss: 0.2226 - val_accuracy: 0.9082\n",
            "Epoch 46/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.2199 - accuracy: 0.9032 - val_loss: 0.2071 - val_accuracy: 0.9307\n",
            "Epoch 47/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.2092 - accuracy: 0.9017 - val_loss: 0.2173 - val_accuracy: 0.8975\n",
            "Epoch 48/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1926 - accuracy: 0.9198 - val_loss: 0.2127 - val_accuracy: 0.9121\n",
            "Epoch 49/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1828 - accuracy: 0.9203 - val_loss: 0.1951 - val_accuracy: 0.9336\n",
            "Epoch 50/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1898 - accuracy: 0.9177 - val_loss: 0.1958 - val_accuracy: 0.9219\n",
            "Epoch 51/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1810 - accuracy: 0.9173 - val_loss: 0.2039 - val_accuracy: 0.9229\n",
            "Epoch 52/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1790 - accuracy: 0.9185 - val_loss: 6.7012 - val_accuracy: 0.3623\n",
            "Epoch 53/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.2177 - accuracy: 0.9129 - val_loss: 0.1965 - val_accuracy: 0.9297\n",
            "Epoch 54/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1931 - accuracy: 0.9132 - val_loss: 0.2459 - val_accuracy: 0.8730\n",
            "Epoch 55/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1878 - accuracy: 0.9178 - val_loss: 0.2006 - val_accuracy: 0.9141\n",
            "Epoch 56/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1973 - accuracy: 0.9108 - val_loss: 0.2222 - val_accuracy: 0.9033\n",
            "Epoch 57/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.2029 - accuracy: 0.9061 - val_loss: 0.1998 - val_accuracy: 0.9189\n",
            "Epoch 58/500\n",
            "128/128 [==============================] - 2s 14ms/step - loss: 0.2001 - accuracy: 0.9090 - val_loss: 0.2104 - val_accuracy: 0.9307\n",
            "Epoch 59/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1873 - accuracy: 0.9092 - val_loss: 0.2157 - val_accuracy: 0.9004\n",
            "Epoch 60/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1635 - accuracy: 0.9266 - val_loss: 0.1961 - val_accuracy: 0.9062\n",
            "Epoch 61/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1862 - accuracy: 0.9155 - val_loss: 0.2149 - val_accuracy: 0.9150\n",
            "Epoch 62/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.2019 - accuracy: 0.9105 - val_loss: 0.2106 - val_accuracy: 0.9229\n",
            "Epoch 63/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1860 - accuracy: 0.9127 - val_loss: 0.2027 - val_accuracy: 0.9238\n",
            "Epoch 64/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1793 - accuracy: 0.9245 - val_loss: 0.1983 - val_accuracy: 0.9199\n",
            "Epoch 65/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1782 - accuracy: 0.9228 - val_loss: 0.2501 - val_accuracy: 0.8906\n",
            "Epoch 66/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1706 - accuracy: 0.9266 - val_loss: 0.1887 - val_accuracy: 0.9277\n",
            "Epoch 67/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1701 - accuracy: 0.9239 - val_loss: 0.3109 - val_accuracy: 0.8828\n",
            "Epoch 68/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1750 - accuracy: 0.9241 - val_loss: 0.2144 - val_accuracy: 0.9023\n",
            "Epoch 69/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1766 - accuracy: 0.9245 - val_loss: 0.1769 - val_accuracy: 0.9287\n",
            "Epoch 70/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1839 - accuracy: 0.9128 - val_loss: 0.2037 - val_accuracy: 0.9160\n",
            "Epoch 71/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1857 - accuracy: 0.9131 - val_loss: 0.2004 - val_accuracy: 0.9053\n",
            "Epoch 72/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1711 - accuracy: 0.9236 - val_loss: 0.2020 - val_accuracy: 0.9043\n",
            "Epoch 73/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1694 - accuracy: 0.9267 - val_loss: 0.1751 - val_accuracy: 0.9336\n",
            "Epoch 74/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1739 - accuracy: 0.9212 - val_loss: 0.2133 - val_accuracy: 0.9121\n",
            "Epoch 75/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1824 - accuracy: 0.9118 - val_loss: 0.1909 - val_accuracy: 0.9229\n",
            "Epoch 76/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1761 - accuracy: 0.9195 - val_loss: 0.1825 - val_accuracy: 0.9316\n",
            "Epoch 77/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1754 - accuracy: 0.9353 - val_loss: 0.1934 - val_accuracy: 0.9346\n",
            "Epoch 78/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1791 - accuracy: 0.9201 - val_loss: 0.1830 - val_accuracy: 0.9150\n",
            "Epoch 79/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1639 - accuracy: 0.9302 - val_loss: 0.2086 - val_accuracy: 0.9189\n",
            "Epoch 80/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1709 - accuracy: 0.9241 - val_loss: 0.1679 - val_accuracy: 0.9404\n",
            "Epoch 81/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1565 - accuracy: 0.9247 - val_loss: 0.1838 - val_accuracy: 0.9248\n",
            "Epoch 82/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1649 - accuracy: 0.9303 - val_loss: 0.1824 - val_accuracy: 0.9346\n",
            "Epoch 83/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1659 - accuracy: 0.9265 - val_loss: 0.1940 - val_accuracy: 0.9180\n",
            "Epoch 84/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1633 - accuracy: 0.9250 - val_loss: 0.1943 - val_accuracy: 0.9248\n",
            "Epoch 85/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1489 - accuracy: 0.9399 - val_loss: 0.2194 - val_accuracy: 0.8906\n",
            "Epoch 86/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1684 - accuracy: 0.9201 - val_loss: 0.2260 - val_accuracy: 0.8955\n",
            "Epoch 87/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1544 - accuracy: 0.9345 - val_loss: 0.1793 - val_accuracy: 0.9287\n",
            "Epoch 88/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1604 - accuracy: 0.9282 - val_loss: 0.1785 - val_accuracy: 0.9365\n",
            "Epoch 89/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1699 - accuracy: 0.9209 - val_loss: 0.1973 - val_accuracy: 0.9287\n",
            "Epoch 90/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1710 - accuracy: 0.9250 - val_loss: 0.1792 - val_accuracy: 0.9316\n",
            "Epoch 91/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1763 - accuracy: 0.9234 - val_loss: 0.2124 - val_accuracy: 0.9004\n",
            "Epoch 92/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1608 - accuracy: 0.9301 - val_loss: 0.1569 - val_accuracy: 0.9482\n",
            "Epoch 93/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1393 - accuracy: 0.9431 - val_loss: 0.1957 - val_accuracy: 0.9092\n",
            "Epoch 94/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1628 - accuracy: 0.9229 - val_loss: 0.1766 - val_accuracy: 0.9365\n",
            "Epoch 95/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1570 - accuracy: 0.9282 - val_loss: 0.1722 - val_accuracy: 0.9326\n",
            "Epoch 96/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1513 - accuracy: 0.9334 - val_loss: 0.1666 - val_accuracy: 0.9355\n",
            "Epoch 97/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1556 - accuracy: 0.9303 - val_loss: 0.1827 - val_accuracy: 0.9316\n",
            "Epoch 98/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1474 - accuracy: 0.9374 - val_loss: 0.1926 - val_accuracy: 0.9238\n",
            "Epoch 99/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1468 - accuracy: 0.9398 - val_loss: 2.8341 - val_accuracy: 0.5586\n",
            "Epoch 100/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1595 - accuracy: 0.9350 - val_loss: 0.1647 - val_accuracy: 0.9414\n",
            "Epoch 101/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1414 - accuracy: 0.9438 - val_loss: 0.2256 - val_accuracy: 0.9043\n",
            "Epoch 102/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1468 - accuracy: 0.9333 - val_loss: 0.1526 - val_accuracy: 0.9434\n",
            "Epoch 103/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1444 - accuracy: 0.9391 - val_loss: 0.2000 - val_accuracy: 0.9082\n",
            "Epoch 104/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1354 - accuracy: 0.9440 - val_loss: 0.1767 - val_accuracy: 0.9297\n",
            "Epoch 105/500\n",
            "128/128 [==============================] - 2s 14ms/step - loss: 0.1241 - accuracy: 0.9511 - val_loss: 0.2013 - val_accuracy: 0.9014\n",
            "Epoch 106/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1238 - accuracy: 0.9514 - val_loss: 0.1452 - val_accuracy: 0.9512\n",
            "Epoch 107/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1248 - accuracy: 0.9475 - val_loss: 7.5070 - val_accuracy: 0.3779\n",
            "Epoch 108/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1298 - accuracy: 0.9507 - val_loss: 17.2118 - val_accuracy: 0.3184\n",
            "Epoch 109/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1218 - accuracy: 0.9511 - val_loss: 0.1671 - val_accuracy: 0.9443\n",
            "Epoch 110/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1238 - accuracy: 0.9531 - val_loss: 0.1514 - val_accuracy: 0.9424\n",
            "Epoch 111/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1197 - accuracy: 0.9514 - val_loss: 0.2228 - val_accuracy: 0.9111\n",
            "Epoch 112/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1153 - accuracy: 0.9489 - val_loss: 0.2544 - val_accuracy: 0.8955\n",
            "Epoch 113/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1360 - accuracy: 0.9416 - val_loss: 0.1456 - val_accuracy: 0.9395\n",
            "Epoch 114/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1147 - accuracy: 0.9518 - val_loss: 0.3675 - val_accuracy: 0.8828\n",
            "Epoch 115/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0996 - accuracy: 0.9629 - val_loss: 0.1513 - val_accuracy: 0.9395\n",
            "Epoch 116/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1066 - accuracy: 0.9563 - val_loss: 0.1756 - val_accuracy: 0.9258\n",
            "Epoch 117/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1028 - accuracy: 0.9607 - val_loss: 0.3324 - val_accuracy: 0.8672\n",
            "Epoch 118/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1052 - accuracy: 0.9568 - val_loss: 0.1496 - val_accuracy: 0.9365\n",
            "Epoch 119/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0885 - accuracy: 0.9693 - val_loss: 0.1259 - val_accuracy: 0.9492\n",
            "Epoch 120/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1140 - accuracy: 0.9571 - val_loss: 0.3847 - val_accuracy: 0.8691\n",
            "Epoch 121/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.1078 - accuracy: 0.9612 - val_loss: 0.1471 - val_accuracy: 0.9365\n",
            "Epoch 122/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0955 - accuracy: 0.9644 - val_loss: 0.1571 - val_accuracy: 0.9424\n",
            "Epoch 123/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0887 - accuracy: 0.9674 - val_loss: 0.1705 - val_accuracy: 0.9326\n",
            "Epoch 124/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0927 - accuracy: 0.9638 - val_loss: 0.1224 - val_accuracy: 0.9502\n",
            "Epoch 125/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0874 - accuracy: 0.9681 - val_loss: 0.4705 - val_accuracy: 0.8359\n",
            "Epoch 126/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0886 - accuracy: 0.9713 - val_loss: 0.3977 - val_accuracy: 0.8662\n",
            "Epoch 127/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0913 - accuracy: 0.9637 - val_loss: 0.1141 - val_accuracy: 0.9492\n",
            "Epoch 128/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0884 - accuracy: 0.9624 - val_loss: 0.1072 - val_accuracy: 0.9600\n",
            "Epoch 129/500\n",
            "128/128 [==============================] - 2s 14ms/step - loss: 0.1036 - accuracy: 0.9603 - val_loss: 0.1623 - val_accuracy: 0.9375\n",
            "Epoch 130/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0823 - accuracy: 0.9690 - val_loss: 0.1101 - val_accuracy: 0.9600\n",
            "Epoch 131/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0714 - accuracy: 0.9704 - val_loss: 0.1252 - val_accuracy: 0.9561\n",
            "Epoch 132/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0621 - accuracy: 0.9753 - val_loss: 0.1505 - val_accuracy: 0.9424\n",
            "Epoch 133/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0737 - accuracy: 0.9707 - val_loss: 0.1830 - val_accuracy: 0.9209\n",
            "Epoch 134/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0901 - accuracy: 0.9655 - val_loss: 0.1226 - val_accuracy: 0.9609\n",
            "Epoch 135/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0685 - accuracy: 0.9748 - val_loss: 0.1231 - val_accuracy: 0.9502\n",
            "Epoch 136/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0732 - accuracy: 0.9747 - val_loss: 0.4252 - val_accuracy: 0.8477\n",
            "Epoch 137/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0714 - accuracy: 0.9726 - val_loss: 0.1430 - val_accuracy: 0.9453\n",
            "Epoch 138/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0694 - accuracy: 0.9739 - val_loss: 0.5713 - val_accuracy: 0.7676\n",
            "Epoch 139/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0573 - accuracy: 0.9808 - val_loss: 0.1019 - val_accuracy: 0.9629\n",
            "Epoch 140/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0524 - accuracy: 0.9829 - val_loss: 0.1166 - val_accuracy: 0.9570\n",
            "Epoch 141/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0698 - accuracy: 0.9724 - val_loss: 0.1050 - val_accuracy: 0.9658\n",
            "Epoch 142/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0678 - accuracy: 0.9740 - val_loss: 0.0797 - val_accuracy: 0.9658\n",
            "Epoch 143/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0535 - accuracy: 0.9805 - val_loss: 0.2715 - val_accuracy: 0.8789\n",
            "Epoch 144/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0536 - accuracy: 0.9805 - val_loss: 0.1214 - val_accuracy: 0.9502\n",
            "Epoch 145/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0559 - accuracy: 0.9800 - val_loss: 0.0998 - val_accuracy: 0.9580\n",
            "Epoch 146/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0517 - accuracy: 0.9811 - val_loss: 0.1287 - val_accuracy: 0.9570\n",
            "Epoch 147/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0589 - accuracy: 0.9796 - val_loss: 0.2397 - val_accuracy: 0.9072\n",
            "Epoch 148/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0490 - accuracy: 0.9814 - val_loss: 0.0871 - val_accuracy: 0.9678\n",
            "Epoch 149/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0691 - accuracy: 0.9760 - val_loss: 0.0957 - val_accuracy: 0.9570\n",
            "Epoch 150/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0563 - accuracy: 0.9768 - val_loss: 0.0862 - val_accuracy: 0.9668\n",
            "Epoch 151/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0444 - accuracy: 0.9841 - val_loss: 0.1770 - val_accuracy: 0.9297\n",
            "Epoch 152/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0504 - accuracy: 0.9803 - val_loss: 0.0698 - val_accuracy: 0.9717\n",
            "Epoch 153/500\n",
            "128/128 [==============================] - 2s 14ms/step - loss: 0.0409 - accuracy: 0.9855 - val_loss: 0.0886 - val_accuracy: 0.9668\n",
            "Epoch 154/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0415 - accuracy: 0.9867 - val_loss: 0.7653 - val_accuracy: 0.7246\n",
            "Epoch 155/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0559 - accuracy: 0.9779 - val_loss: 0.1196 - val_accuracy: 0.9570\n",
            "Epoch 156/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0592 - accuracy: 0.9739 - val_loss: 0.1615 - val_accuracy: 0.9307\n",
            "Epoch 157/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0439 - accuracy: 0.9843 - val_loss: 0.1061 - val_accuracy: 0.9512\n",
            "Epoch 158/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0466 - accuracy: 0.9810 - val_loss: 0.2422 - val_accuracy: 0.9014\n",
            "Epoch 159/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0392 - accuracy: 0.9868 - val_loss: 0.0912 - val_accuracy: 0.9609\n",
            "Epoch 160/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0390 - accuracy: 0.9869 - val_loss: 0.1615 - val_accuracy: 0.9395\n",
            "Epoch 161/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0375 - accuracy: 0.9868 - val_loss: 0.0911 - val_accuracy: 0.9707\n",
            "Epoch 162/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0347 - accuracy: 0.9876 - val_loss: 0.1114 - val_accuracy: 0.9580\n",
            "Epoch 163/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0267 - accuracy: 0.9923 - val_loss: 0.1349 - val_accuracy: 0.9502\n",
            "Epoch 164/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0374 - accuracy: 0.9868 - val_loss: 0.1810 - val_accuracy: 0.9316\n",
            "Epoch 165/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0318 - accuracy: 0.9884 - val_loss: 0.0915 - val_accuracy: 0.9609\n",
            "Epoch 166/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0362 - accuracy: 0.9867 - val_loss: 0.1554 - val_accuracy: 0.9453\n",
            "Epoch 167/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0278 - accuracy: 0.9897 - val_loss: 0.1527 - val_accuracy: 0.9385\n",
            "Epoch 168/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0306 - accuracy: 0.9902 - val_loss: 0.0635 - val_accuracy: 0.9717\n",
            "Epoch 169/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0307 - accuracy: 0.9883 - val_loss: 0.1794 - val_accuracy: 0.9414\n",
            "Epoch 170/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0347 - accuracy: 0.9894 - val_loss: 0.0834 - val_accuracy: 0.9658\n",
            "Epoch 171/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0388 - accuracy: 0.9859 - val_loss: 0.1419 - val_accuracy: 0.9482\n",
            "Epoch 172/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0378 - accuracy: 0.9861 - val_loss: 0.0786 - val_accuracy: 0.9668\n",
            "Epoch 173/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0328 - accuracy: 0.9890 - val_loss: 0.0987 - val_accuracy: 0.9648\n",
            "Epoch 174/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0342 - accuracy: 0.9895 - val_loss: 0.1385 - val_accuracy: 0.9551\n",
            "Epoch 175/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0269 - accuracy: 0.9896 - val_loss: 0.1199 - val_accuracy: 0.9521\n",
            "Epoch 176/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0298 - accuracy: 0.9877 - val_loss: 0.1700 - val_accuracy: 0.9453\n",
            "Epoch 177/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0235 - accuracy: 0.9922 - val_loss: 0.1499 - val_accuracy: 0.9385\n",
            "Epoch 178/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0333 - accuracy: 0.9852 - val_loss: 0.0868 - val_accuracy: 0.9648\n",
            "Epoch 179/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0310 - accuracy: 0.9886 - val_loss: 0.0724 - val_accuracy: 0.9746\n",
            "Epoch 180/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0262 - accuracy: 0.9911 - val_loss: 0.0800 - val_accuracy: 0.9658\n",
            "Epoch 181/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0277 - accuracy: 0.9909 - val_loss: 0.1102 - val_accuracy: 0.9619\n",
            "Epoch 182/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0228 - accuracy: 0.9908 - val_loss: 0.1026 - val_accuracy: 0.9609\n",
            "Epoch 183/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0255 - accuracy: 0.9901 - val_loss: 0.0980 - val_accuracy: 0.9590\n",
            "Epoch 184/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0385 - accuracy: 0.9866 - val_loss: 0.0622 - val_accuracy: 0.9756\n",
            "Epoch 185/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0217 - accuracy: 0.9910 - val_loss: 0.1290 - val_accuracy: 0.9551\n",
            "Epoch 186/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0277 - accuracy: 0.9897 - val_loss: 0.1173 - val_accuracy: 0.9580\n",
            "Epoch 187/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0225 - accuracy: 0.9928 - val_loss: 0.0658 - val_accuracy: 0.9736\n",
            "Epoch 188/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0204 - accuracy: 0.9925 - val_loss: 0.0656 - val_accuracy: 0.9717\n",
            "Epoch 189/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0378 - accuracy: 0.9875 - val_loss: 0.1566 - val_accuracy: 0.9336\n",
            "Epoch 190/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0317 - accuracy: 0.9876 - val_loss: 0.0831 - val_accuracy: 0.9736\n",
            "Epoch 191/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0231 - accuracy: 0.9919 - val_loss: 0.0687 - val_accuracy: 0.9707\n",
            "Epoch 192/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0283 - accuracy: 0.9882 - val_loss: 0.0865 - val_accuracy: 0.9658\n",
            "Epoch 193/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0337 - accuracy: 0.9870 - val_loss: 0.0821 - val_accuracy: 0.9717\n",
            "Epoch 194/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0238 - accuracy: 0.9893 - val_loss: 0.3668 - val_accuracy: 0.8994\n",
            "Epoch 195/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0283 - accuracy: 0.9880 - val_loss: 0.0689 - val_accuracy: 0.9746\n",
            "Epoch 196/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0283 - accuracy: 0.9908 - val_loss: 0.0735 - val_accuracy: 0.9736\n",
            "Epoch 197/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0223 - accuracy: 0.9922 - val_loss: 0.0820 - val_accuracy: 0.9717\n",
            "Epoch 198/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0232 - accuracy: 0.9902 - val_loss: 0.3528 - val_accuracy: 0.8887\n",
            "Epoch 199/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0223 - accuracy: 0.9930 - val_loss: 0.2342 - val_accuracy: 0.9131\n",
            "Epoch 200/500\n",
            "128/128 [==============================] - 2s 14ms/step - loss: 0.0284 - accuracy: 0.9884 - val_loss: 0.0786 - val_accuracy: 0.9697\n",
            "Epoch 201/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0196 - accuracy: 0.9924 - val_loss: 0.1016 - val_accuracy: 0.9619\n",
            "Epoch 202/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0225 - accuracy: 0.9915 - val_loss: 0.1275 - val_accuracy: 0.9492\n",
            "Epoch 203/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0215 - accuracy: 0.9917 - val_loss: 0.0707 - val_accuracy: 0.9727\n",
            "Epoch 204/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0124 - accuracy: 0.9952 - val_loss: 0.0608 - val_accuracy: 0.9756\n",
            "Epoch 205/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0154 - accuracy: 0.9953 - val_loss: 0.0728 - val_accuracy: 0.9756\n",
            "Epoch 206/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0147 - accuracy: 0.9955 - val_loss: 0.1563 - val_accuracy: 0.9248\n",
            "Epoch 207/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0871 - accuracy: 0.9785 - val_loss: 0.1827 - val_accuracy: 0.9355\n",
            "Epoch 208/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0335 - accuracy: 0.9864 - val_loss: 0.1124 - val_accuracy: 0.9629\n",
            "Epoch 209/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0284 - accuracy: 0.9897 - val_loss: 0.0687 - val_accuracy: 0.9746\n",
            "Epoch 210/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0194 - accuracy: 0.9937 - val_loss: 0.1413 - val_accuracy: 0.9473\n",
            "Epoch 211/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0165 - accuracy: 0.9946 - val_loss: 0.1079 - val_accuracy: 0.9668\n",
            "Epoch 212/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0161 - accuracy: 0.9952 - val_loss: 0.0854 - val_accuracy: 0.9688\n",
            "Epoch 213/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0292 - accuracy: 0.9876 - val_loss: 0.0700 - val_accuracy: 0.9746\n",
            "Epoch 214/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0193 - accuracy: 0.9921 - val_loss: 0.0555 - val_accuracy: 0.9766\n",
            "Epoch 215/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0167 - accuracy: 0.9924 - val_loss: 0.0694 - val_accuracy: 0.9766\n",
            "Epoch 216/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0211 - accuracy: 0.9916 - val_loss: 0.1067 - val_accuracy: 0.9590\n",
            "Epoch 217/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0220 - accuracy: 0.9903 - val_loss: 0.0697 - val_accuracy: 0.9697\n",
            "Epoch 218/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0171 - accuracy: 0.9916 - val_loss: 0.0870 - val_accuracy: 0.9707\n",
            "Epoch 219/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0143 - accuracy: 0.9947 - val_loss: 5.4336 - val_accuracy: 0.5400\n",
            "Epoch 220/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0153 - accuracy: 0.9942 - val_loss: 0.1709 - val_accuracy: 0.9326\n",
            "Epoch 221/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0181 - accuracy: 0.9924 - val_loss: 0.1058 - val_accuracy: 0.9600\n",
            "Epoch 222/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0110 - accuracy: 0.9960 - val_loss: 0.1303 - val_accuracy: 0.9541\n",
            "Epoch 223/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0116 - accuracy: 0.9965 - val_loss: 0.0687 - val_accuracy: 0.9746\n",
            "Epoch 224/500\n",
            "128/128 [==============================] - 2s 14ms/step - loss: 0.0105 - accuracy: 0.9960 - val_loss: 0.0869 - val_accuracy: 0.9688\n",
            "Epoch 225/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0132 - accuracy: 0.9957 - val_loss: 0.0751 - val_accuracy: 0.9727\n",
            "Epoch 226/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0175 - accuracy: 0.9938 - val_loss: 0.1299 - val_accuracy: 0.9551\n",
            "Epoch 227/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0337 - accuracy: 0.9901 - val_loss: 3.6812 - val_accuracy: 0.6377\n",
            "Epoch 228/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0162 - accuracy: 0.9942 - val_loss: 0.0843 - val_accuracy: 0.9727\n",
            "Epoch 229/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0287 - accuracy: 0.9922 - val_loss: 0.1784 - val_accuracy: 0.9414\n",
            "Epoch 230/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0239 - accuracy: 0.9922 - val_loss: 0.3776 - val_accuracy: 0.8721\n",
            "Epoch 231/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0195 - accuracy: 0.9940 - val_loss: 0.1748 - val_accuracy: 0.9180\n",
            "Epoch 232/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0164 - accuracy: 0.9941 - val_loss: 0.1158 - val_accuracy: 0.9482\n",
            "Epoch 233/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0169 - accuracy: 0.9927 - val_loss: 0.1230 - val_accuracy: 0.9541\n",
            "Epoch 234/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0185 - accuracy: 0.9930 - val_loss: 0.1253 - val_accuracy: 0.9453\n",
            "Epoch 235/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0247 - accuracy: 0.9895 - val_loss: 0.3791 - val_accuracy: 0.9043\n",
            "Epoch 236/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0205 - accuracy: 0.9945 - val_loss: 0.0624 - val_accuracy: 0.9727\n",
            "Epoch 237/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0122 - accuracy: 0.9963 - val_loss: 0.0628 - val_accuracy: 0.9746\n",
            "Epoch 238/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0169 - accuracy: 0.9932 - val_loss: 0.0569 - val_accuracy: 0.9766\n",
            "Epoch 239/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0107 - accuracy: 0.9981 - val_loss: 0.0574 - val_accuracy: 0.9795\n",
            "Epoch 240/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0075 - accuracy: 0.9981 - val_loss: 0.1057 - val_accuracy: 0.9570\n",
            "Epoch 241/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0123 - accuracy: 0.9958 - val_loss: 0.0614 - val_accuracy: 0.9766\n",
            "Epoch 242/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0080 - accuracy: 0.9969 - val_loss: 0.1476 - val_accuracy: 0.9473\n",
            "Epoch 243/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0106 - accuracy: 0.9960 - val_loss: 0.0824 - val_accuracy: 0.9736\n",
            "Epoch 244/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0121 - accuracy: 0.9951 - val_loss: 0.0653 - val_accuracy: 0.9805\n",
            "Epoch 245/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0377 - accuracy: 0.9875 - val_loss: 0.1244 - val_accuracy: 0.9619\n",
            "Epoch 246/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0093 - accuracy: 0.9968 - val_loss: 0.1174 - val_accuracy: 0.9580\n",
            "Epoch 247/500\n",
            "128/128 [==============================] - 2s 14ms/step - loss: 0.0085 - accuracy: 0.9980 - val_loss: 0.0661 - val_accuracy: 0.9805\n",
            "Epoch 248/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0098 - accuracy: 0.9961 - val_loss: 0.0867 - val_accuracy: 0.9766\n",
            "Epoch 249/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0123 - accuracy: 0.9955 - val_loss: 0.0864 - val_accuracy: 0.9727\n",
            "Epoch 250/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0105 - accuracy: 0.9962 - val_loss: 0.0902 - val_accuracy: 0.9629\n",
            "Epoch 251/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0039 - accuracy: 0.9999 - val_loss: 31.4017 - val_accuracy: 0.3193\n",
            "Epoch 252/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0083 - accuracy: 0.9978 - val_loss: 0.1030 - val_accuracy: 0.9619\n",
            "Epoch 253/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0056 - accuracy: 0.9990 - val_loss: 0.0598 - val_accuracy: 0.9775\n",
            "Epoch 254/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0133 - accuracy: 0.9961 - val_loss: 0.6455 - val_accuracy: 0.8350\n",
            "Epoch 255/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0807 - accuracy: 0.9772 - val_loss: 0.0795 - val_accuracy: 0.9746\n",
            "Epoch 256/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0117 - accuracy: 0.9960 - val_loss: 0.1123 - val_accuracy: 0.9629\n",
            "Epoch 257/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0099 - accuracy: 0.9976 - val_loss: 0.1000 - val_accuracy: 0.9688\n",
            "Epoch 258/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0064 - accuracy: 0.9983 - val_loss: 0.0758 - val_accuracy: 0.9746\n",
            "Epoch 259/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0210 - accuracy: 0.9914 - val_loss: 0.0619 - val_accuracy: 0.9834\n",
            "Epoch 260/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0052 - accuracy: 0.9985 - val_loss: 0.0615 - val_accuracy: 0.9805\n",
            "Epoch 261/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0057 - accuracy: 0.9985 - val_loss: 0.0563 - val_accuracy: 0.9775\n",
            "Epoch 262/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0063 - accuracy: 0.9978 - val_loss: 0.1379 - val_accuracy: 0.9580\n",
            "Epoch 263/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0076 - accuracy: 0.9969 - val_loss: 0.0505 - val_accuracy: 0.9805\n",
            "Epoch 264/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0064 - accuracy: 0.9978 - val_loss: 0.0630 - val_accuracy: 0.9766\n",
            "Epoch 265/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0060 - accuracy: 0.9979 - val_loss: 0.0407 - val_accuracy: 0.9824\n",
            "Epoch 266/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 0.1167 - val_accuracy: 0.9619\n",
            "Epoch 267/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0062 - accuracy: 0.9976 - val_loss: 0.1048 - val_accuracy: 0.9609\n",
            "Epoch 268/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0057 - accuracy: 0.9976 - val_loss: 0.0813 - val_accuracy: 0.9678\n",
            "Epoch 269/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0043 - accuracy: 0.9995 - val_loss: 0.0622 - val_accuracy: 0.9775\n",
            "Epoch 270/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0068 - accuracy: 0.9977 - val_loss: 0.0597 - val_accuracy: 0.9814\n",
            "Epoch 271/500\n",
            "128/128 [==============================] - 2s 14ms/step - loss: 0.0049 - accuracy: 0.9990 - val_loss: 0.0598 - val_accuracy: 0.9785\n",
            "Epoch 272/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0054 - accuracy: 0.9988 - val_loss: 0.0554 - val_accuracy: 0.9805\n",
            "Epoch 273/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0149 - accuracy: 0.9946 - val_loss: 0.0918 - val_accuracy: 0.9717\n",
            "Epoch 274/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0139 - accuracy: 0.9947 - val_loss: 0.2577 - val_accuracy: 0.9102\n",
            "Epoch 275/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0104 - accuracy: 0.9951 - val_loss: 0.1092 - val_accuracy: 0.9678\n",
            "Epoch 276/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0092 - accuracy: 0.9965 - val_loss: 0.0973 - val_accuracy: 0.9688\n",
            "Epoch 277/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0098 - accuracy: 0.9963 - val_loss: 0.1115 - val_accuracy: 0.9717\n",
            "Epoch 278/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0083 - accuracy: 0.9974 - val_loss: 0.0723 - val_accuracy: 0.9736\n",
            "Epoch 279/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0077 - accuracy: 0.9980 - val_loss: 0.1938 - val_accuracy: 0.9248\n",
            "Epoch 280/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0059 - accuracy: 0.9980 - val_loss: 0.0931 - val_accuracy: 0.9707\n",
            "Epoch 281/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0038 - accuracy: 0.9989 - val_loss: 0.0660 - val_accuracy: 0.9785\n",
            "Epoch 282/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.1016 - val_accuracy: 0.9648\n",
            "Epoch 283/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0028 - accuracy: 0.9998 - val_loss: 0.0606 - val_accuracy: 0.9795\n",
            "Epoch 284/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.1943 - val_accuracy: 0.9336\n",
            "Epoch 285/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0022 - accuracy: 0.9998 - val_loss: 0.0518 - val_accuracy: 0.9824\n",
            "Epoch 286/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0607 - val_accuracy: 0.9834\n",
            "Epoch 287/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0042 - accuracy: 0.9992 - val_loss: 0.0583 - val_accuracy: 0.9775\n",
            "Epoch 288/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0021 - accuracy: 0.9999 - val_loss: 0.0574 - val_accuracy: 0.9795\n",
            "Epoch 289/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.1120 - val_accuracy: 0.9580\n",
            "Epoch 290/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0035 - accuracy: 0.9995 - val_loss: 0.0562 - val_accuracy: 0.9805\n",
            "Epoch 291/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0020 - accuracy: 0.9998 - val_loss: 0.0496 - val_accuracy: 0.9844\n",
            "Epoch 292/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0024 - accuracy: 0.9991 - val_loss: 0.1062 - val_accuracy: 0.9746\n",
            "Epoch 293/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0052 - accuracy: 0.9995 - val_loss: 0.8051 - val_accuracy: 0.7998\n",
            "Epoch 294/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0184 - accuracy: 0.9911 - val_loss: 0.0675 - val_accuracy: 0.9814\n",
            "Epoch 295/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0032 - accuracy: 0.9998 - val_loss: 0.0512 - val_accuracy: 0.9814\n",
            "Epoch 296/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0044 - accuracy: 0.9990 - val_loss: 0.0639 - val_accuracy: 0.9824\n",
            "Epoch 297/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.1249 - val_accuracy: 0.9600\n",
            "Epoch 298/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0637 - val_accuracy: 0.9824\n",
            "Epoch 299/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0597 - val_accuracy: 0.9805\n",
            "Epoch 300/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 7.5857e-04 - accuracy: 1.0000 - val_loss: 0.0604 - val_accuracy: 0.9814\n",
            "Epoch 301/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0855 - val_accuracy: 0.9668\n",
            "Epoch 302/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.0681 - val_accuracy: 0.9834\n",
            "Epoch 303/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0721 - val_accuracy: 0.9785\n",
            "Epoch 304/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.2094 - val_accuracy: 0.9219\n",
            "Epoch 305/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0019 - accuracy: 0.9992 - val_loss: 0.0594 - val_accuracy: 0.9844\n",
            "Epoch 306/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.0817 - val_accuracy: 0.9707\n",
            "Epoch 307/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0023 - accuracy: 0.9988 - val_loss: 0.0579 - val_accuracy: 0.9814\n",
            "Epoch 308/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 9.2400e-04 - accuracy: 1.0000 - val_loss: 0.0509 - val_accuracy: 0.9834\n",
            "Epoch 309/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0560 - val_accuracy: 0.9814\n",
            "Epoch 310/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 6.4526e-04 - accuracy: 1.0000 - val_loss: 0.0613 - val_accuracy: 0.9814\n",
            "Epoch 311/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 7.7586e-04 - accuracy: 1.0000 - val_loss: 0.0550 - val_accuracy: 0.9834\n",
            "Epoch 312/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 9.8532e-04 - accuracy: 0.9999 - val_loss: 0.0834 - val_accuracy: 0.9756\n",
            "Epoch 313/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0222 - accuracy: 0.9948 - val_loss: 0.0624 - val_accuracy: 0.9805\n",
            "Epoch 314/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0027 - accuracy: 0.9998 - val_loss: 0.0579 - val_accuracy: 0.9834\n",
            "Epoch 315/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0619 - val_accuracy: 0.9844\n",
            "Epoch 316/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 0.0712 - val_accuracy: 0.9775\n",
            "Epoch 317/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0619 - val_accuracy: 0.9795\n",
            "Epoch 318/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.1778 - val_accuracy: 0.9248\n",
            "Epoch 319/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 8.6581e-04 - accuracy: 1.0000 - val_loss: 0.0572 - val_accuracy: 0.9814\n",
            "Epoch 320/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 5.7183e-04 - accuracy: 1.0000 - val_loss: 0.0797 - val_accuracy: 0.9766\n",
            "Epoch 321/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 5.9947e-04 - accuracy: 1.0000 - val_loss: 0.0654 - val_accuracy: 0.9805\n",
            "Epoch 322/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 6.7103e-04 - accuracy: 1.0000 - val_loss: 0.0546 - val_accuracy: 0.9844\n",
            "Epoch 323/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 5.5325e-04 - accuracy: 1.0000 - val_loss: 0.0605 - val_accuracy: 0.9814\n",
            "Epoch 324/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 6.7491e-04 - accuracy: 1.0000 - val_loss: 0.0698 - val_accuracy: 0.9785\n",
            "Epoch 325/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 6.2997e-04 - accuracy: 1.0000 - val_loss: 0.0547 - val_accuracy: 0.9824\n",
            "Epoch 326/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0276 - accuracy: 0.9887 - val_loss: 0.3437 - val_accuracy: 0.9297\n",
            "Epoch 327/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0384 - accuracy: 0.9882 - val_loss: 0.2188 - val_accuracy: 0.9180\n",
            "Epoch 328/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0046 - accuracy: 0.9986 - val_loss: 0.0807 - val_accuracy: 0.9717\n",
            "Epoch 329/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0129 - accuracy: 0.9958 - val_loss: 0.0763 - val_accuracy: 0.9756\n",
            "Epoch 330/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0824 - val_accuracy: 0.9766\n",
            "Epoch 331/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.0937 - val_accuracy: 0.9727\n",
            "Epoch 332/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0028 - accuracy: 0.9996 - val_loss: 0.0549 - val_accuracy: 0.9814\n",
            "Epoch 333/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0539 - val_accuracy: 0.9824\n",
            "Epoch 334/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0053 - accuracy: 0.9985 - val_loss: 0.0564 - val_accuracy: 0.9775\n",
            "Epoch 335/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0556 - val_accuracy: 0.9844\n",
            "Epoch 336/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 8.2476e-04 - accuracy: 1.0000 - val_loss: 0.0554 - val_accuracy: 0.9824\n",
            "Epoch 337/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 8.0091e-04 - accuracy: 1.0000 - val_loss: 0.0449 - val_accuracy: 0.9834\n",
            "Epoch 338/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.0810 - val_accuracy: 0.9746\n",
            "Epoch 339/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0551 - val_accuracy: 0.9805\n",
            "Epoch 340/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 6.5064e-04 - accuracy: 1.0000 - val_loss: 0.0559 - val_accuracy: 0.9795\n",
            "Epoch 341/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0814 - val_accuracy: 0.9775\n",
            "Epoch 342/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 9.6025e-04 - accuracy: 1.0000 - val_loss: 0.0787 - val_accuracy: 0.9785\n",
            "Epoch 343/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 5.1835e-04 - accuracy: 1.0000 - val_loss: 0.0622 - val_accuracy: 0.9824\n",
            "Epoch 344/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 6.7149e-04 - accuracy: 1.0000 - val_loss: 0.0618 - val_accuracy: 0.9805\n",
            "Epoch 345/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0036 - accuracy: 0.9991 - val_loss: 0.0507 - val_accuracy: 0.9814\n",
            "Epoch 346/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 5.9606e-04 - accuracy: 1.0000 - val_loss: 0.3449 - val_accuracy: 0.9189\n",
            "Epoch 347/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 9.7561e-04 - accuracy: 1.0000 - val_loss: 0.0571 - val_accuracy: 0.9844\n",
            "Epoch 348/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 4.6910e-04 - accuracy: 1.0000 - val_loss: 0.0525 - val_accuracy: 0.9854\n",
            "Epoch 349/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 3.8537e-04 - accuracy: 1.0000 - val_loss: 0.0551 - val_accuracy: 0.9854\n",
            "Epoch 350/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 3.8461e-04 - accuracy: 1.0000 - val_loss: 0.0589 - val_accuracy: 0.9834\n",
            "Epoch 351/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 6.1752e-04 - accuracy: 1.0000 - val_loss: 0.0617 - val_accuracy: 0.9824\n",
            "Epoch 352/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 3.6954e-04 - accuracy: 1.0000 - val_loss: 0.0610 - val_accuracy: 0.9814\n",
            "Epoch 353/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 4.0755e-04 - accuracy: 1.0000 - val_loss: 0.0595 - val_accuracy: 0.9824\n",
            "Epoch 354/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 3.8428e-04 - accuracy: 1.0000 - val_loss: 0.0572 - val_accuracy: 0.9834\n",
            "Epoch 355/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 3.7918e-04 - accuracy: 1.0000 - val_loss: 0.0569 - val_accuracy: 0.9834\n",
            "Epoch 356/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 3.3666e-04 - accuracy: 1.0000 - val_loss: 0.0549 - val_accuracy: 0.9854\n",
            "Epoch 357/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 4.5647e-04 - accuracy: 1.0000 - val_loss: 0.0615 - val_accuracy: 0.9824\n",
            "Epoch 358/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 4.8859e-04 - accuracy: 1.0000 - val_loss: 0.0577 - val_accuracy: 0.9834\n",
            "Epoch 359/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 4.2924e-04 - accuracy: 1.0000 - val_loss: 0.0647 - val_accuracy: 0.9814\n",
            "Epoch 360/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 3.1877e-04 - accuracy: 1.0000 - val_loss: 0.0589 - val_accuracy: 0.9844\n",
            "Epoch 361/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 3.1538e-04 - accuracy: 1.0000 - val_loss: 0.0603 - val_accuracy: 0.9824\n",
            "Epoch 362/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 3.0348e-04 - accuracy: 1.0000 - val_loss: 0.0565 - val_accuracy: 0.9844\n",
            "Epoch 363/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 4.4057e-04 - accuracy: 1.0000 - val_loss: 0.0591 - val_accuracy: 0.9834\n",
            "Epoch 364/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 4.9555e-04 - accuracy: 1.0000 - val_loss: 0.0610 - val_accuracy: 0.9814\n",
            "Epoch 365/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 2.9055e-04 - accuracy: 1.0000 - val_loss: 0.0627 - val_accuracy: 0.9814\n",
            "Epoch 366/500\n",
            "128/128 [==============================] - 2s 14ms/step - loss: 4.2952e-04 - accuracy: 1.0000 - val_loss: 0.0590 - val_accuracy: 0.9844\n",
            "Epoch 367/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 2.7963e-04 - accuracy: 1.0000 - val_loss: 0.0670 - val_accuracy: 0.9805\n",
            "Epoch 368/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 3.2082e-04 - accuracy: 1.0000 - val_loss: 0.0608 - val_accuracy: 0.9824\n",
            "Epoch 369/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 3.6724e-04 - accuracy: 1.0000 - val_loss: 0.0591 - val_accuracy: 0.9834\n",
            "Epoch 370/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 3.1986e-04 - accuracy: 1.0000 - val_loss: 0.0581 - val_accuracy: 0.9834\n",
            "Epoch 371/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.0605 - val_accuracy: 0.9824\n",
            "Epoch 372/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 2.6785e-04 - accuracy: 1.0000 - val_loss: 0.0551 - val_accuracy: 0.9844\n",
            "Epoch 373/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 4.4000e-04 - accuracy: 0.9999 - val_loss: 0.0629 - val_accuracy: 0.9824\n",
            "Epoch 374/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 5.1391e-04 - accuracy: 1.0000 - val_loss: 0.0599 - val_accuracy: 0.9824\n",
            "Epoch 375/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 2.6798e-04 - accuracy: 1.0000 - val_loss: 0.0540 - val_accuracy: 0.9834\n",
            "Epoch 376/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 3.0108e-04 - accuracy: 1.0000 - val_loss: 0.0592 - val_accuracy: 0.9805\n",
            "Epoch 377/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 2.9292e-04 - accuracy: 1.0000 - val_loss: 0.0647 - val_accuracy: 0.9814\n",
            "Epoch 378/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 3.2580e-04 - accuracy: 1.0000 - val_loss: 0.0589 - val_accuracy: 0.9824\n",
            "Epoch 379/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.3261 - val_accuracy: 0.9375\n",
            "Epoch 380/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0185 - accuracy: 0.9977 - val_loss: 0.0520 - val_accuracy: 0.9844\n",
            "Epoch 381/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 4.0940e-04 - accuracy: 1.0000 - val_loss: 0.0516 - val_accuracy: 0.9844\n",
            "Epoch 382/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 4.4980e-04 - accuracy: 1.0000 - val_loss: 0.0609 - val_accuracy: 0.9824\n",
            "Epoch 383/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.1486 - val_accuracy: 0.9297\n",
            "Epoch 384/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.2566 - val_accuracy: 0.9121\n",
            "Epoch 385/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 6.8516e-04 - accuracy: 1.0000 - val_loss: 0.0669 - val_accuracy: 0.9814\n",
            "Epoch 386/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 6.0869e-04 - accuracy: 1.0000 - val_loss: 0.0498 - val_accuracy: 0.9854\n",
            "Epoch 387/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 5.9291e-04 - accuracy: 1.0000 - val_loss: 0.0563 - val_accuracy: 0.9834\n",
            "Epoch 388/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 3.2356e-04 - accuracy: 1.0000 - val_loss: 0.0510 - val_accuracy: 0.9834\n",
            "Epoch 389/500\n",
            "128/128 [==============================] - 2s 14ms/step - loss: 7.6071e-04 - accuracy: 1.0000 - val_loss: 0.0620 - val_accuracy: 0.9805\n",
            "Epoch 390/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 4.2633e-04 - accuracy: 1.0000 - val_loss: 0.0556 - val_accuracy: 0.9834\n",
            "Epoch 391/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 3.5083e-04 - accuracy: 1.0000 - val_loss: 0.0526 - val_accuracy: 0.9834\n",
            "Epoch 392/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 3.6454e-04 - accuracy: 1.0000 - val_loss: 0.0534 - val_accuracy: 0.9814\n",
            "Epoch 393/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 2.5078e-04 - accuracy: 1.0000 - val_loss: 0.0576 - val_accuracy: 0.9805\n",
            "Epoch 394/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 5.3048e-04 - accuracy: 1.0000 - val_loss: 0.0500 - val_accuracy: 0.9834\n",
            "Epoch 395/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 2.3174e-04 - accuracy: 1.0000 - val_loss: 0.0571 - val_accuracy: 0.9834\n",
            "Epoch 396/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 2.6670e-04 - accuracy: 1.0000 - val_loss: 0.0525 - val_accuracy: 0.9844\n",
            "Epoch 397/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 2.4341e-04 - accuracy: 1.0000 - val_loss: 0.0607 - val_accuracy: 0.9805\n",
            "Epoch 398/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 3.5605e-04 - accuracy: 1.0000 - val_loss: 0.0549 - val_accuracy: 0.9834\n",
            "Epoch 399/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 5.5126e-04 - accuracy: 1.0000 - val_loss: 0.0529 - val_accuracy: 0.9824\n",
            "Epoch 400/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 2.2504e-04 - accuracy: 1.0000 - val_loss: 0.0563 - val_accuracy: 0.9834\n",
            "Epoch 401/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 1.7953e-04 - accuracy: 1.0000 - val_loss: 0.0549 - val_accuracy: 0.9834\n",
            "Epoch 402/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 2.3821e-04 - accuracy: 1.0000 - val_loss: 0.0585 - val_accuracy: 0.9834\n",
            "Epoch 403/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 1.8630e-04 - accuracy: 1.0000 - val_loss: 0.0620 - val_accuracy: 0.9805\n",
            "Epoch 404/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 4.0278e-04 - accuracy: 0.9999 - val_loss: 0.0947 - val_accuracy: 0.9756\n",
            "Epoch 405/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 3.7194e-04 - accuracy: 1.0000 - val_loss: 0.0509 - val_accuracy: 0.9854\n",
            "Epoch 406/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 3.0478e-04 - accuracy: 1.0000 - val_loss: 0.0545 - val_accuracy: 0.9834\n",
            "Epoch 407/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 2.1299e-04 - accuracy: 1.0000 - val_loss: 0.0515 - val_accuracy: 0.9854\n",
            "Epoch 408/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 4.4009e-04 - accuracy: 1.0000 - val_loss: 0.0479 - val_accuracy: 0.9844\n",
            "Epoch 409/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 2.9053e-04 - accuracy: 1.0000 - val_loss: 0.1256 - val_accuracy: 0.9746\n",
            "Epoch 410/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 4.3077e-04 - accuracy: 1.0000 - val_loss: 0.0531 - val_accuracy: 0.9824\n",
            "Epoch 411/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 2.5776e-04 - accuracy: 1.0000 - val_loss: 0.0583 - val_accuracy: 0.9824\n",
            "Epoch 412/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 1.9781e-04 - accuracy: 1.0000 - val_loss: 0.0596 - val_accuracy: 0.9824\n",
            "Epoch 413/500\n",
            "128/128 [==============================] - 2s 14ms/step - loss: 2.8810e-04 - accuracy: 1.0000 - val_loss: 0.0488 - val_accuracy: 0.9863\n",
            "Epoch 414/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 4.5292e-04 - accuracy: 1.0000 - val_loss: 0.0561 - val_accuracy: 0.9824\n",
            "Epoch 415/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 2.7584e-04 - accuracy: 1.0000 - val_loss: 0.0551 - val_accuracy: 0.9844\n",
            "Epoch 416/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 2.4675e-04 - accuracy: 1.0000 - val_loss: 0.0532 - val_accuracy: 0.9844\n",
            "Epoch 417/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 1.6095e-04 - accuracy: 1.0000 - val_loss: 0.0576 - val_accuracy: 0.9834\n",
            "Epoch 418/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 2.5386e-04 - accuracy: 1.0000 - val_loss: 0.0528 - val_accuracy: 0.9834\n",
            "Epoch 419/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 4.3448e-04 - accuracy: 1.0000 - val_loss: 0.0652 - val_accuracy: 0.9814\n",
            "Epoch 420/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 1.9280e-04 - accuracy: 1.0000 - val_loss: 0.0572 - val_accuracy: 0.9824\n",
            "Epoch 421/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 2.0557e-04 - accuracy: 1.0000 - val_loss: 0.0592 - val_accuracy: 0.9824\n",
            "Epoch 422/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 1.7286e-04 - accuracy: 1.0000 - val_loss: 0.0599 - val_accuracy: 0.9834\n",
            "Epoch 423/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 2.9453e-04 - accuracy: 1.0000 - val_loss: 0.5636 - val_accuracy: 0.8496\n",
            "Epoch 424/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0054 - accuracy: 0.9981 - val_loss: 0.3553 - val_accuracy: 0.9287\n",
            "Epoch 425/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0232 - accuracy: 0.9946 - val_loss: 0.1409 - val_accuracy: 0.9658\n",
            "Epoch 426/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0026 - accuracy: 0.9998 - val_loss: 0.3201 - val_accuracy: 0.9004\n",
            "Epoch 427/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0032 - accuracy: 0.9986 - val_loss: 0.0903 - val_accuracy: 0.9756\n",
            "Epoch 428/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0475 - val_accuracy: 0.9814\n",
            "Epoch 429/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0098 - accuracy: 0.9971 - val_loss: 0.0549 - val_accuracy: 0.9814\n",
            "Epoch 430/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0113 - accuracy: 0.9968 - val_loss: 0.3322 - val_accuracy: 0.8994\n",
            "Epoch 431/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0487 - val_accuracy: 0.9854\n",
            "Epoch 432/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 5.9137e-04 - accuracy: 1.0000 - val_loss: 0.0475 - val_accuracy: 0.9834\n",
            "Epoch 433/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 6.2404e-04 - accuracy: 1.0000 - val_loss: 0.0503 - val_accuracy: 0.9824\n",
            "Epoch 434/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 3.4799e-04 - accuracy: 1.0000 - val_loss: 0.0479 - val_accuracy: 0.9854\n",
            "Epoch 435/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 5.9479e-04 - accuracy: 1.0000 - val_loss: 0.0363 - val_accuracy: 0.9883\n",
            "Epoch 436/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 4.2317e-04 - accuracy: 1.0000 - val_loss: 0.0439 - val_accuracy: 0.9854\n",
            "Epoch 437/500\n",
            "128/128 [==============================] - 2s 14ms/step - loss: 2.7746e-04 - accuracy: 1.0000 - val_loss: 0.0447 - val_accuracy: 0.9863\n",
            "Epoch 438/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 2.7460e-04 - accuracy: 1.0000 - val_loss: 0.0487 - val_accuracy: 0.9863\n",
            "Epoch 439/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 5.8536e-04 - accuracy: 1.0000 - val_loss: 0.0457 - val_accuracy: 0.9854\n",
            "Epoch 440/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 3.5953e-04 - accuracy: 1.0000 - val_loss: 0.0443 - val_accuracy: 0.9883\n",
            "Epoch 441/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 2.5863e-04 - accuracy: 1.0000 - val_loss: 0.0425 - val_accuracy: 0.9883\n",
            "Epoch 442/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 2.8016e-04 - accuracy: 1.0000 - val_loss: 0.0482 - val_accuracy: 0.9844\n",
            "Epoch 443/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 2.1632e-04 - accuracy: 1.0000 - val_loss: 0.0532 - val_accuracy: 0.9834\n",
            "Epoch 444/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 2.1297e-04 - accuracy: 1.0000 - val_loss: 0.2401 - val_accuracy: 0.9189\n",
            "Epoch 445/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.0393 - val_accuracy: 0.9863\n",
            "Epoch 446/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 3.9792e-04 - accuracy: 1.0000 - val_loss: 0.0557 - val_accuracy: 0.9824\n",
            "Epoch 447/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 3.3509e-04 - accuracy: 1.0000 - val_loss: 0.0492 - val_accuracy: 0.9824\n",
            "Epoch 448/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 4.0926e-04 - accuracy: 1.0000 - val_loss: 0.0536 - val_accuracy: 0.9824\n",
            "Epoch 449/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 3.5679e-04 - accuracy: 1.0000 - val_loss: 0.0511 - val_accuracy: 0.9863\n",
            "Epoch 450/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 1.8178e-04 - accuracy: 1.0000 - val_loss: 0.0503 - val_accuracy: 0.9844\n",
            "Epoch 451/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 1.8928e-04 - accuracy: 1.0000 - val_loss: 0.0488 - val_accuracy: 0.9873\n",
            "Epoch 452/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 1.8551e-04 - accuracy: 1.0000 - val_loss: 0.0471 - val_accuracy: 0.9873\n",
            "Epoch 453/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 2.3114e-04 - accuracy: 1.0000 - val_loss: 0.0507 - val_accuracy: 0.9854\n",
            "Epoch 454/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 2.3975e-04 - accuracy: 1.0000 - val_loss: 0.0501 - val_accuracy: 0.9854\n",
            "Epoch 455/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 2.0415e-04 - accuracy: 1.0000 - val_loss: 0.0507 - val_accuracy: 0.9863\n",
            "Epoch 456/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0206 - accuracy: 0.9963 - val_loss: 11.8782 - val_accuracy: 0.3428\n",
            "Epoch 457/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0200 - accuracy: 0.9951 - val_loss: 0.1012 - val_accuracy: 0.9590\n",
            "Epoch 458/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0108 - accuracy: 0.9963 - val_loss: 0.0840 - val_accuracy: 0.9746\n",
            "Epoch 459/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0043 - accuracy: 0.9989 - val_loss: 0.0675 - val_accuracy: 0.9795\n",
            "Epoch 460/500\n",
            "128/128 [==============================] - 2s 14ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0521 - val_accuracy: 0.9834\n",
            "Epoch 461/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 9.9739e-04 - accuracy: 1.0000 - val_loss: 0.0861 - val_accuracy: 0.9727\n",
            "Epoch 462/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.1644 - val_accuracy: 0.9336\n",
            "Epoch 463/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0677 - val_accuracy: 0.9795\n",
            "Epoch 464/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0022 - accuracy: 0.9998 - val_loss: 0.1088 - val_accuracy: 0.9648\n",
            "Epoch 465/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0491 - val_accuracy: 0.9844\n",
            "Epoch 466/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 7.7730e-04 - accuracy: 1.0000 - val_loss: 0.0469 - val_accuracy: 0.9854\n",
            "Epoch 467/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.0705 - val_accuracy: 0.9795\n",
            "Epoch 468/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 9.3098e-04 - accuracy: 1.0000 - val_loss: 0.0531 - val_accuracy: 0.9834\n",
            "Epoch 469/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0187 - accuracy: 0.9971 - val_loss: 0.0767 - val_accuracy: 0.9717\n",
            "Epoch 470/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0934 - val_accuracy: 0.9756\n",
            "Epoch 471/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 7.0665e-04 - accuracy: 1.0000 - val_loss: 0.0551 - val_accuracy: 0.9824\n",
            "Epoch 472/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0734 - accuracy: 0.9795 - val_loss: 1.2158 - val_accuracy: 0.6846\n",
            "Epoch 473/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0937 - accuracy: 0.9758 - val_loss: 0.9484 - val_accuracy: 0.7207\n",
            "Epoch 474/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0141 - accuracy: 0.9956 - val_loss: 0.5987 - val_accuracy: 0.8418\n",
            "Epoch 475/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0080 - accuracy: 0.9978 - val_loss: 0.0892 - val_accuracy: 0.9746\n",
            "Epoch 476/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0038 - accuracy: 0.9989 - val_loss: 0.0830 - val_accuracy: 0.9688\n",
            "Epoch 477/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0500 - val_accuracy: 0.9854\n",
            "Epoch 478/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 0.0503 - val_accuracy: 0.9854\n",
            "Epoch 479/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0096 - accuracy: 0.9975 - val_loss: 0.0987 - val_accuracy: 0.9727\n",
            "Epoch 480/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0063 - accuracy: 0.9990 - val_loss: 0.0686 - val_accuracy: 0.9795\n",
            "Epoch 481/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0080 - accuracy: 0.9982 - val_loss: 0.0666 - val_accuracy: 0.9775\n",
            "Epoch 482/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0458 - val_accuracy: 0.9854\n",
            "Epoch 483/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0484 - val_accuracy: 0.9844\n",
            "Epoch 484/500\n",
            "128/128 [==============================] - 2s 14ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0631 - val_accuracy: 0.9805\n",
            "Epoch 485/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0522 - val_accuracy: 0.9814\n",
            "Epoch 486/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0496 - val_accuracy: 0.9863\n",
            "Epoch 487/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0030 - accuracy: 0.9990 - val_loss: 0.0951 - val_accuracy: 0.9688\n",
            "Epoch 488/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0516 - val_accuracy: 0.9834\n",
            "Epoch 489/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 7.5407e-04 - accuracy: 1.0000 - val_loss: 0.0463 - val_accuracy: 0.9863\n",
            "Epoch 490/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 5.6735e-04 - accuracy: 1.0000 - val_loss: 0.0509 - val_accuracy: 0.9854\n",
            "Epoch 491/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 6.0368e-04 - accuracy: 1.0000 - val_loss: 0.0504 - val_accuracy: 0.9863\n",
            "Epoch 492/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 4.9228e-04 - accuracy: 1.0000 - val_loss: 0.0482 - val_accuracy: 0.9863\n",
            "Epoch 493/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 5.5115e-04 - accuracy: 1.0000 - val_loss: 0.0533 - val_accuracy: 0.9844\n",
            "Epoch 494/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 6.7866e-04 - accuracy: 1.0000 - val_loss: 0.0583 - val_accuracy: 0.9824\n",
            "Epoch 495/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0513 - val_accuracy: 0.9844\n",
            "Epoch 496/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 4.8413e-04 - accuracy: 1.0000 - val_loss: 0.0529 - val_accuracy: 0.9834\n",
            "Epoch 497/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 5.8055e-04 - accuracy: 1.0000 - val_loss: 0.0570 - val_accuracy: 0.9863\n",
            "Epoch 498/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 5.5846e-04 - accuracy: 1.0000 - val_loss: 0.0484 - val_accuracy: 0.9854\n",
            "Epoch 499/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 5.3402e-04 - accuracy: 1.0000 - val_loss: 0.0427 - val_accuracy: 0.9854\n",
            "Epoch 500/500\n",
            "128/128 [==============================] - 2s 13ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.0605 - val_accuracy: 0.9795\n",
            "\n",
            "Accuracy: 97.95%\n",
            "Loss: 0.06050872802734375 \n",
            "\n",
            "NEXT SET OF HYPERPARAMETERS IS: \n",
            " num_conv_layers: 4 \n",
            " num_conv_nodes: 106 \n",
            " num_dense_layers: 1 \n",
            " num_dense_nodes: 35 \n",
            "\n",
            "Epoch 1/500\n",
            "128/128 [==============================] - 2s 9ms/step - loss: 1.5101 - accuracy: 0.4689 - val_loss: 2.6958 - val_accuracy: 0.0703\n",
            "Epoch 2/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.8594 - accuracy: 0.6258 - val_loss: 2.6106 - val_accuracy: 0.1455\n",
            "Epoch 3/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.8082 - accuracy: 0.6427 - val_loss: 2.1017 - val_accuracy: 0.2012\n",
            "Epoch 4/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7664 - accuracy: 0.6757 - val_loss: 1.5673 - val_accuracy: 0.3955\n",
            "Epoch 5/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6697 - accuracy: 0.7399 - val_loss: 1.3645 - val_accuracy: 0.4355\n",
            "Epoch 6/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6856 - accuracy: 0.7105 - val_loss: 0.9108 - val_accuracy: 0.6094\n",
            "Epoch 7/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6390 - accuracy: 0.7449 - val_loss: 0.8033 - val_accuracy: 0.6221\n",
            "Epoch 8/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6426 - accuracy: 0.7300 - val_loss: 1.3015 - val_accuracy: 0.6572\n",
            "Epoch 9/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4212 - accuracy: 0.8419 - val_loss: 2.5194 - val_accuracy: 0.5352\n",
            "Epoch 10/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6282 - accuracy: 0.7361 - val_loss: 1.9326 - val_accuracy: 0.4980\n",
            "Epoch 11/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7057 - accuracy: 0.7102 - val_loss: 1.2468 - val_accuracy: 0.4268\n",
            "Epoch 12/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6209 - accuracy: 0.7313 - val_loss: 1.4482 - val_accuracy: 0.4658\n",
            "Epoch 13/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4495 - accuracy: 0.8363 - val_loss: 1.2477 - val_accuracy: 0.4785\n",
            "Epoch 14/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6944 - accuracy: 0.7009 - val_loss: 1.9166 - val_accuracy: 0.3066\n",
            "Epoch 15/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4132 - accuracy: 0.8330 - val_loss: 0.8417 - val_accuracy: 0.5928\n",
            "Epoch 16/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.3187 - accuracy: 0.8782 - val_loss: 1.5262 - val_accuracy: 0.5420\n",
            "Epoch 17/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.2925 - accuracy: 0.8847 - val_loss: 0.6548 - val_accuracy: 0.6895\n",
            "Epoch 18/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.3143 - accuracy: 0.8817 - val_loss: 0.3999 - val_accuracy: 0.8643\n",
            "Epoch 19/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.2559 - accuracy: 0.8959 - val_loss: 0.5906 - val_accuracy: 0.8008\n",
            "Epoch 20/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.2263 - accuracy: 0.9112 - val_loss: 1.9513 - val_accuracy: 0.6494\n",
            "Epoch 21/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.2562 - accuracy: 0.9020 - val_loss: 1.9300 - val_accuracy: 0.3633\n",
            "Epoch 22/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.3791 - accuracy: 0.8556 - val_loss: 92.4603 - val_accuracy: 0.0293\n",
            "Epoch 23/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.3675 - accuracy: 0.8586 - val_loss: 4.7830 - val_accuracy: 0.4189\n",
            "Epoch 24/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4726 - accuracy: 0.8402 - val_loss: 1.1388 - val_accuracy: 0.6094\n",
            "Epoch 25/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6102 - accuracy: 0.7547 - val_loss: 1.8078 - val_accuracy: 0.4678\n",
            "Epoch 26/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5412 - accuracy: 0.7891 - val_loss: 1.4300 - val_accuracy: 0.4375\n",
            "Epoch 27/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.3940 - accuracy: 0.8326 - val_loss: 27.7252 - val_accuracy: 0.0703\n",
            "Epoch 28/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4704 - accuracy: 0.8184 - val_loss: 10.4123 - val_accuracy: 0.1230\n",
            "Epoch 29/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.3734 - accuracy: 0.8489 - val_loss: 2.8456 - val_accuracy: 0.3486\n",
            "Epoch 30/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4041 - accuracy: 0.8331 - val_loss: 2.9417 - val_accuracy: 0.4434\n",
            "Epoch 31/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4625 - accuracy: 0.8158 - val_loss: 0.9303 - val_accuracy: 0.6123\n",
            "Epoch 32/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.3333 - accuracy: 0.8745 - val_loss: 1.6661 - val_accuracy: 0.5625\n",
            "Epoch 33/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.3559 - accuracy: 0.8656 - val_loss: 0.5853 - val_accuracy: 0.7598\n",
            "Epoch 34/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.2965 - accuracy: 0.8888 - val_loss: 1.6945 - val_accuracy: 0.5674\n",
            "Epoch 35/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.2677 - accuracy: 0.8934 - val_loss: 15.1713 - val_accuracy: 0.1230\n",
            "Epoch 36/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.2344 - accuracy: 0.9020 - val_loss: 10.5536 - val_accuracy: 0.1152\n",
            "Epoch 37/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.3176 - accuracy: 0.8822 - val_loss: 9.4194 - val_accuracy: 0.2031\n",
            "Epoch 38/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.3718 - accuracy: 0.8505 - val_loss: 0.3771 - val_accuracy: 0.9023\n",
            "Epoch 39/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.2801 - accuracy: 0.9035 - val_loss: 1.0870 - val_accuracy: 0.6719\n",
            "Epoch 40/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.2509 - accuracy: 0.9030 - val_loss: 0.4414 - val_accuracy: 0.7920\n",
            "Epoch 41/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.2437 - accuracy: 0.9021 - val_loss: 0.2953 - val_accuracy: 0.8936\n",
            "Epoch 42/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.2246 - accuracy: 0.9061 - val_loss: 0.3912 - val_accuracy: 0.8496\n",
            "Epoch 43/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.2489 - accuracy: 0.8945 - val_loss: 0.4566 - val_accuracy: 0.8281\n",
            "Epoch 44/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.2511 - accuracy: 0.8938 - val_loss: 19.5628 - val_accuracy: 0.1406\n",
            "Epoch 45/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.2346 - accuracy: 0.9059 - val_loss: 1.0156 - val_accuracy: 0.5879\n",
            "Epoch 46/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.2436 - accuracy: 0.9006 - val_loss: 0.3868 - val_accuracy: 0.8369\n",
            "Epoch 47/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.2140 - accuracy: 0.9071 - val_loss: 0.3424 - val_accuracy: 0.8623\n",
            "Epoch 48/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.2273 - accuracy: 0.9024 - val_loss: 0.4960 - val_accuracy: 0.7969\n",
            "Epoch 49/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.2708 - accuracy: 0.8928 - val_loss: 2.2650 - val_accuracy: 0.4434\n",
            "Epoch 50/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.2362 - accuracy: 0.9052 - val_loss: 1.0976 - val_accuracy: 0.6504\n",
            "Epoch 51/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.2451 - accuracy: 0.8992 - val_loss: 0.7755 - val_accuracy: 0.6729\n",
            "Epoch 52/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.2418 - accuracy: 0.8991 - val_loss: 2.3961 - val_accuracy: 0.7041\n",
            "Epoch 53/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5361 - accuracy: 0.7975 - val_loss: 1.0626 - val_accuracy: 0.6045\n",
            "Epoch 54/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.2511 - accuracy: 0.9025 - val_loss: 3.4192 - val_accuracy: 0.2598\n",
            "Epoch 55/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.3031 - accuracy: 0.8815 - val_loss: 14.0436 - val_accuracy: 0.2207\n",
            "Epoch 56/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.2462 - accuracy: 0.9080 - val_loss: 0.6316 - val_accuracy: 0.7285\n",
            "Epoch 57/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.2509 - accuracy: 0.8937 - val_loss: 5.2224 - val_accuracy: 0.2246\n",
            "Epoch 58/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.2324 - accuracy: 0.9017 - val_loss: 3.6422 - val_accuracy: 0.2607\n",
            "Epoch 59/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.2348 - accuracy: 0.8982 - val_loss: 1.5218 - val_accuracy: 0.6592\n",
            "Epoch 60/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.2743 - accuracy: 0.9050 - val_loss: 1.0924 - val_accuracy: 0.6504\n",
            "Epoch 61/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.3443 - accuracy: 0.8572 - val_loss: 1.2866 - val_accuracy: 0.5039\n",
            "Epoch 62/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.2113 - accuracy: 0.9189 - val_loss: 0.2749 - val_accuracy: 0.9053\n",
            "Epoch 63/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.2165 - accuracy: 0.9052 - val_loss: 0.4544 - val_accuracy: 0.8115\n",
            "Epoch 64/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1965 - accuracy: 0.9149 - val_loss: 0.6154 - val_accuracy: 0.7842\n",
            "Epoch 65/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1877 - accuracy: 0.9219 - val_loss: 0.3479 - val_accuracy: 0.8398\n",
            "Epoch 66/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1960 - accuracy: 0.9156 - val_loss: 0.2512 - val_accuracy: 0.9014\n",
            "Epoch 67/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.1800 - accuracy: 0.9225 - val_loss: 0.2579 - val_accuracy: 0.8838\n",
            "Epoch 68/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.2113 - accuracy: 0.9064 - val_loss: 0.2341 - val_accuracy: 0.9180\n",
            "Epoch 69/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1785 - accuracy: 0.9211 - val_loss: 0.3538 - val_accuracy: 0.8535\n",
            "Epoch 70/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1897 - accuracy: 0.9056 - val_loss: 0.6752 - val_accuracy: 0.8037\n",
            "Epoch 71/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1823 - accuracy: 0.9207 - val_loss: 0.2121 - val_accuracy: 0.9229\n",
            "Epoch 72/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1739 - accuracy: 0.9207 - val_loss: 0.2725 - val_accuracy: 0.8848\n",
            "Epoch 73/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1873 - accuracy: 0.9154 - val_loss: 0.7293 - val_accuracy: 0.7832\n",
            "Epoch 74/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1767 - accuracy: 0.9266 - val_loss: 0.7880 - val_accuracy: 0.7666\n",
            "Epoch 75/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1740 - accuracy: 0.9204 - val_loss: 0.6887 - val_accuracy: 0.7725\n",
            "Epoch 76/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1633 - accuracy: 0.9365 - val_loss: 0.5760 - val_accuracy: 0.8242\n",
            "Epoch 77/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1785 - accuracy: 0.9224 - val_loss: 0.2815 - val_accuracy: 0.8926\n",
            "Epoch 78/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1862 - accuracy: 0.9172 - val_loss: 0.2330 - val_accuracy: 0.9170\n",
            "Epoch 79/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1735 - accuracy: 0.9264 - val_loss: 0.6693 - val_accuracy: 0.8174\n",
            "Epoch 80/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1735 - accuracy: 0.9226 - val_loss: 0.2448 - val_accuracy: 0.8779\n",
            "Epoch 81/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1769 - accuracy: 0.9168 - val_loss: 3.3747 - val_accuracy: 0.7607\n",
            "Epoch 82/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1834 - accuracy: 0.9226 - val_loss: 0.1830 - val_accuracy: 0.9316\n",
            "Epoch 83/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1716 - accuracy: 0.9295 - val_loss: 0.2372 - val_accuracy: 0.8896\n",
            "Epoch 84/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1757 - accuracy: 0.9272 - val_loss: 0.1887 - val_accuracy: 0.9199\n",
            "Epoch 85/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1798 - accuracy: 0.9194 - val_loss: 0.2748 - val_accuracy: 0.8848\n",
            "Epoch 86/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.1750 - accuracy: 0.9300 - val_loss: 0.2265 - val_accuracy: 0.9189\n",
            "Epoch 87/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1841 - accuracy: 0.9200 - val_loss: 0.1858 - val_accuracy: 0.9326\n",
            "Epoch 88/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1655 - accuracy: 0.9279 - val_loss: 0.2394 - val_accuracy: 0.9023\n",
            "Epoch 89/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1563 - accuracy: 0.9295 - val_loss: 0.1700 - val_accuracy: 0.9346\n",
            "Epoch 90/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1447 - accuracy: 0.9377 - val_loss: 0.7009 - val_accuracy: 0.8027\n",
            "Epoch 91/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.1551 - accuracy: 0.9337 - val_loss: 0.1588 - val_accuracy: 0.9473\n",
            "Epoch 92/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1557 - accuracy: 0.9303 - val_loss: 0.2943 - val_accuracy: 0.8721\n",
            "Epoch 93/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1588 - accuracy: 0.9365 - val_loss: 0.2890 - val_accuracy: 0.8789\n",
            "Epoch 94/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1545 - accuracy: 0.9352 - val_loss: 0.1575 - val_accuracy: 0.9463\n",
            "Epoch 95/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1672 - accuracy: 0.9293 - val_loss: 0.2448 - val_accuracy: 0.8955\n",
            "Epoch 96/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1489 - accuracy: 0.9397 - val_loss: 0.3215 - val_accuracy: 0.8682\n",
            "Epoch 97/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1600 - accuracy: 0.9278 - val_loss: 0.2481 - val_accuracy: 0.8818\n",
            "Epoch 98/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1477 - accuracy: 0.9402 - val_loss: 0.2473 - val_accuracy: 0.9141\n",
            "Epoch 99/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1538 - accuracy: 0.9381 - val_loss: 0.4103 - val_accuracy: 0.8252\n",
            "Epoch 100/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1393 - accuracy: 0.9443 - val_loss: 0.4362 - val_accuracy: 0.8154\n",
            "Epoch 101/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1543 - accuracy: 0.9415 - val_loss: 0.2096 - val_accuracy: 0.9131\n",
            "Epoch 102/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1407 - accuracy: 0.9416 - val_loss: 0.1539 - val_accuracy: 0.9424\n",
            "Epoch 103/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1370 - accuracy: 0.9457 - val_loss: 0.1929 - val_accuracy: 0.9150\n",
            "Epoch 104/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1386 - accuracy: 0.9440 - val_loss: 0.1697 - val_accuracy: 0.9355\n",
            "Epoch 105/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1272 - accuracy: 0.9472 - val_loss: 0.2130 - val_accuracy: 0.9268\n",
            "Epoch 106/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1237 - accuracy: 0.9555 - val_loss: 0.1962 - val_accuracy: 0.9072\n",
            "Epoch 107/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1149 - accuracy: 0.9576 - val_loss: 0.1752 - val_accuracy: 0.9307\n",
            "Epoch 108/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1174 - accuracy: 0.9535 - val_loss: 0.1880 - val_accuracy: 0.9248\n",
            "Epoch 109/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1302 - accuracy: 0.9442 - val_loss: 0.2305 - val_accuracy: 0.9131\n",
            "Epoch 110/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1320 - accuracy: 0.9517 - val_loss: 0.6948 - val_accuracy: 0.8174\n",
            "Epoch 111/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1231 - accuracy: 0.9528 - val_loss: 0.1348 - val_accuracy: 0.9453\n",
            "Epoch 112/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1136 - accuracy: 0.9545 - val_loss: 0.1487 - val_accuracy: 0.9453\n",
            "Epoch 113/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1205 - accuracy: 0.9511 - val_loss: 0.2124 - val_accuracy: 0.9141\n",
            "Epoch 114/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1291 - accuracy: 0.9499 - val_loss: 1.0164 - val_accuracy: 0.7656\n",
            "Epoch 115/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.1176 - accuracy: 0.9534 - val_loss: 0.8635 - val_accuracy: 0.7480\n",
            "Epoch 116/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1193 - accuracy: 0.9532 - val_loss: 0.1739 - val_accuracy: 0.9307\n",
            "Epoch 117/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.1060 - accuracy: 0.9597 - val_loss: 0.2452 - val_accuracy: 0.9229\n",
            "Epoch 118/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0999 - accuracy: 0.9575 - val_loss: 0.1303 - val_accuracy: 0.9463\n",
            "Epoch 119/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.1029 - accuracy: 0.9658 - val_loss: 0.1439 - val_accuracy: 0.9482\n",
            "Epoch 120/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1060 - accuracy: 0.9594 - val_loss: 0.1517 - val_accuracy: 0.9453\n",
            "Epoch 121/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1008 - accuracy: 0.9624 - val_loss: 0.3474 - val_accuracy: 0.8545\n",
            "Epoch 122/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1104 - accuracy: 0.9577 - val_loss: 0.2199 - val_accuracy: 0.9111\n",
            "Epoch 123/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0996 - accuracy: 0.9626 - val_loss: 0.6823 - val_accuracy: 0.7988\n",
            "Epoch 124/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1014 - accuracy: 0.9636 - val_loss: 0.2348 - val_accuracy: 0.9102\n",
            "Epoch 125/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0928 - accuracy: 0.9663 - val_loss: 0.3907 - val_accuracy: 0.8252\n",
            "Epoch 126/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0943 - accuracy: 0.9663 - val_loss: 0.3443 - val_accuracy: 0.8809\n",
            "Epoch 127/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0819 - accuracy: 0.9724 - val_loss: 0.1616 - val_accuracy: 0.9395\n",
            "Epoch 128/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0853 - accuracy: 0.9715 - val_loss: 0.3242 - val_accuracy: 0.8779\n",
            "Epoch 129/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0954 - accuracy: 0.9632 - val_loss: 0.1786 - val_accuracy: 0.9297\n",
            "Epoch 130/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0760 - accuracy: 0.9737 - val_loss: 0.1503 - val_accuracy: 0.9473\n",
            "Epoch 131/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0875 - accuracy: 0.9669 - val_loss: 0.1270 - val_accuracy: 0.9521\n",
            "Epoch 132/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1024 - accuracy: 0.9612 - val_loss: 0.1073 - val_accuracy: 0.9600\n",
            "Epoch 133/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0852 - accuracy: 0.9684 - val_loss: 0.6090 - val_accuracy: 0.8008\n",
            "Epoch 134/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0848 - accuracy: 0.9687 - val_loss: 0.2553 - val_accuracy: 0.8887\n",
            "Epoch 135/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0904 - accuracy: 0.9664 - val_loss: 0.1098 - val_accuracy: 0.9580\n",
            "Epoch 136/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0799 - accuracy: 0.9726 - val_loss: 7.2933 - val_accuracy: 0.7344\n",
            "Epoch 137/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0791 - accuracy: 0.9746 - val_loss: 0.8525 - val_accuracy: 0.6836\n",
            "Epoch 138/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0819 - accuracy: 0.9742 - val_loss: 1.0765 - val_accuracy: 0.7471\n",
            "Epoch 139/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0885 - accuracy: 0.9678 - val_loss: 1.1038 - val_accuracy: 0.7803\n",
            "Epoch 140/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0846 - accuracy: 0.9681 - val_loss: 0.4082 - val_accuracy: 0.8535\n",
            "Epoch 141/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0940 - accuracy: 0.9657 - val_loss: 0.6843 - val_accuracy: 0.7979\n",
            "Epoch 142/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0716 - accuracy: 0.9742 - val_loss: 0.1131 - val_accuracy: 0.9648\n",
            "Epoch 143/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0641 - accuracy: 0.9803 - val_loss: 0.6410 - val_accuracy: 0.7998\n",
            "Epoch 144/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0702 - accuracy: 0.9767 - val_loss: 0.1726 - val_accuracy: 0.9365\n",
            "Epoch 145/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0722 - accuracy: 0.9695 - val_loss: 0.6389 - val_accuracy: 0.8447\n",
            "Epoch 146/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0666 - accuracy: 0.9742 - val_loss: 0.2544 - val_accuracy: 0.9102\n",
            "Epoch 147/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0616 - accuracy: 0.9804 - val_loss: 0.1284 - val_accuracy: 0.9541\n",
            "Epoch 148/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0670 - accuracy: 0.9763 - val_loss: 0.3852 - val_accuracy: 0.8398\n",
            "Epoch 149/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0592 - accuracy: 0.9801 - val_loss: 0.1083 - val_accuracy: 0.9590\n",
            "Epoch 150/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0555 - accuracy: 0.9804 - val_loss: 0.2901 - val_accuracy: 0.8955\n",
            "Epoch 151/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0561 - accuracy: 0.9796 - val_loss: 1.0486 - val_accuracy: 0.7520\n",
            "Epoch 152/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0653 - accuracy: 0.9730 - val_loss: 2.1820 - val_accuracy: 0.7012\n",
            "Epoch 153/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0627 - accuracy: 0.9820 - val_loss: 0.8665 - val_accuracy: 0.7812\n",
            "Epoch 154/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0574 - accuracy: 0.9799 - val_loss: 0.5043 - val_accuracy: 0.8193\n",
            "Epoch 155/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0580 - accuracy: 0.9820 - val_loss: 0.2575 - val_accuracy: 0.9014\n",
            "Epoch 156/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0482 - accuracy: 0.9841 - val_loss: 0.1381 - val_accuracy: 0.9502\n",
            "Epoch 157/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0547 - accuracy: 0.9806 - val_loss: 0.1774 - val_accuracy: 0.9297\n",
            "Epoch 158/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0508 - accuracy: 0.9808 - val_loss: 0.2542 - val_accuracy: 0.9150\n",
            "Epoch 159/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0596 - accuracy: 0.9797 - val_loss: 0.0987 - val_accuracy: 0.9580\n",
            "Epoch 160/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0505 - accuracy: 0.9830 - val_loss: 0.5565 - val_accuracy: 0.8320\n",
            "Epoch 161/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0726 - accuracy: 0.9730 - val_loss: 0.0859 - val_accuracy: 0.9639\n",
            "Epoch 162/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0445 - accuracy: 0.9855 - val_loss: 0.5232 - val_accuracy: 0.8359\n",
            "Epoch 163/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0418 - accuracy: 0.9871 - val_loss: 0.1670 - val_accuracy: 0.9385\n",
            "Epoch 164/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0496 - accuracy: 0.9803 - val_loss: 0.1325 - val_accuracy: 0.9482\n",
            "Epoch 165/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0524 - accuracy: 0.9798 - val_loss: 0.1019 - val_accuracy: 0.9639\n",
            "Epoch 166/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0370 - accuracy: 0.9883 - val_loss: 0.1362 - val_accuracy: 0.9482\n",
            "Epoch 167/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0430 - accuracy: 0.9868 - val_loss: 1.0958 - val_accuracy: 0.8369\n",
            "Epoch 168/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0529 - accuracy: 0.9804 - val_loss: 0.0895 - val_accuracy: 0.9678\n",
            "Epoch 169/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0475 - accuracy: 0.9859 - val_loss: 0.1625 - val_accuracy: 0.9375\n",
            "Epoch 170/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0561 - accuracy: 0.9812 - val_loss: 0.1560 - val_accuracy: 0.9443\n",
            "Epoch 171/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0808 - accuracy: 0.9744 - val_loss: 0.0953 - val_accuracy: 0.9648\n",
            "Epoch 172/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0454 - accuracy: 0.9835 - val_loss: 0.6249 - val_accuracy: 0.7891\n",
            "Epoch 173/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0535 - accuracy: 0.9814 - val_loss: 0.7374 - val_accuracy: 0.8037\n",
            "Epoch 174/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0538 - accuracy: 0.9812 - val_loss: 0.3116 - val_accuracy: 0.8682\n",
            "Epoch 175/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0365 - accuracy: 0.9885 - val_loss: 0.1764 - val_accuracy: 0.9160\n",
            "Epoch 176/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0420 - accuracy: 0.9829 - val_loss: 0.0936 - val_accuracy: 0.9629\n",
            "Epoch 177/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0425 - accuracy: 0.9834 - val_loss: 0.0952 - val_accuracy: 0.9639\n",
            "Epoch 178/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0381 - accuracy: 0.9869 - val_loss: 0.1409 - val_accuracy: 0.9316\n",
            "Epoch 179/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0442 - accuracy: 0.9861 - val_loss: 0.1661 - val_accuracy: 0.9375\n",
            "Epoch 180/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0383 - accuracy: 0.9886 - val_loss: 0.5338 - val_accuracy: 0.7959\n",
            "Epoch 181/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0446 - accuracy: 0.9843 - val_loss: 0.8595 - val_accuracy: 0.7852\n",
            "Epoch 182/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0340 - accuracy: 0.9885 - val_loss: 0.8373 - val_accuracy: 0.7920\n",
            "Epoch 183/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0443 - accuracy: 0.9841 - val_loss: 0.3395 - val_accuracy: 0.8477\n",
            "Epoch 184/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0347 - accuracy: 0.9894 - val_loss: 0.7869 - val_accuracy: 0.7891\n",
            "Epoch 185/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0352 - accuracy: 0.9876 - val_loss: 0.8501 - val_accuracy: 0.7363\n",
            "Epoch 186/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0317 - accuracy: 0.9885 - val_loss: 0.0988 - val_accuracy: 0.9668\n",
            "Epoch 187/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0334 - accuracy: 0.9878 - val_loss: 0.2539 - val_accuracy: 0.8994\n",
            "Epoch 188/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0326 - accuracy: 0.9886 - val_loss: 0.0944 - val_accuracy: 0.9629\n",
            "Epoch 189/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0292 - accuracy: 0.9889 - val_loss: 0.1719 - val_accuracy: 0.9453\n",
            "Epoch 190/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0381 - accuracy: 0.9844 - val_loss: 0.0908 - val_accuracy: 0.9658\n",
            "Epoch 191/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0410 - accuracy: 0.9826 - val_loss: 0.1108 - val_accuracy: 0.9551\n",
            "Epoch 192/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0333 - accuracy: 0.9876 - val_loss: 0.0776 - val_accuracy: 0.9678\n",
            "Epoch 193/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0288 - accuracy: 0.9892 - val_loss: 0.0859 - val_accuracy: 0.9697\n",
            "Epoch 194/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0264 - accuracy: 0.9919 - val_loss: 34.9277 - val_accuracy: 0.1191\n",
            "Epoch 195/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0459 - accuracy: 0.9852 - val_loss: 2.6423 - val_accuracy: 0.5654\n",
            "Epoch 196/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0384 - accuracy: 0.9886 - val_loss: 0.9818 - val_accuracy: 0.7783\n",
            "Epoch 197/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0490 - accuracy: 0.9816 - val_loss: 0.7405 - val_accuracy: 0.7930\n",
            "Epoch 198/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0500 - accuracy: 0.9834 - val_loss: 0.8993 - val_accuracy: 0.7900\n",
            "Epoch 199/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0342 - accuracy: 0.9899 - val_loss: 0.1908 - val_accuracy: 0.9307\n",
            "Epoch 200/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0289 - accuracy: 0.9919 - val_loss: 0.5004 - val_accuracy: 0.8213\n",
            "Epoch 201/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0328 - accuracy: 0.9881 - val_loss: 0.6731 - val_accuracy: 0.8203\n",
            "Epoch 202/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0285 - accuracy: 0.9918 - val_loss: 0.2269 - val_accuracy: 0.8926\n",
            "Epoch 203/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0344 - accuracy: 0.9882 - val_loss: 0.2218 - val_accuracy: 0.9141\n",
            "Epoch 204/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0294 - accuracy: 0.9906 - val_loss: 0.3226 - val_accuracy: 0.8545\n",
            "Epoch 205/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0286 - accuracy: 0.9916 - val_loss: 0.1016 - val_accuracy: 0.9668\n",
            "Epoch 206/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0299 - accuracy: 0.9901 - val_loss: 0.5327 - val_accuracy: 0.8320\n",
            "Epoch 207/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1130 - accuracy: 0.9608 - val_loss: 10.0642 - val_accuracy: 0.3271\n",
            "Epoch 208/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1491 - accuracy: 0.9482 - val_loss: 39.4426 - val_accuracy: 0.1709\n",
            "Epoch 209/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.1289 - accuracy: 0.9498 - val_loss: 0.9358 - val_accuracy: 0.7246\n",
            "Epoch 210/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0780 - accuracy: 0.9700 - val_loss: 0.7067 - val_accuracy: 0.7666\n",
            "Epoch 211/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0598 - accuracy: 0.9777 - val_loss: 0.4282 - val_accuracy: 0.8643\n",
            "Epoch 212/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0477 - accuracy: 0.9854 - val_loss: 0.1116 - val_accuracy: 0.9609\n",
            "Epoch 213/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0421 - accuracy: 0.9849 - val_loss: 1.9295 - val_accuracy: 0.5742\n",
            "Epoch 214/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0597 - accuracy: 0.9793 - val_loss: 2.2278 - val_accuracy: 0.6025\n",
            "Epoch 215/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0500 - accuracy: 0.9833 - val_loss: 0.4543 - val_accuracy: 0.8311\n",
            "Epoch 216/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0416 - accuracy: 0.9851 - val_loss: 0.1083 - val_accuracy: 0.9648\n",
            "Epoch 217/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0336 - accuracy: 0.9890 - val_loss: 0.2455 - val_accuracy: 0.9033\n",
            "Epoch 218/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0431 - accuracy: 0.9844 - val_loss: 0.1545 - val_accuracy: 0.9395\n",
            "Epoch 219/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0419 - accuracy: 0.9821 - val_loss: 0.1404 - val_accuracy: 0.9561\n",
            "Epoch 220/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0261 - accuracy: 0.9919 - val_loss: 0.1308 - val_accuracy: 0.9502\n",
            "Epoch 221/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0310 - accuracy: 0.9866 - val_loss: 0.0621 - val_accuracy: 0.9717\n",
            "Epoch 222/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0324 - accuracy: 0.9888 - val_loss: 1.0834 - val_accuracy: 0.8320\n",
            "Epoch 223/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0452 - accuracy: 0.9824 - val_loss: 0.3820 - val_accuracy: 0.8789\n",
            "Epoch 224/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0296 - accuracy: 0.9916 - val_loss: 0.1570 - val_accuracy: 0.9385\n",
            "Epoch 225/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0368 - accuracy: 0.9891 - val_loss: 0.1308 - val_accuracy: 0.9531\n",
            "Epoch 226/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0366 - accuracy: 0.9844 - val_loss: 0.0655 - val_accuracy: 0.9727\n",
            "Epoch 227/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0313 - accuracy: 0.9889 - val_loss: 0.0846 - val_accuracy: 0.9707\n",
            "Epoch 228/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0234 - accuracy: 0.9906 - val_loss: 0.1463 - val_accuracy: 0.9395\n",
            "Epoch 229/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0236 - accuracy: 0.9927 - val_loss: 0.1340 - val_accuracy: 0.9414\n",
            "Epoch 230/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0273 - accuracy: 0.9900 - val_loss: 0.0779 - val_accuracy: 0.9678\n",
            "Epoch 231/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0217 - accuracy: 0.9946 - val_loss: 0.1282 - val_accuracy: 0.9502\n",
            "Epoch 232/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0213 - accuracy: 0.9941 - val_loss: 0.0695 - val_accuracy: 0.9756\n",
            "Epoch 233/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0243 - accuracy: 0.9927 - val_loss: 0.0945 - val_accuracy: 0.9648\n",
            "Epoch 234/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0233 - accuracy: 0.9920 - val_loss: 0.1090 - val_accuracy: 0.9648\n",
            "Epoch 235/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0214 - accuracy: 0.9918 - val_loss: 0.0621 - val_accuracy: 0.9727\n",
            "Epoch 236/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0251 - accuracy: 0.9909 - val_loss: 0.0743 - val_accuracy: 0.9658\n",
            "Epoch 237/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0198 - accuracy: 0.9929 - val_loss: 0.1562 - val_accuracy: 0.9375\n",
            "Epoch 238/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0197 - accuracy: 0.9959 - val_loss: 0.1116 - val_accuracy: 0.9551\n",
            "Epoch 239/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0318 - accuracy: 0.9877 - val_loss: 0.1071 - val_accuracy: 0.9717\n",
            "Epoch 240/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0298 - accuracy: 0.9892 - val_loss: 0.1136 - val_accuracy: 0.9639\n",
            "Epoch 241/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0295 - accuracy: 0.9899 - val_loss: 0.1419 - val_accuracy: 0.9443\n",
            "Epoch 242/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0211 - accuracy: 0.9944 - val_loss: 0.0932 - val_accuracy: 0.9658\n",
            "Epoch 243/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0177 - accuracy: 0.9936 - val_loss: 0.1467 - val_accuracy: 0.9482\n",
            "Epoch 244/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0186 - accuracy: 0.9943 - val_loss: 0.0932 - val_accuracy: 0.9658\n",
            "Epoch 245/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0195 - accuracy: 0.9950 - val_loss: 0.0831 - val_accuracy: 0.9648\n",
            "Epoch 246/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0137 - accuracy: 0.9965 - val_loss: 0.1331 - val_accuracy: 0.9482\n",
            "Epoch 247/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0145 - accuracy: 0.9950 - val_loss: 0.2093 - val_accuracy: 0.9248\n",
            "Epoch 248/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0225 - accuracy: 0.9898 - val_loss: 0.0558 - val_accuracy: 0.9785\n",
            "Epoch 249/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0108 - accuracy: 0.9977 - val_loss: 0.0591 - val_accuracy: 0.9707\n",
            "Epoch 250/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0138 - accuracy: 0.9974 - val_loss: 0.5422 - val_accuracy: 0.8057\n",
            "Epoch 251/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0181 - accuracy: 0.9960 - val_loss: 0.5560 - val_accuracy: 0.8350\n",
            "Epoch 252/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0136 - accuracy: 0.9962 - val_loss: 0.0633 - val_accuracy: 0.9746\n",
            "Epoch 253/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0161 - accuracy: 0.9960 - val_loss: 0.0736 - val_accuracy: 0.9746\n",
            "Epoch 254/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0158 - accuracy: 0.9939 - val_loss: 0.0569 - val_accuracy: 0.9805\n",
            "Epoch 255/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0158 - accuracy: 0.9953 - val_loss: 0.1185 - val_accuracy: 0.9502\n",
            "Epoch 256/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0145 - accuracy: 0.9953 - val_loss: 0.0897 - val_accuracy: 0.9688\n",
            "Epoch 257/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0177 - accuracy: 0.9945 - val_loss: 0.0655 - val_accuracy: 0.9756\n",
            "Epoch 258/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0123 - accuracy: 0.9961 - val_loss: 0.0664 - val_accuracy: 0.9766\n",
            "Epoch 259/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0143 - accuracy: 0.9967 - val_loss: 0.0716 - val_accuracy: 0.9785\n",
            "Epoch 260/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0104 - accuracy: 0.9984 - val_loss: 0.0658 - val_accuracy: 0.9785\n",
            "Epoch 261/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0102 - accuracy: 0.9984 - val_loss: 0.0662 - val_accuracy: 0.9785\n",
            "Epoch 262/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0122 - accuracy: 0.9973 - val_loss: 0.0503 - val_accuracy: 0.9834\n",
            "Epoch 263/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0102 - accuracy: 0.9986 - val_loss: 0.0550 - val_accuracy: 0.9785\n",
            "Epoch 264/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0091 - accuracy: 0.9981 - val_loss: 0.0596 - val_accuracy: 0.9766\n",
            "Epoch 265/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0132 - accuracy: 0.9941 - val_loss: 0.1158 - val_accuracy: 0.9561\n",
            "Epoch 266/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0104 - accuracy: 0.9988 - val_loss: 0.0762 - val_accuracy: 0.9717\n",
            "Epoch 267/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0125 - accuracy: 0.9974 - val_loss: 0.0623 - val_accuracy: 0.9766\n",
            "Epoch 268/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0097 - accuracy: 0.9977 - val_loss: 0.1316 - val_accuracy: 0.9492\n",
            "Epoch 269/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0062 - accuracy: 0.9993 - val_loss: 0.0692 - val_accuracy: 0.9795\n",
            "Epoch 270/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0197 - accuracy: 0.9956 - val_loss: 0.2271 - val_accuracy: 0.9023\n",
            "Epoch 271/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0188 - accuracy: 0.9939 - val_loss: 0.2064 - val_accuracy: 0.9209\n",
            "Epoch 272/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0168 - accuracy: 0.9931 - val_loss: 0.1376 - val_accuracy: 0.9502\n",
            "Epoch 273/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0313 - accuracy: 0.9908 - val_loss: 0.2459 - val_accuracy: 0.9082\n",
            "Epoch 274/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0144 - accuracy: 0.9963 - val_loss: 0.1585 - val_accuracy: 0.9502\n",
            "Epoch 275/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0344 - accuracy: 0.9874 - val_loss: 0.0787 - val_accuracy: 0.9756\n",
            "Epoch 276/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0140 - accuracy: 0.9965 - val_loss: 0.0580 - val_accuracy: 0.9824\n",
            "Epoch 277/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0163 - accuracy: 0.9937 - val_loss: 0.1799 - val_accuracy: 0.9355\n",
            "Epoch 278/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0201 - accuracy: 0.9936 - val_loss: 0.1784 - val_accuracy: 0.9307\n",
            "Epoch 279/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0179 - accuracy: 0.9930 - val_loss: 0.1046 - val_accuracy: 0.9609\n",
            "Epoch 280/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0096 - accuracy: 0.9982 - val_loss: 0.0605 - val_accuracy: 0.9805\n",
            "Epoch 281/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0198 - accuracy: 0.9931 - val_loss: 0.0779 - val_accuracy: 0.9717\n",
            "Epoch 282/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0129 - accuracy: 0.9975 - val_loss: 0.0539 - val_accuracy: 0.9834\n",
            "Epoch 283/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0103 - accuracy: 0.9975 - val_loss: 23.2454 - val_accuracy: 0.3223\n",
            "Epoch 284/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0168 - accuracy: 0.9936 - val_loss: 1.3168 - val_accuracy: 0.7461\n",
            "Epoch 285/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0125 - accuracy: 0.9979 - val_loss: 0.3989 - val_accuracy: 0.8545\n",
            "Epoch 286/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0198 - accuracy: 0.9932 - val_loss: 0.0643 - val_accuracy: 0.9766\n",
            "Epoch 287/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0116 - accuracy: 0.9977 - val_loss: 0.0602 - val_accuracy: 0.9814\n",
            "Epoch 288/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0074 - accuracy: 0.9977 - val_loss: 0.0533 - val_accuracy: 0.9824\n",
            "Epoch 289/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0125 - accuracy: 0.9957 - val_loss: 0.0595 - val_accuracy: 0.9795\n",
            "Epoch 290/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0087 - accuracy: 0.9982 - val_loss: 0.2131 - val_accuracy: 0.9258\n",
            "Epoch 291/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0172 - accuracy: 0.9945 - val_loss: 0.0645 - val_accuracy: 0.9785\n",
            "Epoch 292/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0083 - accuracy: 0.9981 - val_loss: 0.0581 - val_accuracy: 0.9766\n",
            "Epoch 293/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0073 - accuracy: 0.9979 - val_loss: 0.0556 - val_accuracy: 0.9834\n",
            "Epoch 294/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0112 - accuracy: 0.9977 - val_loss: 0.0687 - val_accuracy: 0.9746\n",
            "Epoch 295/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0068 - accuracy: 0.9984 - val_loss: 0.0834 - val_accuracy: 0.9707\n",
            "Epoch 296/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0078 - accuracy: 0.9984 - val_loss: 0.1535 - val_accuracy: 0.9512\n",
            "Epoch 297/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0161 - accuracy: 0.9948 - val_loss: 0.1156 - val_accuracy: 0.9580\n",
            "Epoch 298/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0070 - accuracy: 0.9989 - val_loss: 0.0757 - val_accuracy: 0.9678\n",
            "Epoch 299/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0073 - accuracy: 0.9988 - val_loss: 0.0898 - val_accuracy: 0.9648\n",
            "Epoch 300/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0093 - accuracy: 0.9978 - val_loss: 0.0637 - val_accuracy: 0.9727\n",
            "Epoch 301/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0078 - accuracy: 0.9982 - val_loss: 0.0761 - val_accuracy: 0.9697\n",
            "Epoch 302/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0070 - accuracy: 0.9987 - val_loss: 0.0674 - val_accuracy: 0.9746\n",
            "Epoch 303/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0056 - accuracy: 0.9993 - val_loss: 0.0594 - val_accuracy: 0.9814\n",
            "Epoch 304/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0057 - accuracy: 0.9992 - val_loss: 0.0539 - val_accuracy: 0.9805\n",
            "Epoch 305/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0049 - accuracy: 0.9994 - val_loss: 0.1188 - val_accuracy: 0.9619\n",
            "Epoch 306/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0067 - accuracy: 0.9984 - val_loss: 0.0582 - val_accuracy: 0.9775\n",
            "Epoch 307/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0075 - accuracy: 0.9986 - val_loss: 0.0617 - val_accuracy: 0.9775\n",
            "Epoch 308/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0072 - accuracy: 0.9987 - val_loss: 0.0764 - val_accuracy: 0.9795\n",
            "Epoch 309/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0070 - accuracy: 0.9981 - val_loss: 0.0575 - val_accuracy: 0.9775\n",
            "Epoch 310/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0098 - accuracy: 0.9974 - val_loss: 0.1033 - val_accuracy: 0.9609\n",
            "Epoch 311/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0196 - accuracy: 0.9926 - val_loss: 0.1668 - val_accuracy: 0.9375\n",
            "Epoch 312/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0092 - accuracy: 0.9984 - val_loss: 0.0556 - val_accuracy: 0.9824\n",
            "Epoch 313/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0083 - accuracy: 0.9988 - val_loss: 0.0622 - val_accuracy: 0.9785\n",
            "Epoch 314/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0072 - accuracy: 0.9988 - val_loss: 0.0863 - val_accuracy: 0.9648\n",
            "Epoch 315/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0052 - accuracy: 0.9994 - val_loss: 0.0539 - val_accuracy: 0.9814\n",
            "Epoch 316/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0070 - accuracy: 0.9984 - val_loss: 0.0643 - val_accuracy: 0.9785\n",
            "Epoch 317/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0075 - accuracy: 0.9989 - val_loss: 0.0621 - val_accuracy: 0.9785\n",
            "Epoch 318/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0070 - accuracy: 0.9980 - val_loss: 0.0630 - val_accuracy: 0.9795\n",
            "Epoch 319/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0042 - accuracy: 0.9999 - val_loss: 0.1378 - val_accuracy: 0.9512\n",
            "Epoch 320/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0049 - accuracy: 0.9997 - val_loss: 0.0468 - val_accuracy: 0.9824\n",
            "Epoch 321/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0780 - val_accuracy: 0.9688\n",
            "Epoch 322/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0067 - accuracy: 0.9983 - val_loss: 0.1251 - val_accuracy: 0.9580\n",
            "Epoch 323/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0045 - accuracy: 0.9993 - val_loss: 0.0565 - val_accuracy: 0.9824\n",
            "Epoch 324/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0042 - accuracy: 0.9990 - val_loss: 0.0700 - val_accuracy: 0.9727\n",
            "Epoch 325/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0079 - accuracy: 0.9979 - val_loss: 0.0568 - val_accuracy: 0.9834\n",
            "Epoch 326/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0031 - accuracy: 0.9999 - val_loss: 0.0514 - val_accuracy: 0.9854\n",
            "Epoch 327/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0042 - accuracy: 0.9992 - val_loss: 0.0597 - val_accuracy: 0.9805\n",
            "Epoch 328/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0787 - val_accuracy: 0.9746\n",
            "Epoch 329/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0046 - accuracy: 0.9994 - val_loss: 0.0457 - val_accuracy: 0.9844\n",
            "Epoch 330/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0609 - val_accuracy: 0.9766\n",
            "Epoch 331/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0042 - accuracy: 0.9991 - val_loss: 0.0439 - val_accuracy: 0.9824\n",
            "Epoch 332/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0028 - accuracy: 0.9999 - val_loss: 0.0612 - val_accuracy: 0.9795\n",
            "Epoch 333/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0054 - accuracy: 0.9995 - val_loss: 0.0678 - val_accuracy: 0.9775\n",
            "Epoch 334/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0032 - accuracy: 0.9998 - val_loss: 0.0531 - val_accuracy: 0.9834\n",
            "Epoch 335/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0659 - val_accuracy: 0.9795\n",
            "Epoch 336/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0031 - accuracy: 0.9997 - val_loss: 0.0628 - val_accuracy: 0.9775\n",
            "Epoch 337/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0029 - accuracy: 0.9999 - val_loss: 0.0609 - val_accuracy: 0.9775\n",
            "Epoch 338/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0024 - accuracy: 0.9998 - val_loss: 0.0447 - val_accuracy: 0.9863\n",
            "Epoch 339/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0028 - accuracy: 0.9997 - val_loss: 0.0527 - val_accuracy: 0.9854\n",
            "Epoch 340/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0597 - val_accuracy: 0.9824\n",
            "Epoch 341/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0038 - accuracy: 0.9997 - val_loss: 0.0707 - val_accuracy: 0.9746\n",
            "Epoch 342/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0566 - val_accuracy: 0.9824\n",
            "Epoch 343/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0511 - val_accuracy: 0.9844\n",
            "Epoch 344/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0539 - val_accuracy: 0.9834\n",
            "Epoch 345/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0024 - accuracy: 0.9998 - val_loss: 0.0630 - val_accuracy: 0.9785\n",
            "Epoch 346/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0027 - accuracy: 0.9999 - val_loss: 0.0549 - val_accuracy: 0.9844\n",
            "Epoch 347/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0040 - accuracy: 0.9988 - val_loss: 0.0551 - val_accuracy: 0.9824\n",
            "Epoch 348/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0020 - accuracy: 0.9999 - val_loss: 0.0577 - val_accuracy: 0.9844\n",
            "Epoch 349/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0777 - val_accuracy: 0.9736\n",
            "Epoch 350/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0585 - val_accuracy: 0.9824\n",
            "Epoch 351/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0032 - accuracy: 0.9999 - val_loss: 0.0533 - val_accuracy: 0.9834\n",
            "Epoch 352/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0638 - val_accuracy: 0.9785\n",
            "Epoch 353/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0654 - val_accuracy: 0.9775\n",
            "Epoch 354/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0035 - accuracy: 0.9988 - val_loss: 0.0588 - val_accuracy: 0.9834\n",
            "Epoch 355/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0609 - val_accuracy: 0.9775\n",
            "Epoch 356/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0584 - val_accuracy: 0.9795\n",
            "Epoch 357/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0024 - accuracy: 0.9999 - val_loss: 0.0806 - val_accuracy: 0.9766\n",
            "Epoch 358/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0138 - accuracy: 0.9959 - val_loss: 0.0573 - val_accuracy: 0.9824\n",
            "Epoch 359/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0481 - val_accuracy: 0.9834\n",
            "Epoch 360/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0522 - val_accuracy: 0.9854\n",
            "Epoch 361/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0059 - accuracy: 0.9987 - val_loss: 0.1270 - val_accuracy: 0.9600\n",
            "Epoch 362/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0088 - accuracy: 0.9981 - val_loss: 0.0653 - val_accuracy: 0.9766\n",
            "Epoch 363/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0824 - accuracy: 0.9763 - val_loss: 0.1852 - val_accuracy: 0.9473\n",
            "Epoch 364/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0454 - accuracy: 0.9830 - val_loss: 0.4242 - val_accuracy: 0.8311\n",
            "Epoch 365/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0242 - accuracy: 0.9908 - val_loss: 0.0909 - val_accuracy: 0.9697\n",
            "Epoch 366/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0086 - accuracy: 0.9979 - val_loss: 0.6649 - val_accuracy: 0.8164\n",
            "Epoch 367/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.1119 - val_accuracy: 0.9629\n",
            "Epoch 368/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0098 - accuracy: 0.9981 - val_loss: 0.1293 - val_accuracy: 0.9580\n",
            "Epoch 369/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0077 - accuracy: 0.9986 - val_loss: 0.0752 - val_accuracy: 0.9707\n",
            "Epoch 370/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0049 - accuracy: 0.9994 - val_loss: 0.0569 - val_accuracy: 0.9814\n",
            "Epoch 371/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0037 - accuracy: 0.9999 - val_loss: 0.0686 - val_accuracy: 0.9736\n",
            "Epoch 372/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0079 - accuracy: 0.9986 - val_loss: 0.0803 - val_accuracy: 0.9746\n",
            "Epoch 373/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0576 - val_accuracy: 0.9844\n",
            "Epoch 374/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0652 - val_accuracy: 0.9766\n",
            "Epoch 375/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0033 - accuracy: 0.9998 - val_loss: 0.0630 - val_accuracy: 0.9814\n",
            "Epoch 376/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0027 - accuracy: 0.9998 - val_loss: 0.0471 - val_accuracy: 0.9854\n",
            "Epoch 377/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0029 - accuracy: 0.9998 - val_loss: 0.0472 - val_accuracy: 0.9873\n",
            "Epoch 378/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0662 - val_accuracy: 0.9766\n",
            "Epoch 379/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0560 - val_accuracy: 0.9824\n",
            "Epoch 380/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0020 - accuracy: 0.9999 - val_loss: 0.0498 - val_accuracy: 0.9863\n",
            "Epoch 381/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0016 - accuracy: 0.9999 - val_loss: 0.0482 - val_accuracy: 0.9854\n",
            "Epoch 382/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0029 - accuracy: 0.9986 - val_loss: 0.0587 - val_accuracy: 0.9814\n",
            "Epoch 383/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0530 - val_accuracy: 0.9814\n",
            "Epoch 384/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0543 - val_accuracy: 0.9824\n",
            "Epoch 385/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0702 - val_accuracy: 0.9756\n",
            "Epoch 386/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0571 - val_accuracy: 0.9824\n",
            "Epoch 387/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0051 - accuracy: 0.9985 - val_loss: 0.0630 - val_accuracy: 0.9834\n",
            "Epoch 388/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0550 - val_accuracy: 0.9854\n",
            "Epoch 389/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0561 - val_accuracy: 0.9834\n",
            "Epoch 390/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0436 - val_accuracy: 0.9863\n",
            "Epoch 391/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.0598 - val_accuracy: 0.9766\n",
            "Epoch 392/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0021 - accuracy: 0.9999 - val_loss: 0.0540 - val_accuracy: 0.9844\n",
            "Epoch 393/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0577 - val_accuracy: 0.9854\n",
            "Epoch 394/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0564 - val_accuracy: 0.9824\n",
            "Epoch 395/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0038 - accuracy: 0.9995 - val_loss: 0.0474 - val_accuracy: 0.9854\n",
            "Epoch 396/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0794 - val_accuracy: 0.9697\n",
            "Epoch 397/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0578 - val_accuracy: 0.9834\n",
            "Epoch 398/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0574 - val_accuracy: 0.9805\n",
            "Epoch 399/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0729 - val_accuracy: 0.9746\n",
            "Epoch 400/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0574 - val_accuracy: 0.9814\n",
            "Epoch 401/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0579 - val_accuracy: 0.9824\n",
            "Epoch 402/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0025 - accuracy: 0.9997 - val_loss: 0.0540 - val_accuracy: 0.9844\n",
            "Epoch 403/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 0.0736 - val_accuracy: 0.9766\n",
            "Epoch 404/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0544 - accuracy: 0.9863 - val_loss: 0.2719 - val_accuracy: 0.9004\n",
            "Epoch 405/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0037 - accuracy: 0.9998 - val_loss: 0.0517 - val_accuracy: 0.9805\n",
            "Epoch 406/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0063 - accuracy: 0.9982 - val_loss: 0.0551 - val_accuracy: 0.9854\n",
            "Epoch 407/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0030 - accuracy: 0.9996 - val_loss: 0.0608 - val_accuracy: 0.9844\n",
            "Epoch 408/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0660 - val_accuracy: 0.9795\n",
            "Epoch 409/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0026 - accuracy: 0.9997 - val_loss: 0.0607 - val_accuracy: 0.9844\n",
            "Epoch 410/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0022 - accuracy: 0.9998 - val_loss: 0.0581 - val_accuracy: 0.9844\n",
            "Epoch 411/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 0.0596 - val_accuracy: 0.9854\n",
            "Epoch 412/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.0490 - val_accuracy: 0.9863\n",
            "Epoch 413/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0594 - val_accuracy: 0.9834\n",
            "Epoch 414/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.0613 - val_accuracy: 0.9805\n",
            "Epoch 415/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 0.9908 - val_accuracy: 0.7900\n",
            "Epoch 416/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0471 - val_accuracy: 0.9873\n",
            "Epoch 417/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.0478 - val_accuracy: 0.9863\n",
            "Epoch 418/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0552 - val_accuracy: 0.9795\n",
            "Epoch 419/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0621 - val_accuracy: 0.9834\n",
            "Epoch 420/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0557 - val_accuracy: 0.9844\n",
            "Epoch 421/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0600 - val_accuracy: 0.9795\n",
            "Epoch 422/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0571 - val_accuracy: 0.9824\n",
            "Epoch 423/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0613 - val_accuracy: 0.9824\n",
            "Epoch 424/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0566 - val_accuracy: 0.9824\n",
            "Epoch 425/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0523 - val_accuracy: 0.9844\n",
            "Epoch 426/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0536 - val_accuracy: 0.9854\n",
            "Epoch 427/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 9.1046e-04 - accuracy: 1.0000 - val_loss: 0.0515 - val_accuracy: 0.9873\n",
            "Epoch 428/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0016 - accuracy: 0.9999 - val_loss: 0.0606 - val_accuracy: 0.9834\n",
            "Epoch 429/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 8.5146e-04 - accuracy: 1.0000 - val_loss: 0.0563 - val_accuracy: 0.9844\n",
            "Epoch 430/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0603 - val_accuracy: 0.9834\n",
            "Epoch 431/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0586 - val_accuracy: 0.9834\n",
            "Epoch 432/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0531 - val_accuracy: 0.9854\n",
            "Epoch 433/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0567 - val_accuracy: 0.9834\n",
            "Epoch 434/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0592 - val_accuracy: 0.9844\n",
            "Epoch 435/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0577 - val_accuracy: 0.9844\n",
            "Epoch 436/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 9.8093e-04 - accuracy: 1.0000 - val_loss: 0.0526 - val_accuracy: 0.9834\n",
            "Epoch 437/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 8.5730e-04 - accuracy: 1.0000 - val_loss: 0.0564 - val_accuracy: 0.9854\n",
            "Epoch 438/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 9.7996e-04 - accuracy: 1.0000 - val_loss: 0.0620 - val_accuracy: 0.9834\n",
            "Epoch 439/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0500 - val_accuracy: 0.9863\n",
            "Epoch 440/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0587 - val_accuracy: 0.9824\n",
            "Epoch 441/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0690 - val_accuracy: 0.9834\n",
            "Epoch 442/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 9.8658e-04 - accuracy: 1.0000 - val_loss: 0.0583 - val_accuracy: 0.9844\n",
            "Epoch 443/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.0506 - val_accuracy: 0.9854\n",
            "Epoch 444/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.0709 - val_accuracy: 0.9785\n",
            "Epoch 445/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 9.9455e-04 - accuracy: 1.0000 - val_loss: 0.0514 - val_accuracy: 0.9863\n",
            "Epoch 446/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0514 - val_accuracy: 0.9863\n",
            "Epoch 447/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0597 - val_accuracy: 0.9854\n",
            "Epoch 448/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.0576 - val_accuracy: 0.9854\n",
            "Epoch 449/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0014 - accuracy: 0.9993 - val_loss: 0.0547 - val_accuracy: 0.9844\n",
            "Epoch 450/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0572 - val_accuracy: 0.9844\n",
            "Epoch 451/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 9.1241e-04 - accuracy: 1.0000 - val_loss: 0.0584 - val_accuracy: 0.9824\n",
            "Epoch 452/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 7.0128e-04 - accuracy: 1.0000 - val_loss: 0.0563 - val_accuracy: 0.9854\n",
            "Epoch 453/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0561 - val_accuracy: 0.9863\n",
            "Epoch 454/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0571 - val_accuracy: 0.9854\n",
            "Epoch 455/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 8.5580e-04 - accuracy: 0.9999 - val_loss: 0.0504 - val_accuracy: 0.9854\n",
            "Epoch 456/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0592 - val_accuracy: 0.9844\n",
            "Epoch 457/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0542 - val_accuracy: 0.9844\n",
            "Epoch 458/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 6.7032e-04 - accuracy: 1.0000 - val_loss: 0.0542 - val_accuracy: 0.9844\n",
            "Epoch 459/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.0584 - val_accuracy: 0.9854\n",
            "Epoch 460/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0576 - val_accuracy: 0.9844\n",
            "Epoch 461/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0579 - val_accuracy: 0.9854\n",
            "Epoch 462/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 8.9061e-04 - accuracy: 1.0000 - val_loss: 0.0645 - val_accuracy: 0.9854\n",
            "Epoch 463/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0598 - val_accuracy: 0.9844\n",
            "Epoch 464/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 8.2382e-04 - accuracy: 1.0000 - val_loss: 0.0544 - val_accuracy: 0.9863\n",
            "Epoch 465/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0054 - accuracy: 0.9974 - val_loss: 0.0613 - val_accuracy: 0.9844\n",
            "Epoch 466/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 9.2624e-04 - accuracy: 1.0000 - val_loss: 0.0554 - val_accuracy: 0.9844\n",
            "Epoch 467/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0540 - val_accuracy: 0.9854\n",
            "Epoch 468/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 7.0695e-04 - accuracy: 1.0000 - val_loss: 0.0762 - val_accuracy: 0.9746\n",
            "Epoch 469/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 9.8198e-04 - accuracy: 1.0000 - val_loss: 0.0568 - val_accuracy: 0.9863\n",
            "Epoch 470/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 8.1846e-04 - accuracy: 1.0000 - val_loss: 0.1095 - val_accuracy: 0.9668\n",
            "Epoch 471/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0040 - accuracy: 0.9994 - val_loss: 0.0624 - val_accuracy: 0.9775\n",
            "Epoch 472/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 8.1667e-04 - accuracy: 1.0000 - val_loss: 0.0627 - val_accuracy: 0.9824\n",
            "Epoch 473/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 7.1928e-04 - accuracy: 1.0000 - val_loss: 0.0596 - val_accuracy: 0.9844\n",
            "Epoch 474/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 7.3998e-04 - accuracy: 1.0000 - val_loss: 0.0595 - val_accuracy: 0.9834\n",
            "Epoch 475/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 9.4882e-04 - accuracy: 1.0000 - val_loss: 0.0570 - val_accuracy: 0.9844\n",
            "Epoch 476/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 6.2852e-04 - accuracy: 1.0000 - val_loss: 0.0555 - val_accuracy: 0.9863\n",
            "Epoch 477/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 6.7287e-04 - accuracy: 1.0000 - val_loss: 0.0604 - val_accuracy: 0.9795\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00477: early stopping\n",
            "\n",
            "Accuracy: 97.95%\n",
            "Loss: 0.060367174446582794 \n",
            "\n",
            "NEXT SET OF HYPERPARAMETERS IS: \n",
            " num_conv_layers: 4 \n",
            " num_conv_nodes: 110 \n",
            " num_dense_layers: 1 \n",
            " num_dense_nodes: 174 \n",
            "\n",
            "Epoch 1/500\n",
            "128/128 [==============================] - 2s 9ms/step - loss: 1.4948 - accuracy: 0.4597 - val_loss: 2.8213 - val_accuracy: 0.0459\n",
            "Epoch 2/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.8287 - accuracy: 0.6277 - val_loss: 2.6952 - val_accuracy: 0.2275\n",
            "Epoch 3/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7336 - accuracy: 0.6934 - val_loss: 2.7616 - val_accuracy: 0.1768\n",
            "Epoch 4/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7742 - accuracy: 0.6643 - val_loss: 2.0538 - val_accuracy: 0.2656\n",
            "Epoch 5/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6652 - accuracy: 0.7357 - val_loss: 1.3338 - val_accuracy: 0.4531\n",
            "Epoch 6/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.6076 - accuracy: 0.7583 - val_loss: 1.4768 - val_accuracy: 0.4873\n",
            "Epoch 7/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.6075 - accuracy: 0.7438 - val_loss: 1.6744 - val_accuracy: 0.5273\n",
            "Epoch 8/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5277 - accuracy: 0.7896 - val_loss: 0.9975 - val_accuracy: 0.5020\n",
            "Epoch 9/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.5572 - accuracy: 0.7759 - val_loss: 0.8736 - val_accuracy: 0.6631\n",
            "Epoch 10/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.3591 - accuracy: 0.8522 - val_loss: 1.7130 - val_accuracy: 0.4990\n",
            "Epoch 11/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5427 - accuracy: 0.7786 - val_loss: 1.8625 - val_accuracy: 0.4873\n",
            "Epoch 12/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7810 - accuracy: 0.6716 - val_loss: 1.6872 - val_accuracy: 0.3740\n",
            "Epoch 13/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7079 - accuracy: 0.7025 - val_loss: 2.8398 - val_accuracy: 0.4502\n",
            "Epoch 14/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5865 - accuracy: 0.7666 - val_loss: 0.9185 - val_accuracy: 0.5908\n",
            "Epoch 15/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4281 - accuracy: 0.8393 - val_loss: 1.1172 - val_accuracy: 0.6094\n",
            "Epoch 16/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4262 - accuracy: 0.8268 - val_loss: 2.5476 - val_accuracy: 0.3066\n",
            "Epoch 17/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.3843 - accuracy: 0.8448 - val_loss: 1.2817 - val_accuracy: 0.4541\n",
            "Epoch 18/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.3584 - accuracy: 0.8544 - val_loss: 1.4819 - val_accuracy: 0.3467\n",
            "Epoch 19/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.3853 - accuracy: 0.8513 - val_loss: 3.8529 - val_accuracy: 0.3652\n",
            "Epoch 20/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4024 - accuracy: 0.8401 - val_loss: 20.7718 - val_accuracy: 0.0615\n",
            "Epoch 21/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7885 - accuracy: 0.6849 - val_loss: 6.1491 - val_accuracy: 0.1133\n",
            "Epoch 22/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4861 - accuracy: 0.7995 - val_loss: 5.9374 - val_accuracy: 0.1729\n",
            "Epoch 23/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7968 - accuracy: 0.6778 - val_loss: 1.3970 - val_accuracy: 0.3525\n",
            "Epoch 24/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7831 - accuracy: 0.6403 - val_loss: 1.2019 - val_accuracy: 0.4434\n",
            "Epoch 25/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7546 - accuracy: 0.6515 - val_loss: 1.6629 - val_accuracy: 0.3623\n",
            "Epoch 26/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7661 - accuracy: 0.6547 - val_loss: 0.9228 - val_accuracy: 0.6475\n",
            "Epoch 27/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7311 - accuracy: 0.6709 - val_loss: 0.8699 - val_accuracy: 0.6113\n",
            "Epoch 28/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7175 - accuracy: 0.6578 - val_loss: 0.7769 - val_accuracy: 0.6650\n",
            "Epoch 29/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7552 - accuracy: 0.6459 - val_loss: 0.7742 - val_accuracy: 0.6406\n",
            "Epoch 30/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7249 - accuracy: 0.6661 - val_loss: 0.8021 - val_accuracy: 0.6338\n",
            "Epoch 31/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7444 - accuracy: 0.6561 - val_loss: 0.8896 - val_accuracy: 0.6064\n",
            "Epoch 32/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7555 - accuracy: 0.6319 - val_loss: 0.7405 - val_accuracy: 0.6670\n",
            "Epoch 33/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7190 - accuracy: 0.6573 - val_loss: 0.8633 - val_accuracy: 0.5811\n",
            "Epoch 34/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7358 - accuracy: 0.6518 - val_loss: 0.7237 - val_accuracy: 0.6758\n",
            "Epoch 35/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7152 - accuracy: 0.6650 - val_loss: 0.7499 - val_accuracy: 0.6553\n",
            "Epoch 36/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.7345 - accuracy: 0.6402 - val_loss: 0.7956 - val_accuracy: 0.6162\n",
            "Epoch 37/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7125 - accuracy: 0.6636 - val_loss: 0.7427 - val_accuracy: 0.6572\n",
            "Epoch 38/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7011 - accuracy: 0.6752 - val_loss: 0.8564 - val_accuracy: 0.5801\n",
            "Epoch 39/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7055 - accuracy: 0.6604 - val_loss: 0.7009 - val_accuracy: 0.6621\n",
            "Epoch 40/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.7030 - accuracy: 0.6695 - val_loss: 1.0848 - val_accuracy: 0.5459\n",
            "Epoch 41/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7023 - accuracy: 0.6669 - val_loss: 0.6985 - val_accuracy: 0.6855\n",
            "Epoch 42/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7015 - accuracy: 0.6684 - val_loss: 0.7539 - val_accuracy: 0.6465\n",
            "Epoch 43/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7150 - accuracy: 0.6573 - val_loss: 0.7426 - val_accuracy: 0.6553\n",
            "Epoch 44/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7137 - accuracy: 0.6574 - val_loss: 0.6937 - val_accuracy: 0.6924\n",
            "Epoch 45/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6942 - accuracy: 0.6724 - val_loss: 0.7903 - val_accuracy: 0.6182\n",
            "Epoch 46/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6981 - accuracy: 0.6704 - val_loss: 0.7303 - val_accuracy: 0.6602\n",
            "Epoch 47/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6879 - accuracy: 0.6708 - val_loss: 0.7446 - val_accuracy: 0.6562\n",
            "Epoch 48/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6780 - accuracy: 0.6727 - val_loss: 0.7315 - val_accuracy: 0.6514\n",
            "Epoch 49/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6984 - accuracy: 0.6662 - val_loss: 0.7343 - val_accuracy: 0.6455\n",
            "Epoch 50/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6917 - accuracy: 0.6647 - val_loss: 0.7385 - val_accuracy: 0.6406\n",
            "Epoch 51/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7066 - accuracy: 0.6651 - val_loss: 0.7043 - val_accuracy: 0.6738\n",
            "Epoch 52/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6801 - accuracy: 0.6835 - val_loss: 0.7405 - val_accuracy: 0.6426\n",
            "Epoch 53/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6844 - accuracy: 0.6651 - val_loss: 1.1266 - val_accuracy: 0.5127\n",
            "Epoch 54/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6816 - accuracy: 0.6612 - val_loss: 0.7271 - val_accuracy: 0.6719\n",
            "Epoch 55/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6918 - accuracy: 0.6592 - val_loss: 0.7043 - val_accuracy: 0.6787\n",
            "Epoch 56/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6836 - accuracy: 0.6777 - val_loss: 0.7138 - val_accuracy: 0.6865\n",
            "Epoch 57/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6909 - accuracy: 0.6691 - val_loss: 0.6964 - val_accuracy: 0.6816\n",
            "Epoch 58/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6792 - accuracy: 0.6715 - val_loss: 0.7161 - val_accuracy: 0.6670\n",
            "Epoch 59/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7050 - accuracy: 0.6590 - val_loss: 0.7019 - val_accuracy: 0.6855\n",
            "Epoch 60/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7023 - accuracy: 0.6738 - val_loss: 0.8140 - val_accuracy: 0.6133\n",
            "Epoch 61/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6757 - accuracy: 0.6749 - val_loss: 0.7269 - val_accuracy: 0.6543\n",
            "Epoch 62/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6910 - accuracy: 0.6661 - val_loss: 0.7084 - val_accuracy: 0.6709\n",
            "Epoch 63/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6862 - accuracy: 0.6730 - val_loss: 0.7246 - val_accuracy: 0.6484\n",
            "Epoch 64/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6839 - accuracy: 0.6734 - val_loss: 0.6918 - val_accuracy: 0.6816\n",
            "Epoch 65/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.6586 - accuracy: 0.6922 - val_loss: 0.7282 - val_accuracy: 0.6689\n",
            "Epoch 66/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6908 - accuracy: 0.6635 - val_loss: 0.6909 - val_accuracy: 0.6914\n",
            "Epoch 67/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6576 - accuracy: 0.6890 - val_loss: 0.6952 - val_accuracy: 0.6846\n",
            "Epoch 68/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6786 - accuracy: 0.6835 - val_loss: 0.6789 - val_accuracy: 0.6865\n",
            "Epoch 69/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6797 - accuracy: 0.6680 - val_loss: 0.7206 - val_accuracy: 0.6475\n",
            "Epoch 70/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6783 - accuracy: 0.6781 - val_loss: 0.6819 - val_accuracy: 0.6846\n",
            "Epoch 71/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6714 - accuracy: 0.6728 - val_loss: 0.6736 - val_accuracy: 0.6953\n",
            "Epoch 72/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6677 - accuracy: 0.6831 - val_loss: 0.7144 - val_accuracy: 0.6660\n",
            "Epoch 73/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6664 - accuracy: 0.6780 - val_loss: 0.7326 - val_accuracy: 0.6592\n",
            "Epoch 74/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6594 - accuracy: 0.6913 - val_loss: 0.7194 - val_accuracy: 0.6523\n",
            "Epoch 75/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6803 - accuracy: 0.6796 - val_loss: 0.6777 - val_accuracy: 0.6826\n",
            "Epoch 76/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6857 - accuracy: 0.6766 - val_loss: 0.7314 - val_accuracy: 0.6377\n",
            "Epoch 77/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6885 - accuracy: 0.6722 - val_loss: 1.1873 - val_accuracy: 0.4395\n",
            "Epoch 78/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6663 - accuracy: 0.6749 - val_loss: 0.9208 - val_accuracy: 0.5557\n",
            "Epoch 79/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.6348 - accuracy: 0.6865 - val_loss: 0.7038 - val_accuracy: 0.6719\n",
            "Epoch 80/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.6581 - accuracy: 0.6824 - val_loss: 0.7047 - val_accuracy: 0.6826\n",
            "Epoch 81/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.6775 - accuracy: 0.6816 - val_loss: 0.7488 - val_accuracy: 0.6504\n",
            "Epoch 82/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.6378 - accuracy: 0.7000 - val_loss: 0.7016 - val_accuracy: 0.6631\n",
            "Epoch 83/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.6670 - accuracy: 0.6855 - val_loss: 0.6763 - val_accuracy: 0.6963\n",
            "Epoch 84/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.6403 - accuracy: 0.7082 - val_loss: 0.6773 - val_accuracy: 0.6865\n",
            "Epoch 85/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6414 - accuracy: 0.7082 - val_loss: 0.6957 - val_accuracy: 0.6748\n",
            "Epoch 86/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6456 - accuracy: 0.6972 - val_loss: 0.6582 - val_accuracy: 0.6924\n",
            "Epoch 87/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6360 - accuracy: 0.7028 - val_loss: 0.7955 - val_accuracy: 0.6309\n",
            "Epoch 88/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6406 - accuracy: 0.7004 - val_loss: 0.6693 - val_accuracy: 0.6924\n",
            "Epoch 89/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6258 - accuracy: 0.7116 - val_loss: 0.6751 - val_accuracy: 0.6758\n",
            "Epoch 90/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6279 - accuracy: 0.6974 - val_loss: 0.6690 - val_accuracy: 0.7012\n",
            "Epoch 91/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6129 - accuracy: 0.7047 - val_loss: 0.6465 - val_accuracy: 0.7070\n",
            "Epoch 92/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6162 - accuracy: 0.7143 - val_loss: 0.6381 - val_accuracy: 0.6953\n",
            "Epoch 93/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.6295 - accuracy: 0.6993 - val_loss: 0.6322 - val_accuracy: 0.7158\n",
            "Epoch 94/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6283 - accuracy: 0.6972 - val_loss: 0.6326 - val_accuracy: 0.7100\n",
            "Epoch 95/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5940 - accuracy: 0.7291 - val_loss: 0.6751 - val_accuracy: 0.6777\n",
            "Epoch 96/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6077 - accuracy: 0.7151 - val_loss: 0.7489 - val_accuracy: 0.6387\n",
            "Epoch 97/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6000 - accuracy: 0.7126 - val_loss: 1.1489 - val_accuracy: 0.5811\n",
            "Epoch 98/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6327 - accuracy: 0.6989 - val_loss: 0.6494 - val_accuracy: 0.6943\n",
            "Epoch 99/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6043 - accuracy: 0.7187 - val_loss: 0.6290 - val_accuracy: 0.7051\n",
            "Epoch 100/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5757 - accuracy: 0.7329 - val_loss: 0.6247 - val_accuracy: 0.7070\n",
            "Epoch 101/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5903 - accuracy: 0.7206 - val_loss: 0.6346 - val_accuracy: 0.7041\n",
            "Epoch 102/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6034 - accuracy: 0.7053 - val_loss: 0.6910 - val_accuracy: 0.6660\n",
            "Epoch 103/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5894 - accuracy: 0.7273 - val_loss: 0.6261 - val_accuracy: 0.7031\n",
            "Epoch 104/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6063 - accuracy: 0.7231 - val_loss: 0.6558 - val_accuracy: 0.6973\n",
            "Epoch 105/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5941 - accuracy: 0.7081 - val_loss: 0.6246 - val_accuracy: 0.7051\n",
            "Epoch 106/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5725 - accuracy: 0.7245 - val_loss: 0.6422 - val_accuracy: 0.7002\n",
            "Epoch 107/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5638 - accuracy: 0.7276 - val_loss: 0.6586 - val_accuracy: 0.6904\n",
            "Epoch 108/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6033 - accuracy: 0.7140 - val_loss: 0.6010 - val_accuracy: 0.7051\n",
            "Epoch 109/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5676 - accuracy: 0.7229 - val_loss: 0.6402 - val_accuracy: 0.7090\n",
            "Epoch 110/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5889 - accuracy: 0.7170 - val_loss: 0.6665 - val_accuracy: 0.6826\n",
            "Epoch 111/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5595 - accuracy: 0.7328 - val_loss: 0.6046 - val_accuracy: 0.7090\n",
            "Epoch 112/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5723 - accuracy: 0.7327 - val_loss: 0.6038 - val_accuracy: 0.7041\n",
            "Epoch 113/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5829 - accuracy: 0.7150 - val_loss: 0.5975 - val_accuracy: 0.7227\n",
            "Epoch 114/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5489 - accuracy: 0.7289 - val_loss: 0.6104 - val_accuracy: 0.7041\n",
            "Epoch 115/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5600 - accuracy: 0.7245 - val_loss: 0.5811 - val_accuracy: 0.7256\n",
            "Epoch 116/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5646 - accuracy: 0.7264 - val_loss: 0.5927 - val_accuracy: 0.7217\n",
            "Epoch 117/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5639 - accuracy: 0.7357 - val_loss: 0.6147 - val_accuracy: 0.6943\n",
            "Epoch 118/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5631 - accuracy: 0.7317 - val_loss: 0.6728 - val_accuracy: 0.7021\n",
            "Epoch 119/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5449 - accuracy: 0.7418 - val_loss: 0.9844 - val_accuracy: 0.5957\n",
            "Epoch 120/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5825 - accuracy: 0.7209 - val_loss: 0.6156 - val_accuracy: 0.7129\n",
            "Epoch 121/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5579 - accuracy: 0.7261 - val_loss: 0.5786 - val_accuracy: 0.7158\n",
            "Epoch 122/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.5632 - accuracy: 0.7425 - val_loss: 0.6180 - val_accuracy: 0.7031\n",
            "Epoch 123/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5375 - accuracy: 0.7395 - val_loss: 0.5833 - val_accuracy: 0.7100\n",
            "Epoch 124/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5622 - accuracy: 0.7301 - val_loss: 0.5921 - val_accuracy: 0.7129\n",
            "Epoch 125/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5643 - accuracy: 0.7243 - val_loss: 0.6224 - val_accuracy: 0.7002\n",
            "Epoch 126/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5354 - accuracy: 0.7435 - val_loss: 0.5926 - val_accuracy: 0.7207\n",
            "Epoch 127/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5492 - accuracy: 0.7341 - val_loss: 0.6246 - val_accuracy: 0.7041\n",
            "Epoch 128/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5565 - accuracy: 0.7258 - val_loss: 0.6557 - val_accuracy: 0.6914\n",
            "Epoch 129/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5529 - accuracy: 0.7308 - val_loss: 0.6135 - val_accuracy: 0.6992\n",
            "Epoch 130/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5501 - accuracy: 0.7333 - val_loss: 0.5626 - val_accuracy: 0.7217\n",
            "Epoch 131/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5253 - accuracy: 0.7450 - val_loss: 0.8527 - val_accuracy: 0.6426\n",
            "Epoch 132/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5726 - accuracy: 0.7203 - val_loss: 0.5894 - val_accuracy: 0.7119\n",
            "Epoch 133/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5620 - accuracy: 0.7239 - val_loss: 0.6253 - val_accuracy: 0.7021\n",
            "Epoch 134/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5536 - accuracy: 0.7289 - val_loss: 0.6089 - val_accuracy: 0.7100\n",
            "Epoch 135/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5621 - accuracy: 0.7233 - val_loss: 0.5799 - val_accuracy: 0.7178\n",
            "Epoch 136/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5312 - accuracy: 0.7419 - val_loss: 0.5763 - val_accuracy: 0.7207\n",
            "Epoch 137/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5237 - accuracy: 0.7435 - val_loss: 0.5719 - val_accuracy: 0.7197\n",
            "Epoch 138/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5462 - accuracy: 0.7309 - val_loss: 0.6658 - val_accuracy: 0.6826\n",
            "Epoch 139/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5569 - accuracy: 0.7340 - val_loss: 0.6029 - val_accuracy: 0.7041\n",
            "Epoch 140/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5438 - accuracy: 0.7305 - val_loss: 0.5748 - val_accuracy: 0.7197\n",
            "Epoch 141/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5250 - accuracy: 0.7521 - val_loss: 0.5741 - val_accuracy: 0.7227\n",
            "Epoch 142/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5439 - accuracy: 0.7421 - val_loss: 0.5657 - val_accuracy: 0.7207\n",
            "Epoch 143/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5266 - accuracy: 0.7526 - val_loss: 0.5625 - val_accuracy: 0.7266\n",
            "Epoch 144/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5361 - accuracy: 0.7366 - val_loss: 0.5637 - val_accuracy: 0.7207\n",
            "Epoch 145/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5415 - accuracy: 0.7366 - val_loss: 0.5803 - val_accuracy: 0.7178\n",
            "Epoch 146/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5163 - accuracy: 0.7441 - val_loss: 0.5897 - val_accuracy: 0.7188\n",
            "Epoch 147/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5311 - accuracy: 0.7396 - val_loss: 0.6112 - val_accuracy: 0.6953\n",
            "Epoch 148/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5306 - accuracy: 0.7355 - val_loss: 0.6696 - val_accuracy: 0.6777\n",
            "Epoch 149/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5324 - accuracy: 0.7442 - val_loss: 0.5831 - val_accuracy: 0.7148\n",
            "Epoch 150/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6233 - accuracy: 0.7072 - val_loss: 1.0972 - val_accuracy: 0.5420\n",
            "Epoch 151/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.5235 - accuracy: 0.7516 - val_loss: 0.6034 - val_accuracy: 0.7070\n",
            "Epoch 152/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5141 - accuracy: 0.7499 - val_loss: 0.6730 - val_accuracy: 0.6699\n",
            "Epoch 153/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5267 - accuracy: 0.7467 - val_loss: 0.6433 - val_accuracy: 0.6934\n",
            "Epoch 154/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5380 - accuracy: 0.7349 - val_loss: 0.5855 - val_accuracy: 0.7109\n",
            "Epoch 155/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5377 - accuracy: 0.7320 - val_loss: 0.6006 - val_accuracy: 0.7090\n",
            "Epoch 156/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5415 - accuracy: 0.7371 - val_loss: 0.5458 - val_accuracy: 0.7344\n",
            "Epoch 157/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5251 - accuracy: 0.7434 - val_loss: 0.5672 - val_accuracy: 0.7227\n",
            "Epoch 158/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5094 - accuracy: 0.7534 - val_loss: 0.5680 - val_accuracy: 0.7275\n",
            "Epoch 159/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5183 - accuracy: 0.7469 - val_loss: 0.5818 - val_accuracy: 0.7188\n",
            "Epoch 160/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5049 - accuracy: 0.7546 - val_loss: 0.6081 - val_accuracy: 0.7148\n",
            "Epoch 161/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5029 - accuracy: 0.7470 - val_loss: 0.5683 - val_accuracy: 0.7178\n",
            "Epoch 162/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5307 - accuracy: 0.7406 - val_loss: 0.5425 - val_accuracy: 0.7256\n",
            "Epoch 163/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.5219 - accuracy: 0.7479 - val_loss: 0.5672 - val_accuracy: 0.7266\n",
            "Epoch 164/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5150 - accuracy: 0.7476 - val_loss: 0.5541 - val_accuracy: 0.7246\n",
            "Epoch 165/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5165 - accuracy: 0.7366 - val_loss: 1.0244 - val_accuracy: 0.6270\n",
            "Epoch 166/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.5286 - accuracy: 0.7438 - val_loss: 0.5731 - val_accuracy: 0.7139\n",
            "Epoch 167/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4919 - accuracy: 0.7615 - val_loss: 0.5452 - val_accuracy: 0.7393\n",
            "Epoch 168/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5192 - accuracy: 0.7428 - val_loss: 0.5428 - val_accuracy: 0.7266\n",
            "Epoch 169/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5050 - accuracy: 0.7568 - val_loss: 0.5548 - val_accuracy: 0.7266\n",
            "Epoch 170/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5023 - accuracy: 0.7542 - val_loss: 0.5638 - val_accuracy: 0.7188\n",
            "Epoch 171/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5276 - accuracy: 0.7463 - val_loss: 0.5806 - val_accuracy: 0.7178\n",
            "Epoch 172/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5045 - accuracy: 0.7640 - val_loss: 0.5682 - val_accuracy: 0.7256\n",
            "Epoch 173/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5064 - accuracy: 0.7495 - val_loss: 0.5504 - val_accuracy: 0.7236\n",
            "Epoch 174/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5116 - accuracy: 0.7422 - val_loss: 0.5600 - val_accuracy: 0.7285\n",
            "Epoch 175/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5058 - accuracy: 0.7531 - val_loss: 1.9369 - val_accuracy: 0.4980\n",
            "Epoch 176/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5175 - accuracy: 0.7607 - val_loss: 0.5655 - val_accuracy: 0.7217\n",
            "Epoch 177/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5165 - accuracy: 0.7418 - val_loss: 0.5441 - val_accuracy: 0.7344\n",
            "Epoch 178/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4934 - accuracy: 0.7626 - val_loss: 0.5659 - val_accuracy: 0.7246\n",
            "Epoch 179/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.5142 - accuracy: 0.7524 - val_loss: 0.5378 - val_accuracy: 0.7285\n",
            "Epoch 180/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5087 - accuracy: 0.7501 - val_loss: 0.5427 - val_accuracy: 0.7402\n",
            "Epoch 181/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5236 - accuracy: 0.7464 - val_loss: 0.5377 - val_accuracy: 0.7422\n",
            "Epoch 182/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5342 - accuracy: 0.7343 - val_loss: 0.5407 - val_accuracy: 0.7314\n",
            "Epoch 183/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5255 - accuracy: 0.7456 - val_loss: 0.5623 - val_accuracy: 0.7256\n",
            "Epoch 184/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5180 - accuracy: 0.7490 - val_loss: 0.5390 - val_accuracy: 0.7295\n",
            "Epoch 185/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5164 - accuracy: 0.7508 - val_loss: 0.5605 - val_accuracy: 0.7256\n",
            "Epoch 186/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5203 - accuracy: 0.7463 - val_loss: 0.5473 - val_accuracy: 0.7305\n",
            "Epoch 187/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4896 - accuracy: 0.7611 - val_loss: 0.5676 - val_accuracy: 0.7197\n",
            "Epoch 188/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5103 - accuracy: 0.7502 - val_loss: 0.5477 - val_accuracy: 0.7266\n",
            "Epoch 189/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4893 - accuracy: 0.7550 - val_loss: 0.5706 - val_accuracy: 0.7227\n",
            "Epoch 190/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5051 - accuracy: 0.7499 - val_loss: 0.5322 - val_accuracy: 0.7324\n",
            "Epoch 191/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4955 - accuracy: 0.7542 - val_loss: 0.5370 - val_accuracy: 0.7324\n",
            "Epoch 192/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4995 - accuracy: 0.7540 - val_loss: 0.8291 - val_accuracy: 0.6299\n",
            "Epoch 193/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5271 - accuracy: 0.7320 - val_loss: 0.5528 - val_accuracy: 0.7275\n",
            "Epoch 194/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.4887 - accuracy: 0.7559 - val_loss: 0.5510 - val_accuracy: 0.7295\n",
            "Epoch 195/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5026 - accuracy: 0.7598 - val_loss: 0.5650 - val_accuracy: 0.7217\n",
            "Epoch 196/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5065 - accuracy: 0.7517 - val_loss: 0.5910 - val_accuracy: 0.7129\n",
            "Epoch 197/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5311 - accuracy: 0.7404 - val_loss: 0.5400 - val_accuracy: 0.7461\n",
            "Epoch 198/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4979 - accuracy: 0.7643 - val_loss: 0.5558 - val_accuracy: 0.7256\n",
            "Epoch 199/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5159 - accuracy: 0.7487 - val_loss: 0.5419 - val_accuracy: 0.7305\n",
            "Epoch 200/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5076 - accuracy: 0.7471 - val_loss: 0.5358 - val_accuracy: 0.7344\n",
            "Epoch 201/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5080 - accuracy: 0.7509 - val_loss: 0.5436 - val_accuracy: 0.7432\n",
            "Epoch 202/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4941 - accuracy: 0.7629 - val_loss: 0.5347 - val_accuracy: 0.7451\n",
            "Epoch 203/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5118 - accuracy: 0.7436 - val_loss: 0.5347 - val_accuracy: 0.7314\n",
            "Epoch 204/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5057 - accuracy: 0.7507 - val_loss: 0.5381 - val_accuracy: 0.7373\n",
            "Epoch 205/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5283 - accuracy: 0.7482 - val_loss: 0.5326 - val_accuracy: 0.7363\n",
            "Epoch 206/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5047 - accuracy: 0.7424 - val_loss: 0.5381 - val_accuracy: 0.7324\n",
            "Epoch 207/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5057 - accuracy: 0.7560 - val_loss: 0.5384 - val_accuracy: 0.7402\n",
            "Epoch 208/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.5158 - accuracy: 0.7405 - val_loss: 0.5331 - val_accuracy: 0.7324\n",
            "Epoch 209/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4953 - accuracy: 0.7610 - val_loss: 0.5768 - val_accuracy: 0.7480\n",
            "Epoch 210/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4959 - accuracy: 0.7566 - val_loss: 0.5307 - val_accuracy: 0.7461\n",
            "Epoch 211/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4908 - accuracy: 0.7567 - val_loss: 0.5414 - val_accuracy: 0.7432\n",
            "Epoch 212/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5064 - accuracy: 0.7563 - val_loss: 0.5401 - val_accuracy: 0.7295\n",
            "Epoch 213/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5095 - accuracy: 0.7471 - val_loss: 0.5346 - val_accuracy: 0.7344\n",
            "Epoch 214/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5157 - accuracy: 0.7441 - val_loss: 0.5638 - val_accuracy: 0.7227\n",
            "Epoch 215/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5055 - accuracy: 0.7509 - val_loss: 0.5363 - val_accuracy: 0.7412\n",
            "Epoch 216/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4901 - accuracy: 0.7648 - val_loss: 0.5282 - val_accuracy: 0.7471\n",
            "Epoch 217/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4960 - accuracy: 0.7468 - val_loss: 0.5315 - val_accuracy: 0.7324\n",
            "Epoch 218/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4908 - accuracy: 0.7629 - val_loss: 0.5403 - val_accuracy: 0.7344\n",
            "Epoch 219/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5222 - accuracy: 0.7354 - val_loss: 0.5483 - val_accuracy: 0.7412\n",
            "Epoch 220/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4894 - accuracy: 0.7595 - val_loss: 0.5441 - val_accuracy: 0.7314\n",
            "Epoch 221/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5033 - accuracy: 0.7572 - val_loss: 0.5320 - val_accuracy: 0.7363\n",
            "Epoch 222/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4984 - accuracy: 0.7498 - val_loss: 0.5302 - val_accuracy: 0.7383\n",
            "Epoch 223/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5122 - accuracy: 0.7521 - val_loss: 0.5354 - val_accuracy: 0.7363\n",
            "Epoch 224/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5016 - accuracy: 0.7498 - val_loss: 0.5808 - val_accuracy: 0.7168\n",
            "Epoch 225/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5098 - accuracy: 0.7415 - val_loss: 0.5358 - val_accuracy: 0.7334\n",
            "Epoch 226/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5022 - accuracy: 0.7522 - val_loss: 0.5312 - val_accuracy: 0.7402\n",
            "Epoch 227/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5110 - accuracy: 0.7476 - val_loss: 0.5379 - val_accuracy: 0.7344\n",
            "Epoch 228/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5274 - accuracy: 0.7413 - val_loss: 0.5611 - val_accuracy: 0.7197\n",
            "Epoch 229/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4978 - accuracy: 0.7602 - val_loss: 0.5270 - val_accuracy: 0.7471\n",
            "Epoch 230/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5140 - accuracy: 0.7422 - val_loss: 0.5339 - val_accuracy: 0.7451\n",
            "Epoch 231/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4976 - accuracy: 0.7513 - val_loss: 0.5427 - val_accuracy: 0.7295\n",
            "Epoch 232/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5277 - accuracy: 0.7407 - val_loss: 0.5323 - val_accuracy: 0.7305\n",
            "Epoch 233/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5123 - accuracy: 0.7566 - val_loss: 0.5368 - val_accuracy: 0.7305\n",
            "Epoch 234/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5062 - accuracy: 0.7489 - val_loss: 0.5323 - val_accuracy: 0.7363\n",
            "Epoch 235/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4991 - accuracy: 0.7496 - val_loss: 0.5320 - val_accuracy: 0.7441\n",
            "Epoch 236/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5022 - accuracy: 0.7568 - val_loss: 0.5344 - val_accuracy: 0.7441\n",
            "Epoch 237/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.5076 - accuracy: 0.7499 - val_loss: 0.5371 - val_accuracy: 0.7393\n",
            "Epoch 238/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5103 - accuracy: 0.7422 - val_loss: 0.5288 - val_accuracy: 0.7451\n",
            "Epoch 239/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5124 - accuracy: 0.7454 - val_loss: 0.5369 - val_accuracy: 0.7295\n",
            "Epoch 240/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4749 - accuracy: 0.7671 - val_loss: 0.5355 - val_accuracy: 0.7363\n",
            "Epoch 241/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5332 - accuracy: 0.7362 - val_loss: 0.5300 - val_accuracy: 0.7412\n",
            "Epoch 242/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5192 - accuracy: 0.7415 - val_loss: 0.5338 - val_accuracy: 0.7314\n",
            "Epoch 243/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4893 - accuracy: 0.7427 - val_loss: 0.5343 - val_accuracy: 0.7441\n",
            "Epoch 244/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.4961 - accuracy: 0.7555 - val_loss: 0.5378 - val_accuracy: 0.7354\n",
            "Epoch 245/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5081 - accuracy: 0.7529 - val_loss: 0.5312 - val_accuracy: 0.7461\n",
            "Epoch 246/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5048 - accuracy: 0.7505 - val_loss: 0.5359 - val_accuracy: 0.7314\n",
            "Epoch 247/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5156 - accuracy: 0.7469 - val_loss: 0.5354 - val_accuracy: 0.7324\n",
            "Epoch 248/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5039 - accuracy: 0.7437 - val_loss: 0.5405 - val_accuracy: 0.7363\n",
            "Epoch 249/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.4981 - accuracy: 0.7534 - val_loss: 0.5373 - val_accuracy: 0.7334\n",
            "Epoch 250/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4967 - accuracy: 0.7532 - val_loss: 0.5313 - val_accuracy: 0.7324\n",
            "Epoch 251/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.5047 - accuracy: 0.7489 - val_loss: 0.5355 - val_accuracy: 0.7305\n",
            "Epoch 252/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4869 - accuracy: 0.7621 - val_loss: 0.5367 - val_accuracy: 0.7480\n",
            "Epoch 253/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4976 - accuracy: 0.7516 - val_loss: 0.5412 - val_accuracy: 0.7285\n",
            "Epoch 254/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4829 - accuracy: 0.7594 - val_loss: 0.5375 - val_accuracy: 0.7285\n",
            "Epoch 255/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4946 - accuracy: 0.7572 - val_loss: 0.5377 - val_accuracy: 0.7314\n",
            "Epoch 256/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5080 - accuracy: 0.7547 - val_loss: 0.5405 - val_accuracy: 0.7354\n",
            "Epoch 257/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4829 - accuracy: 0.7567 - val_loss: 0.5356 - val_accuracy: 0.7461\n",
            "Epoch 258/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4785 - accuracy: 0.7668 - val_loss: 0.5383 - val_accuracy: 0.7363\n",
            "Epoch 259/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4919 - accuracy: 0.7613 - val_loss: 0.5351 - val_accuracy: 0.7441\n",
            "Epoch 260/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4756 - accuracy: 0.7685 - val_loss: 0.5345 - val_accuracy: 0.7314\n",
            "Epoch 261/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5092 - accuracy: 0.7458 - val_loss: 0.5350 - val_accuracy: 0.7344\n",
            "Epoch 262/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4917 - accuracy: 0.7568 - val_loss: 1.5203 - val_accuracy: 0.5049\n",
            "Epoch 263/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.8829 - accuracy: 0.6543 - val_loss: 2.3984 - val_accuracy: 0.2637\n",
            "Epoch 264/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6384 - accuracy: 0.7006 - val_loss: 0.7785 - val_accuracy: 0.6582\n",
            "Epoch 265/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.5372 - accuracy: 0.7321 - val_loss: 0.6080 - val_accuracy: 0.6982\n",
            "Epoch 266/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5711 - accuracy: 0.7204 - val_loss: 0.6401 - val_accuracy: 0.6973\n",
            "Epoch 267/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5044 - accuracy: 0.7599 - val_loss: 0.5480 - val_accuracy: 0.7266\n",
            "Epoch 268/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5189 - accuracy: 0.7427 - val_loss: 0.5362 - val_accuracy: 0.7441\n",
            "Epoch 269/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5097 - accuracy: 0.7617 - val_loss: 0.6654 - val_accuracy: 0.7031\n",
            "Epoch 270/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5299 - accuracy: 0.7371 - val_loss: 0.5521 - val_accuracy: 0.7266\n",
            "Epoch 271/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5065 - accuracy: 0.7519 - val_loss: 0.5569 - val_accuracy: 0.7305\n",
            "Epoch 272/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5015 - accuracy: 0.7450 - val_loss: 162.6913 - val_accuracy: 0.1133\n",
            "Epoch 273/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7731 - accuracy: 0.6572 - val_loss: 3.5774 - val_accuracy: 0.3906\n",
            "Epoch 274/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6189 - accuracy: 0.7031 - val_loss: 0.8179 - val_accuracy: 0.6367\n",
            "Epoch 275/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5456 - accuracy: 0.7404 - val_loss: 0.6976 - val_accuracy: 0.6904\n",
            "Epoch 276/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5308 - accuracy: 0.7341 - val_loss: 0.5700 - val_accuracy: 0.7227\n",
            "Epoch 277/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5001 - accuracy: 0.7623 - val_loss: 0.5791 - val_accuracy: 0.7207\n",
            "Epoch 278/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5330 - accuracy: 0.7428 - val_loss: 0.5844 - val_accuracy: 0.7158\n",
            "Epoch 279/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5168 - accuracy: 0.7526 - val_loss: 0.5464 - val_accuracy: 0.7295\n",
            "Epoch 280/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5088 - accuracy: 0.7437 - val_loss: 0.5896 - val_accuracy: 0.7236\n",
            "Epoch 281/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4989 - accuracy: 0.7534 - val_loss: 0.5497 - val_accuracy: 0.7295\n",
            "Epoch 282/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5045 - accuracy: 0.7538 - val_loss: 0.5457 - val_accuracy: 0.7314\n",
            "Epoch 283/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5163 - accuracy: 0.7433 - val_loss: 2.6249 - val_accuracy: 0.3604\n",
            "Epoch 284/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5576 - accuracy: 0.7311 - val_loss: 0.6588 - val_accuracy: 0.6787\n",
            "Epoch 285/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5154 - accuracy: 0.7481 - val_loss: 0.5870 - val_accuracy: 0.7217\n",
            "Epoch 286/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5452 - accuracy: 0.7351 - val_loss: 0.5667 - val_accuracy: 0.7256\n",
            "Epoch 287/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5100 - accuracy: 0.7549 - val_loss: 0.5590 - val_accuracy: 0.7285\n",
            "Epoch 288/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4799 - accuracy: 0.7732 - val_loss: 0.5605 - val_accuracy: 0.7197\n",
            "Epoch 289/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5084 - accuracy: 0.7489 - val_loss: 0.5390 - val_accuracy: 0.7334\n",
            "Epoch 290/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5237 - accuracy: 0.7397 - val_loss: 0.5589 - val_accuracy: 0.7217\n",
            "Epoch 291/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5006 - accuracy: 0.7592 - val_loss: 0.5452 - val_accuracy: 0.7275\n",
            "Epoch 292/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5104 - accuracy: 0.7484 - val_loss: 0.5327 - val_accuracy: 0.7461\n",
            "Epoch 293/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4974 - accuracy: 0.7523 - val_loss: 0.5429 - val_accuracy: 0.7393\n",
            "Epoch 294/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.4900 - accuracy: 0.7587 - val_loss: 0.5353 - val_accuracy: 0.7383\n",
            "Epoch 295/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5080 - accuracy: 0.7547 - val_loss: 0.5291 - val_accuracy: 0.7383\n",
            "Epoch 296/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4983 - accuracy: 0.7511 - val_loss: 0.5398 - val_accuracy: 0.7324\n",
            "Epoch 297/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5092 - accuracy: 0.7424 - val_loss: 0.5487 - val_accuracy: 0.7314\n",
            "Epoch 298/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4990 - accuracy: 0.7532 - val_loss: 0.5455 - val_accuracy: 0.7266\n",
            "Epoch 299/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5213 - accuracy: 0.7443 - val_loss: 0.5415 - val_accuracy: 0.7441\n",
            "Epoch 300/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5235 - accuracy: 0.7452 - val_loss: 0.5358 - val_accuracy: 0.7344\n",
            "Epoch 301/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4806 - accuracy: 0.7587 - val_loss: 0.5330 - val_accuracy: 0.7344\n",
            "Epoch 302/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.4979 - accuracy: 0.7525 - val_loss: 0.5486 - val_accuracy: 0.7305\n",
            "Epoch 303/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5119 - accuracy: 0.7491 - val_loss: 0.5311 - val_accuracy: 0.7354\n",
            "Epoch 304/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.4922 - accuracy: 0.7564 - val_loss: 0.5397 - val_accuracy: 0.7314\n",
            "Epoch 305/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.4964 - accuracy: 0.7654 - val_loss: 0.5474 - val_accuracy: 0.7305\n",
            "Epoch 306/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.5158 - accuracy: 0.7520 - val_loss: 0.5377 - val_accuracy: 0.7451\n",
            "Epoch 307/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4825 - accuracy: 0.7652 - val_loss: 0.5352 - val_accuracy: 0.7461\n",
            "Epoch 308/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.5265 - accuracy: 0.7336 - val_loss: 0.5338 - val_accuracy: 0.7373\n",
            "Epoch 309/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5149 - accuracy: 0.7446 - val_loss: 0.5305 - val_accuracy: 0.7344\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00309: early stopping\n",
            "\n",
            "Accuracy: 73.44%\n",
            "Loss: 0.5305382013320923 \n",
            "\n",
            "NEXT SET OF HYPERPARAMETERS IS: \n",
            " num_conv_layers: 2 \n",
            " num_conv_nodes: 242 \n",
            " num_dense_layers: 3 \n",
            " num_dense_nodes: 111 \n",
            "\n",
            "Epoch 1/500\n",
            "  1/128 [..............................] - ETA: 1:32 - loss: 3.1457 - accuracy: 0.0938WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0033s vs `on_train_batch_end` time: 0.0051s). Check your callbacks.\n",
            "128/128 [==============================] - 2s 12ms/step - loss: 2.5685 - accuracy: 0.3428 - val_loss: 2.7818 - val_accuracy: 0.1621\n",
            "Epoch 2/500\n",
            "128/128 [==============================] - 1s 11ms/step - loss: 1.0167 - accuracy: 0.5567 - val_loss: 3.4988 - val_accuracy: 0.1016\n",
            "Epoch 3/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.8831 - accuracy: 0.6060 - val_loss: 4.3411 - val_accuracy: 0.1279\n",
            "Epoch 4/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.8367 - accuracy: 0.6012 - val_loss: 3.4203 - val_accuracy: 0.2236\n",
            "Epoch 5/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.8319 - accuracy: 0.6101 - val_loss: 1.0221 - val_accuracy: 0.5420\n",
            "Epoch 6/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.8247 - accuracy: 0.6221 - val_loss: 0.8314 - val_accuracy: 0.6260\n",
            "Epoch 7/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.8089 - accuracy: 0.6152 - val_loss: 0.8482 - val_accuracy: 0.5830\n",
            "Epoch 8/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.8144 - accuracy: 0.6277 - val_loss: 0.7655 - val_accuracy: 0.6387\n",
            "Epoch 9/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7481 - accuracy: 0.6516 - val_loss: 0.7694 - val_accuracy: 0.6396\n",
            "Epoch 10/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7916 - accuracy: 0.6353 - val_loss: 0.7351 - val_accuracy: 0.6514\n",
            "Epoch 11/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7565 - accuracy: 0.6481 - val_loss: 0.8499 - val_accuracy: 0.6045\n",
            "Epoch 12/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7415 - accuracy: 0.6491 - val_loss: 0.7420 - val_accuracy: 0.6582\n",
            "Epoch 13/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7380 - accuracy: 0.6525 - val_loss: 0.7247 - val_accuracy: 0.6719\n",
            "Epoch 14/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7138 - accuracy: 0.6678 - val_loss: 0.8001 - val_accuracy: 0.6172\n",
            "Epoch 15/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7529 - accuracy: 0.6496 - val_loss: 0.7493 - val_accuracy: 0.6641\n",
            "Epoch 16/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7531 - accuracy: 0.6481 - val_loss: 0.7276 - val_accuracy: 0.6416\n",
            "Epoch 17/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7366 - accuracy: 0.6472 - val_loss: 0.7201 - val_accuracy: 0.6650\n",
            "Epoch 18/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7267 - accuracy: 0.6560 - val_loss: 0.7495 - val_accuracy: 0.6602\n",
            "Epoch 19/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7036 - accuracy: 0.6742 - val_loss: 0.7157 - val_accuracy: 0.6738\n",
            "Epoch 20/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6970 - accuracy: 0.6798 - val_loss: 0.7478 - val_accuracy: 0.6426\n",
            "Epoch 21/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7194 - accuracy: 0.6630 - val_loss: 0.7164 - val_accuracy: 0.6611\n",
            "Epoch 22/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7091 - accuracy: 0.6725 - val_loss: 0.7126 - val_accuracy: 0.6543\n",
            "Epoch 23/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6811 - accuracy: 0.6754 - val_loss: 0.7461 - val_accuracy: 0.6914\n",
            "Epoch 24/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6873 - accuracy: 0.7054 - val_loss: 0.7593 - val_accuracy: 0.6738\n",
            "Epoch 25/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6964 - accuracy: 0.6936 - val_loss: 0.7191 - val_accuracy: 0.6387\n",
            "Epoch 26/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6580 - accuracy: 0.7117 - val_loss: 0.7634 - val_accuracy: 0.6582\n",
            "Epoch 27/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6806 - accuracy: 0.7083 - val_loss: 0.8822 - val_accuracy: 0.6055\n",
            "Epoch 28/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6141 - accuracy: 0.7475 - val_loss: 0.8404 - val_accuracy: 0.5674\n",
            "Epoch 29/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6330 - accuracy: 0.7257 - val_loss: 0.8858 - val_accuracy: 0.5723\n",
            "Epoch 30/500\n",
            "128/128 [==============================] - 1s 11ms/step - loss: 0.6982 - accuracy: 0.6978 - val_loss: 1.2299 - val_accuracy: 0.4619\n",
            "Epoch 31/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6731 - accuracy: 0.7146 - val_loss: 3.8601 - val_accuracy: 0.1309\n",
            "Epoch 32/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7244 - accuracy: 0.6842 - val_loss: 2.3610 - val_accuracy: 0.1963\n",
            "Epoch 33/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7658 - accuracy: 0.6466 - val_loss: 1.1564 - val_accuracy: 0.4932\n",
            "Epoch 34/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7415 - accuracy: 0.6554 - val_loss: 0.8881 - val_accuracy: 0.6123\n",
            "Epoch 35/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7260 - accuracy: 0.6636 - val_loss: 0.8618 - val_accuracy: 0.6074\n",
            "Epoch 36/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7324 - accuracy: 0.6435 - val_loss: 1.0600 - val_accuracy: 0.5508\n",
            "Epoch 37/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7249 - accuracy: 0.6545 - val_loss: 0.7449 - val_accuracy: 0.6572\n",
            "Epoch 38/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7153 - accuracy: 0.6506 - val_loss: 0.7579 - val_accuracy: 0.6709\n",
            "Epoch 39/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7175 - accuracy: 0.6621 - val_loss: 1.0314 - val_accuracy: 0.6230\n",
            "Epoch 40/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6780 - accuracy: 0.7072 - val_loss: 0.7552 - val_accuracy: 0.6406\n",
            "Epoch 41/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7422 - accuracy: 0.6529 - val_loss: 2.2991 - val_accuracy: 0.3799\n",
            "Epoch 42/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7025 - accuracy: 0.6801 - val_loss: 0.7261 - val_accuracy: 0.6602\n",
            "Epoch 43/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7026 - accuracy: 0.6769 - val_loss: 1.5569 - val_accuracy: 0.5371\n",
            "Epoch 44/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6870 - accuracy: 0.6908 - val_loss: 2.1923 - val_accuracy: 0.2070\n",
            "Epoch 45/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7375 - accuracy: 0.6526 - val_loss: 0.7640 - val_accuracy: 0.6221\n",
            "Epoch 46/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7171 - accuracy: 0.6612 - val_loss: 0.8308 - val_accuracy: 0.6084\n",
            "Epoch 47/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7037 - accuracy: 0.6708 - val_loss: 0.7170 - val_accuracy: 0.6641\n",
            "Epoch 48/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6889 - accuracy: 0.6630 - val_loss: 0.7430 - val_accuracy: 0.6602\n",
            "Epoch 49/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6990 - accuracy: 0.6685 - val_loss: 2.6998 - val_accuracy: 0.3721\n",
            "Epoch 50/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7011 - accuracy: 0.6618 - val_loss: 0.7077 - val_accuracy: 0.6621\n",
            "Epoch 51/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7180 - accuracy: 0.6465 - val_loss: 0.8002 - val_accuracy: 0.6289\n",
            "Epoch 52/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7044 - accuracy: 0.6608 - val_loss: 0.7348 - val_accuracy: 0.6309\n",
            "Epoch 53/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6957 - accuracy: 0.6783 - val_loss: 0.7435 - val_accuracy: 0.6572\n",
            "Epoch 54/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7052 - accuracy: 0.6615 - val_loss: 0.8116 - val_accuracy: 0.6191\n",
            "Epoch 55/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7310 - accuracy: 0.6529 - val_loss: 0.9295 - val_accuracy: 0.5928\n",
            "Epoch 56/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6849 - accuracy: 0.6728 - val_loss: 0.7120 - val_accuracy: 0.6631\n",
            "Epoch 57/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7281 - accuracy: 0.6473 - val_loss: 0.6982 - val_accuracy: 0.6768\n",
            "Epoch 58/500\n",
            "128/128 [==============================] - 1s 11ms/step - loss: 0.7176 - accuracy: 0.6636 - val_loss: 0.8713 - val_accuracy: 0.6992\n",
            "Epoch 59/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6669 - accuracy: 0.6850 - val_loss: 0.7720 - val_accuracy: 0.6318\n",
            "Epoch 60/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7089 - accuracy: 0.6557 - val_loss: 0.7526 - val_accuracy: 0.6436\n",
            "Epoch 61/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6953 - accuracy: 0.6745 - val_loss: 0.7034 - val_accuracy: 0.6807\n",
            "Epoch 62/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7134 - accuracy: 0.6667 - val_loss: 0.7259 - val_accuracy: 0.6484\n",
            "Epoch 63/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6856 - accuracy: 0.6660 - val_loss: 0.7068 - val_accuracy: 0.6475\n",
            "Epoch 64/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7030 - accuracy: 0.6618 - val_loss: 0.9352 - val_accuracy: 0.5615\n",
            "Epoch 65/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6803 - accuracy: 0.6659 - val_loss: 0.7119 - val_accuracy: 0.6670\n",
            "Epoch 66/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6691 - accuracy: 0.6750 - val_loss: 0.7158 - val_accuracy: 0.6729\n",
            "Epoch 67/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6923 - accuracy: 0.6635 - val_loss: 0.7053 - val_accuracy: 0.6670\n",
            "Epoch 68/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6950 - accuracy: 0.6639 - val_loss: 0.7025 - val_accuracy: 0.6924\n",
            "Epoch 69/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7033 - accuracy: 0.6537 - val_loss: 0.7275 - val_accuracy: 0.6377\n",
            "Epoch 70/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6975 - accuracy: 0.6538 - val_loss: 0.7142 - val_accuracy: 0.6680\n",
            "Epoch 71/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6811 - accuracy: 0.6787 - val_loss: 0.7580 - val_accuracy: 0.6357\n",
            "Epoch 72/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6882 - accuracy: 0.6674 - val_loss: 0.7281 - val_accuracy: 0.6445\n",
            "Epoch 73/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6844 - accuracy: 0.6789 - val_loss: 0.7259 - val_accuracy: 0.6631\n",
            "Epoch 74/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6861 - accuracy: 0.6811 - val_loss: 0.7131 - val_accuracy: 0.6797\n",
            "Epoch 75/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7083 - accuracy: 0.6622 - val_loss: 0.7497 - val_accuracy: 0.6445\n",
            "Epoch 76/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6859 - accuracy: 0.6721 - val_loss: 0.6941 - val_accuracy: 0.6738\n",
            "Epoch 77/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7089 - accuracy: 0.6577 - val_loss: 2.1243 - val_accuracy: 0.4082\n",
            "Epoch 78/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7455 - accuracy: 0.6434 - val_loss: 0.7241 - val_accuracy: 0.6699\n",
            "Epoch 79/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6800 - accuracy: 0.6737 - val_loss: 0.7249 - val_accuracy: 0.6621\n",
            "Epoch 80/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7053 - accuracy: 0.6647 - val_loss: 0.7359 - val_accuracy: 0.6514\n",
            "Epoch 81/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6996 - accuracy: 0.6577 - val_loss: 0.7024 - val_accuracy: 0.6748\n",
            "Epoch 82/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6709 - accuracy: 0.6858 - val_loss: 0.7112 - val_accuracy: 0.6650\n",
            "Epoch 83/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6763 - accuracy: 0.6734 - val_loss: 0.7135 - val_accuracy: 0.6719\n",
            "Epoch 84/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6910 - accuracy: 0.6693 - val_loss: 0.7083 - val_accuracy: 0.6572\n",
            "Epoch 85/500\n",
            "128/128 [==============================] - 1s 11ms/step - loss: 0.6812 - accuracy: 0.6667 - val_loss: 0.7074 - val_accuracy: 0.6660\n",
            "Epoch 86/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7119 - accuracy: 0.6571 - val_loss: 0.7256 - val_accuracy: 0.6455\n",
            "Epoch 87/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6543 - accuracy: 0.6853 - val_loss: 0.6867 - val_accuracy: 0.6895\n",
            "Epoch 88/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6847 - accuracy: 0.6666 - val_loss: 0.7221 - val_accuracy: 0.6895\n",
            "Epoch 89/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6894 - accuracy: 0.6602 - val_loss: 0.7298 - val_accuracy: 0.6641\n",
            "Epoch 90/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6956 - accuracy: 0.6560 - val_loss: 0.7111 - val_accuracy: 0.6436\n",
            "Epoch 91/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6766 - accuracy: 0.6709 - val_loss: 0.6969 - val_accuracy: 0.6807\n",
            "Epoch 92/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6916 - accuracy: 0.6610 - val_loss: 0.7125 - val_accuracy: 0.6865\n",
            "Epoch 93/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6690 - accuracy: 0.6850 - val_loss: 0.7018 - val_accuracy: 0.6689\n",
            "Epoch 94/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6762 - accuracy: 0.6793 - val_loss: 0.6938 - val_accuracy: 0.6885\n",
            "Epoch 95/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6995 - accuracy: 0.6626 - val_loss: 0.6925 - val_accuracy: 0.6855\n",
            "Epoch 96/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7193 - accuracy: 0.6522 - val_loss: 0.6873 - val_accuracy: 0.6875\n",
            "Epoch 97/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6809 - accuracy: 0.6688 - val_loss: 0.7015 - val_accuracy: 0.6631\n",
            "Epoch 98/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6906 - accuracy: 0.6595 - val_loss: 0.7108 - val_accuracy: 0.6758\n",
            "Epoch 99/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6792 - accuracy: 0.6702 - val_loss: 0.7197 - val_accuracy: 0.6533\n",
            "Epoch 100/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6806 - accuracy: 0.6668 - val_loss: 0.7246 - val_accuracy: 0.6719\n",
            "Epoch 101/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6845 - accuracy: 0.6718 - val_loss: 0.6948 - val_accuracy: 0.6797\n",
            "Epoch 102/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6722 - accuracy: 0.6691 - val_loss: 0.6948 - val_accuracy: 0.6719\n",
            "Epoch 103/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6706 - accuracy: 0.6814 - val_loss: 0.7940 - val_accuracy: 0.6289\n",
            "Epoch 104/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6886 - accuracy: 0.6665 - val_loss: 0.7325 - val_accuracy: 0.6533\n",
            "Epoch 105/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6828 - accuracy: 0.6714 - val_loss: 0.7244 - val_accuracy: 0.6445\n",
            "Epoch 106/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6963 - accuracy: 0.6638 - val_loss: 0.6910 - val_accuracy: 0.6699\n",
            "Epoch 107/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6681 - accuracy: 0.6797 - val_loss: 0.7087 - val_accuracy: 0.6650\n",
            "Epoch 108/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6822 - accuracy: 0.6719 - val_loss: 0.6754 - val_accuracy: 0.6816\n",
            "Epoch 109/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6828 - accuracy: 0.6668 - val_loss: 0.7081 - val_accuracy: 0.6689\n",
            "Epoch 110/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6821 - accuracy: 0.6868 - val_loss: 0.6937 - val_accuracy: 0.7129\n",
            "Epoch 111/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6757 - accuracy: 0.6858 - val_loss: 0.6995 - val_accuracy: 0.6699\n",
            "Epoch 112/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6852 - accuracy: 0.6745 - val_loss: 0.6879 - val_accuracy: 0.6807\n",
            "Epoch 113/500\n",
            "128/128 [==============================] - 1s 11ms/step - loss: 0.6822 - accuracy: 0.6713 - val_loss: 0.6955 - val_accuracy: 0.6689\n",
            "Epoch 114/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6700 - accuracy: 0.6820 - val_loss: 0.7120 - val_accuracy: 0.6543\n",
            "Epoch 115/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6752 - accuracy: 0.6683 - val_loss: 0.7013 - val_accuracy: 0.6729\n",
            "Epoch 116/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6866 - accuracy: 0.6701 - val_loss: 0.7049 - val_accuracy: 0.6709\n",
            "Epoch 117/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6897 - accuracy: 0.6814 - val_loss: 0.6847 - val_accuracy: 0.6475\n",
            "Epoch 118/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6448 - accuracy: 0.7234 - val_loss: 0.7146 - val_accuracy: 0.6738\n",
            "Epoch 119/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6959 - accuracy: 0.7096 - val_loss: 0.7214 - val_accuracy: 0.6670\n",
            "Epoch 120/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6735 - accuracy: 0.6727 - val_loss: 0.7129 - val_accuracy: 0.6641\n",
            "Epoch 121/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7006 - accuracy: 0.6467 - val_loss: 0.7233 - val_accuracy: 0.6377\n",
            "Epoch 122/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6866 - accuracy: 0.6618 - val_loss: 0.7148 - val_accuracy: 0.6641\n",
            "Epoch 123/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7087 - accuracy: 0.6533 - val_loss: 0.6878 - val_accuracy: 0.6855\n",
            "Epoch 124/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6784 - accuracy: 0.6720 - val_loss: 0.6923 - val_accuracy: 0.6768\n",
            "Epoch 125/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6683 - accuracy: 0.6807 - val_loss: 0.7007 - val_accuracy: 0.6895\n",
            "Epoch 126/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6622 - accuracy: 0.6814 - val_loss: 0.6859 - val_accuracy: 0.6758\n",
            "Epoch 127/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6726 - accuracy: 0.6774 - val_loss: 0.6900 - val_accuracy: 0.6660\n",
            "Epoch 128/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6680 - accuracy: 0.6844 - val_loss: 0.7165 - val_accuracy: 0.6641\n",
            "Epoch 129/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6886 - accuracy: 0.6688 - val_loss: 0.6871 - val_accuracy: 0.6768\n",
            "Epoch 130/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6787 - accuracy: 0.6725 - val_loss: 0.7205 - val_accuracy: 0.6494\n",
            "Epoch 131/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6664 - accuracy: 0.6735 - val_loss: 0.7193 - val_accuracy: 0.6523\n",
            "Epoch 132/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6816 - accuracy: 0.6723 - val_loss: 0.7201 - val_accuracy: 0.6602\n",
            "Epoch 133/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6672 - accuracy: 0.6757 - val_loss: 0.6977 - val_accuracy: 0.6777\n",
            "Epoch 134/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6742 - accuracy: 0.6770 - val_loss: 0.6891 - val_accuracy: 0.6709\n",
            "Epoch 135/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6735 - accuracy: 0.6783 - val_loss: 0.6803 - val_accuracy: 0.6787\n",
            "Epoch 136/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6871 - accuracy: 0.6750 - val_loss: 0.6820 - val_accuracy: 0.6846\n",
            "Epoch 137/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6552 - accuracy: 0.6859 - val_loss: 0.6890 - val_accuracy: 0.6748\n",
            "Epoch 138/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6629 - accuracy: 0.6730 - val_loss: 0.6894 - val_accuracy: 0.6768\n",
            "Epoch 139/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6998 - accuracy: 0.6538 - val_loss: 0.7012 - val_accuracy: 0.6621\n",
            "Epoch 140/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7038 - accuracy: 0.6577 - val_loss: 0.7137 - val_accuracy: 0.6748\n",
            "Epoch 141/500\n",
            "128/128 [==============================] - 1s 11ms/step - loss: 0.6838 - accuracy: 0.6692 - val_loss: 0.6872 - val_accuracy: 0.6826\n",
            "Epoch 142/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6693 - accuracy: 0.6733 - val_loss: 0.8255 - val_accuracy: 0.6270\n",
            "Epoch 143/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6627 - accuracy: 0.6833 - val_loss: 0.7034 - val_accuracy: 0.6865\n",
            "Epoch 144/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6723 - accuracy: 0.6794 - val_loss: 0.7487 - val_accuracy: 0.6377\n",
            "Epoch 145/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6688 - accuracy: 0.6795 - val_loss: 0.6993 - val_accuracy: 0.6738\n",
            "Epoch 146/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6524 - accuracy: 0.6874 - val_loss: 0.6932 - val_accuracy: 0.6680\n",
            "Epoch 147/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6905 - accuracy: 0.6628 - val_loss: 0.6911 - val_accuracy: 0.6719\n",
            "Epoch 148/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6860 - accuracy: 0.6690 - val_loss: 0.6877 - val_accuracy: 0.6768\n",
            "Epoch 149/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6742 - accuracy: 0.6781 - val_loss: 0.6813 - val_accuracy: 0.6943\n",
            "Epoch 150/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6531 - accuracy: 0.6902 - val_loss: 0.7148 - val_accuracy: 0.6650\n",
            "Epoch 151/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6682 - accuracy: 0.6715 - val_loss: 0.6922 - val_accuracy: 0.6719\n",
            "Epoch 152/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6554 - accuracy: 0.6788 - val_loss: 0.6985 - val_accuracy: 0.6758\n",
            "Epoch 153/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6310 - accuracy: 0.6862 - val_loss: 0.7199 - val_accuracy: 0.6543\n",
            "Epoch 154/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6548 - accuracy: 0.6823 - val_loss: 0.6892 - val_accuracy: 0.6885\n",
            "Epoch 155/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6534 - accuracy: 0.6815 - val_loss: 0.6707 - val_accuracy: 0.6865\n",
            "Epoch 156/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6434 - accuracy: 0.6875 - val_loss: 0.7095 - val_accuracy: 0.6592\n",
            "Epoch 157/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6629 - accuracy: 0.6680 - val_loss: 0.6846 - val_accuracy: 0.6826\n",
            "Epoch 158/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6789 - accuracy: 0.6614 - val_loss: 0.6921 - val_accuracy: 0.6758\n",
            "Epoch 159/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6665 - accuracy: 0.6813 - val_loss: 0.7075 - val_accuracy: 0.6807\n",
            "Epoch 160/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6562 - accuracy: 0.6869 - val_loss: 0.6819 - val_accuracy: 0.6797\n",
            "Epoch 161/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6759 - accuracy: 0.6736 - val_loss: 0.6810 - val_accuracy: 0.6816\n",
            "Epoch 162/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6486 - accuracy: 0.6886 - val_loss: 0.7123 - val_accuracy: 0.6738\n",
            "Epoch 163/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6717 - accuracy: 0.6678 - val_loss: 0.6780 - val_accuracy: 0.6680\n",
            "Epoch 164/500\n",
            "128/128 [==============================] - 1s 11ms/step - loss: 0.6637 - accuracy: 0.6808 - val_loss: 0.6791 - val_accuracy: 0.6787\n",
            "Epoch 165/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6692 - accuracy: 0.6866 - val_loss: 0.6838 - val_accuracy: 0.6826\n",
            "Epoch 166/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6751 - accuracy: 0.6735 - val_loss: 0.6841 - val_accuracy: 0.6748\n",
            "Epoch 167/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6415 - accuracy: 0.6986 - val_loss: 0.6845 - val_accuracy: 0.6797\n",
            "Epoch 168/500\n",
            "128/128 [==============================] - 1s 11ms/step - loss: 0.6759 - accuracy: 0.6627 - val_loss: 0.6883 - val_accuracy: 0.6787\n",
            "Epoch 169/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6601 - accuracy: 0.6767 - val_loss: 0.6887 - val_accuracy: 0.6689\n",
            "Epoch 170/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6642 - accuracy: 0.6744 - val_loss: 0.7062 - val_accuracy: 0.6680\n",
            "Epoch 171/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6631 - accuracy: 0.6826 - val_loss: 0.7042 - val_accuracy: 0.6699\n",
            "Epoch 172/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6578 - accuracy: 0.6752 - val_loss: 0.6895 - val_accuracy: 0.6816\n",
            "Epoch 173/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6530 - accuracy: 0.6884 - val_loss: 0.6923 - val_accuracy: 0.6748\n",
            "Epoch 174/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6659 - accuracy: 0.6645 - val_loss: 0.6867 - val_accuracy: 0.6729\n",
            "Epoch 175/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6768 - accuracy: 0.6814 - val_loss: 0.6815 - val_accuracy: 0.6865\n",
            "Epoch 176/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6269 - accuracy: 0.6944 - val_loss: 0.6972 - val_accuracy: 0.6777\n",
            "Epoch 177/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6704 - accuracy: 0.6822 - val_loss: 0.6809 - val_accuracy: 0.6826\n",
            "Epoch 178/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6867 - accuracy: 0.6733 - val_loss: 0.7035 - val_accuracy: 0.6484\n",
            "Epoch 179/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6606 - accuracy: 0.6862 - val_loss: 0.7242 - val_accuracy: 0.6621\n",
            "Epoch 180/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6630 - accuracy: 0.6704 - val_loss: 0.6849 - val_accuracy: 0.6895\n",
            "Epoch 181/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6524 - accuracy: 0.6842 - val_loss: 0.6907 - val_accuracy: 0.7295\n",
            "Epoch 182/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6376 - accuracy: 0.7015 - val_loss: 0.7013 - val_accuracy: 0.6670\n",
            "Epoch 183/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6675 - accuracy: 0.6870 - val_loss: 0.7009 - val_accuracy: 0.6777\n",
            "Epoch 184/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6601 - accuracy: 0.6781 - val_loss: 0.6833 - val_accuracy: 0.6787\n",
            "Epoch 185/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6630 - accuracy: 0.6832 - val_loss: 0.6919 - val_accuracy: 0.6797\n",
            "Epoch 186/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6633 - accuracy: 0.6821 - val_loss: 0.6850 - val_accuracy: 0.6787\n",
            "Epoch 187/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6458 - accuracy: 0.6798 - val_loss: 0.7032 - val_accuracy: 0.6562\n",
            "Epoch 188/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6609 - accuracy: 0.6731 - val_loss: 0.6770 - val_accuracy: 0.6924\n",
            "Epoch 189/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6400 - accuracy: 0.6880 - val_loss: 0.6768 - val_accuracy: 0.6855\n",
            "Epoch 190/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6650 - accuracy: 0.6741 - val_loss: 0.6914 - val_accuracy: 0.6797\n",
            "Epoch 191/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6772 - accuracy: 0.6766 - val_loss: 0.6916 - val_accuracy: 0.6719\n",
            "Epoch 192/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6774 - accuracy: 0.6815 - val_loss: 0.6741 - val_accuracy: 0.6865\n",
            "Epoch 193/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6476 - accuracy: 0.6880 - val_loss: 0.6989 - val_accuracy: 0.6719\n",
            "Epoch 194/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6496 - accuracy: 0.6838 - val_loss: 0.6938 - val_accuracy: 0.6650\n",
            "Epoch 195/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6632 - accuracy: 0.6745 - val_loss: 0.7077 - val_accuracy: 0.6660\n",
            "Epoch 196/500\n",
            "128/128 [==============================] - 1s 11ms/step - loss: 0.6488 - accuracy: 0.6843 - val_loss: 0.6975 - val_accuracy: 0.6621\n",
            "Epoch 197/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6571 - accuracy: 0.6867 - val_loss: 0.6858 - val_accuracy: 0.6885\n",
            "Epoch 198/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6634 - accuracy: 0.6808 - val_loss: 0.6958 - val_accuracy: 0.6797\n",
            "Epoch 199/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6667 - accuracy: 0.6792 - val_loss: 0.6822 - val_accuracy: 0.6807\n",
            "Epoch 200/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6596 - accuracy: 0.6792 - val_loss: 0.7212 - val_accuracy: 0.6611\n",
            "Epoch 201/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6790 - accuracy: 0.6894 - val_loss: 0.6889 - val_accuracy: 0.6768\n",
            "Epoch 202/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6470 - accuracy: 0.6957 - val_loss: 0.6807 - val_accuracy: 0.6836\n",
            "Epoch 203/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6499 - accuracy: 0.6813 - val_loss: 0.6744 - val_accuracy: 0.6846\n",
            "Epoch 204/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6482 - accuracy: 0.6893 - val_loss: 0.6847 - val_accuracy: 0.6777\n",
            "Epoch 205/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6449 - accuracy: 0.6995 - val_loss: 0.6759 - val_accuracy: 0.6807\n",
            "Epoch 206/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6563 - accuracy: 0.6822 - val_loss: 0.6790 - val_accuracy: 0.6777\n",
            "Epoch 207/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6489 - accuracy: 0.6772 - val_loss: 0.6900 - val_accuracy: 0.6641\n",
            "Epoch 208/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6437 - accuracy: 0.6891 - val_loss: 0.6980 - val_accuracy: 0.6826\n",
            "Epoch 209/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6680 - accuracy: 0.6771 - val_loss: 0.7004 - val_accuracy: 0.6777\n",
            "Epoch 210/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6617 - accuracy: 0.6833 - val_loss: 0.6959 - val_accuracy: 0.6572\n",
            "Epoch 211/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6598 - accuracy: 0.6795 - val_loss: 0.6821 - val_accuracy: 0.6777\n",
            "Epoch 212/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6616 - accuracy: 0.6855 - val_loss: 0.6729 - val_accuracy: 0.6768\n",
            "Epoch 213/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6414 - accuracy: 0.6962 - val_loss: 0.6930 - val_accuracy: 0.6602\n",
            "Epoch 214/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6211 - accuracy: 0.7017 - val_loss: 0.6727 - val_accuracy: 0.6738\n",
            "Epoch 215/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6473 - accuracy: 0.6820 - val_loss: 0.6917 - val_accuracy: 0.6777\n",
            "Epoch 216/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6463 - accuracy: 0.6964 - val_loss: 0.6731 - val_accuracy: 0.6768\n",
            "Epoch 217/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6464 - accuracy: 0.6849 - val_loss: 0.6837 - val_accuracy: 0.6963\n",
            "Epoch 218/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6587 - accuracy: 0.6757 - val_loss: 0.6852 - val_accuracy: 0.6787\n",
            "Epoch 219/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6617 - accuracy: 0.6818 - val_loss: 0.6915 - val_accuracy: 0.6846\n",
            "Epoch 220/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6866 - accuracy: 0.6656 - val_loss: 0.6783 - val_accuracy: 0.6973\n",
            "Epoch 221/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6268 - accuracy: 0.6994 - val_loss: 0.6881 - val_accuracy: 0.6719\n",
            "Epoch 222/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6335 - accuracy: 0.6872 - val_loss: 0.6877 - val_accuracy: 0.6611\n",
            "Epoch 223/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6461 - accuracy: 0.6818 - val_loss: 0.6819 - val_accuracy: 0.6768\n",
            "Epoch 224/500\n",
            "128/128 [==============================] - 1s 11ms/step - loss: 0.6266 - accuracy: 0.7011 - val_loss: 0.6535 - val_accuracy: 0.6992\n",
            "Epoch 225/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6713 - accuracy: 0.6802 - val_loss: 0.6764 - val_accuracy: 0.6807\n",
            "Epoch 226/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6477 - accuracy: 0.6848 - val_loss: 0.6702 - val_accuracy: 0.6865\n",
            "Epoch 227/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6400 - accuracy: 0.6930 - val_loss: 0.6770 - val_accuracy: 0.6973\n",
            "Epoch 228/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6493 - accuracy: 0.6945 - val_loss: 0.6730 - val_accuracy: 0.6816\n",
            "Epoch 229/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6372 - accuracy: 0.6967 - val_loss: 0.6822 - val_accuracy: 0.6816\n",
            "Epoch 230/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6329 - accuracy: 0.6947 - val_loss: 0.6606 - val_accuracy: 0.6885\n",
            "Epoch 231/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6345 - accuracy: 0.7002 - val_loss: 0.6754 - val_accuracy: 0.6816\n",
            "Epoch 232/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6743 - accuracy: 0.6720 - val_loss: 0.6697 - val_accuracy: 0.6865\n",
            "Epoch 233/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6573 - accuracy: 0.6824 - val_loss: 0.6782 - val_accuracy: 0.6924\n",
            "Epoch 234/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6415 - accuracy: 0.6935 - val_loss: 0.6774 - val_accuracy: 0.6895\n",
            "Epoch 235/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6425 - accuracy: 0.6938 - val_loss: 0.6682 - val_accuracy: 0.6885\n",
            "Epoch 236/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6465 - accuracy: 0.6927 - val_loss: 0.6641 - val_accuracy: 0.6865\n",
            "Epoch 237/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6298 - accuracy: 0.6973 - val_loss: 0.6859 - val_accuracy: 0.6758\n",
            "Epoch 238/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6300 - accuracy: 0.6946 - val_loss: 0.6514 - val_accuracy: 0.6963\n",
            "Epoch 239/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6080 - accuracy: 0.7084 - val_loss: 0.6734 - val_accuracy: 0.6914\n",
            "Epoch 240/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6361 - accuracy: 0.6949 - val_loss: 0.6916 - val_accuracy: 0.6846\n",
            "Epoch 241/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6346 - accuracy: 0.6919 - val_loss: 0.6980 - val_accuracy: 0.6846\n",
            "Epoch 242/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6537 - accuracy: 0.6906 - val_loss: 0.6556 - val_accuracy: 0.6953\n",
            "Epoch 243/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6145 - accuracy: 0.7009 - val_loss: 0.6660 - val_accuracy: 0.6924\n",
            "Epoch 244/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6514 - accuracy: 0.6870 - val_loss: 0.6728 - val_accuracy: 0.6953\n",
            "Epoch 245/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6500 - accuracy: 0.6849 - val_loss: 0.6682 - val_accuracy: 0.7002\n",
            "Epoch 246/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6526 - accuracy: 0.6871 - val_loss: 0.6798 - val_accuracy: 0.6738\n",
            "Epoch 247/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6353 - accuracy: 0.6965 - val_loss: 0.6547 - val_accuracy: 0.6816\n",
            "Epoch 248/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6613 - accuracy: 0.6875 - val_loss: 0.6911 - val_accuracy: 0.6572\n",
            "Epoch 249/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6573 - accuracy: 0.6894 - val_loss: 0.6580 - val_accuracy: 0.6895\n",
            "Epoch 250/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6500 - accuracy: 0.6935 - val_loss: 0.6732 - val_accuracy: 0.6904\n",
            "Epoch 251/500\n",
            "128/128 [==============================] - 1s 11ms/step - loss: 0.6404 - accuracy: 0.6821 - val_loss: 0.6808 - val_accuracy: 0.6875\n",
            "Epoch 252/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6579 - accuracy: 0.6783 - val_loss: 0.6970 - val_accuracy: 0.6602\n",
            "Epoch 253/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6256 - accuracy: 0.7074 - val_loss: 0.6762 - val_accuracy: 0.6816\n",
            "Epoch 254/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6228 - accuracy: 0.7006 - val_loss: 0.6770 - val_accuracy: 0.6729\n",
            "Epoch 255/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6454 - accuracy: 0.6909 - val_loss: 0.6410 - val_accuracy: 0.6934\n",
            "Epoch 256/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6409 - accuracy: 0.6845 - val_loss: 0.6546 - val_accuracy: 0.6777\n",
            "Epoch 257/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6361 - accuracy: 0.6864 - val_loss: 0.6723 - val_accuracy: 0.6953\n",
            "Epoch 258/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6264 - accuracy: 0.6956 - val_loss: 0.6586 - val_accuracy: 0.6816\n",
            "Epoch 259/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6365 - accuracy: 0.6813 - val_loss: 0.6819 - val_accuracy: 0.6709\n",
            "Epoch 260/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6121 - accuracy: 0.7035 - val_loss: 0.6624 - val_accuracy: 0.6768\n",
            "Epoch 261/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6245 - accuracy: 0.7023 - val_loss: 0.6548 - val_accuracy: 0.6846\n",
            "Epoch 262/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6219 - accuracy: 0.6984 - val_loss: 0.6806 - val_accuracy: 0.6953\n",
            "Epoch 263/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6240 - accuracy: 0.7023 - val_loss: 0.6785 - val_accuracy: 0.6836\n",
            "Epoch 264/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6455 - accuracy: 0.6891 - val_loss: 0.6834 - val_accuracy: 0.6768\n",
            "Epoch 265/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6362 - accuracy: 0.6950 - val_loss: 0.6895 - val_accuracy: 0.6816\n",
            "Epoch 266/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6369 - accuracy: 0.6881 - val_loss: 0.6707 - val_accuracy: 0.6865\n",
            "Epoch 267/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6217 - accuracy: 0.7065 - val_loss: 0.6542 - val_accuracy: 0.6777\n",
            "Epoch 268/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6377 - accuracy: 0.7048 - val_loss: 0.6683 - val_accuracy: 0.6777\n",
            "Epoch 269/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6047 - accuracy: 0.7088 - val_loss: 0.6713 - val_accuracy: 0.6787\n",
            "Epoch 270/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6300 - accuracy: 0.6899 - val_loss: 0.6697 - val_accuracy: 0.6904\n",
            "Epoch 271/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6069 - accuracy: 0.7072 - val_loss: 0.6728 - val_accuracy: 0.6982\n",
            "Epoch 272/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6525 - accuracy: 0.6742 - val_loss: 0.6708 - val_accuracy: 0.6855\n",
            "Epoch 273/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6139 - accuracy: 0.7015 - val_loss: 0.6825 - val_accuracy: 0.6846\n",
            "Epoch 274/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6242 - accuracy: 0.7052 - val_loss: 0.6676 - val_accuracy: 0.6924\n",
            "Epoch 275/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6275 - accuracy: 0.6974 - val_loss: 0.6710 - val_accuracy: 0.7432\n",
            "Epoch 276/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6202 - accuracy: 0.7074 - val_loss: 0.6710 - val_accuracy: 0.6816\n",
            "Epoch 277/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6351 - accuracy: 0.7056 - val_loss: 0.6684 - val_accuracy: 0.6777\n",
            "Epoch 278/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6225 - accuracy: 0.7121 - val_loss: 0.6851 - val_accuracy: 0.6895\n",
            "Epoch 279/500\n",
            "128/128 [==============================] - 1s 11ms/step - loss: 0.6053 - accuracy: 0.7125 - val_loss: 0.6921 - val_accuracy: 0.6738\n",
            "Epoch 280/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6383 - accuracy: 0.6885 - val_loss: 0.6775 - val_accuracy: 0.6865\n",
            "Epoch 281/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6384 - accuracy: 0.6879 - val_loss: 0.6689 - val_accuracy: 0.6943\n",
            "Epoch 282/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6359 - accuracy: 0.7149 - val_loss: 0.6768 - val_accuracy: 0.6797\n",
            "Epoch 283/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6206 - accuracy: 0.7004 - val_loss: 0.6699 - val_accuracy: 0.6816\n",
            "Epoch 284/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6507 - accuracy: 0.6822 - val_loss: 0.6729 - val_accuracy: 0.6855\n",
            "Epoch 285/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6381 - accuracy: 0.6974 - val_loss: 0.6531 - val_accuracy: 0.6943\n",
            "Epoch 286/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6305 - accuracy: 0.6941 - val_loss: 0.6688 - val_accuracy: 0.6855\n",
            "Epoch 287/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6202 - accuracy: 0.7046 - val_loss: 0.6685 - val_accuracy: 0.6885\n",
            "Epoch 288/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6394 - accuracy: 0.6974 - val_loss: 0.6857 - val_accuracy: 0.6748\n",
            "Epoch 289/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6218 - accuracy: 0.7032 - val_loss: 0.6608 - val_accuracy: 0.6855\n",
            "Epoch 290/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6412 - accuracy: 0.6868 - val_loss: 0.6723 - val_accuracy: 0.7002\n",
            "Epoch 291/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6148 - accuracy: 0.7101 - val_loss: 0.6803 - val_accuracy: 0.6777\n",
            "Epoch 292/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6196 - accuracy: 0.7064 - val_loss: 0.6814 - val_accuracy: 0.6992\n",
            "Epoch 293/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6101 - accuracy: 0.7089 - val_loss: 0.6661 - val_accuracy: 0.6797\n",
            "Epoch 294/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6025 - accuracy: 0.7122 - val_loss: 0.6754 - val_accuracy: 0.6836\n",
            "Epoch 295/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6226 - accuracy: 0.6954 - val_loss: 0.6855 - val_accuracy: 0.6660\n",
            "Epoch 296/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6023 - accuracy: 0.7105 - val_loss: 0.6883 - val_accuracy: 0.6807\n",
            "Epoch 297/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6396 - accuracy: 0.6933 - val_loss: 0.6809 - val_accuracy: 0.6826\n",
            "Epoch 298/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6375 - accuracy: 0.6952 - val_loss: 0.6635 - val_accuracy: 0.6982\n",
            "Epoch 299/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6068 - accuracy: 0.7099 - val_loss: 0.6744 - val_accuracy: 0.6846\n",
            "Epoch 300/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6084 - accuracy: 0.7025 - val_loss: 0.6644 - val_accuracy: 0.6865\n",
            "Epoch 301/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6043 - accuracy: 0.7146 - val_loss: 0.6928 - val_accuracy: 0.6807\n",
            "Epoch 302/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6097 - accuracy: 0.7152 - val_loss: 0.6716 - val_accuracy: 0.6807\n",
            "Epoch 303/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6361 - accuracy: 0.6981 - val_loss: 0.6587 - val_accuracy: 0.6846\n",
            "Epoch 304/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6275 - accuracy: 0.7058 - val_loss: 0.6661 - val_accuracy: 0.6904\n",
            "Epoch 305/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6297 - accuracy: 0.7043 - val_loss: 0.6708 - val_accuracy: 0.6836\n",
            "Epoch 306/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6164 - accuracy: 0.7058 - val_loss: 0.6706 - val_accuracy: 0.6943\n",
            "Epoch 307/500\n",
            "128/128 [==============================] - 1s 11ms/step - loss: 0.6116 - accuracy: 0.7067 - val_loss: 0.6763 - val_accuracy: 0.6963\n",
            "Epoch 308/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6123 - accuracy: 0.6992 - val_loss: 0.6509 - val_accuracy: 0.6992\n",
            "Epoch 309/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6217 - accuracy: 0.6877 - val_loss: 0.6581 - val_accuracy: 0.6934\n",
            "Epoch 310/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6100 - accuracy: 0.7078 - val_loss: 0.6845 - val_accuracy: 0.6143\n",
            "Epoch 311/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6175 - accuracy: 0.7103 - val_loss: 0.6653 - val_accuracy: 0.6797\n",
            "Epoch 312/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6410 - accuracy: 0.6937 - val_loss: 0.6792 - val_accuracy: 0.6777\n",
            "Epoch 313/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6028 - accuracy: 0.7061 - val_loss: 0.6741 - val_accuracy: 0.6885\n",
            "Epoch 314/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6183 - accuracy: 0.7108 - val_loss: 0.6534 - val_accuracy: 0.7021\n",
            "Epoch 315/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6324 - accuracy: 0.6976 - val_loss: 0.7067 - val_accuracy: 0.6650\n",
            "Epoch 316/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5922 - accuracy: 0.7194 - val_loss: 0.6545 - val_accuracy: 0.7021\n",
            "Epoch 317/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5848 - accuracy: 0.7230 - val_loss: 0.6729 - val_accuracy: 0.6758\n",
            "Epoch 318/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6024 - accuracy: 0.7113 - val_loss: 0.6833 - val_accuracy: 0.6875\n",
            "Epoch 319/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6103 - accuracy: 0.7060 - val_loss: 0.6542 - val_accuracy: 0.6797\n",
            "Epoch 320/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6131 - accuracy: 0.7128 - val_loss: 0.6997 - val_accuracy: 0.6670\n",
            "Epoch 321/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6094 - accuracy: 0.7168 - val_loss: 0.6563 - val_accuracy: 0.7021\n",
            "Epoch 322/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6132 - accuracy: 0.6992 - val_loss: 0.6532 - val_accuracy: 0.6865\n",
            "Epoch 323/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6227 - accuracy: 0.6977 - val_loss: 0.6600 - val_accuracy: 0.6943\n",
            "Epoch 324/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5858 - accuracy: 0.7203 - val_loss: 0.6741 - val_accuracy: 0.6934\n",
            "Epoch 325/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5947 - accuracy: 0.7178 - val_loss: 0.6775 - val_accuracy: 0.6680\n",
            "Epoch 326/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6143 - accuracy: 0.7154 - val_loss: 0.6813 - val_accuracy: 0.6816\n",
            "Epoch 327/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6395 - accuracy: 0.6907 - val_loss: 0.6699 - val_accuracy: 0.7021\n",
            "Epoch 328/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5810 - accuracy: 0.7354 - val_loss: 0.6653 - val_accuracy: 0.6934\n",
            "Epoch 329/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5910 - accuracy: 0.7253 - val_loss: 0.6762 - val_accuracy: 0.6777\n",
            "Epoch 330/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6006 - accuracy: 0.7104 - val_loss: 0.6811 - val_accuracy: 0.6836\n",
            "Epoch 331/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5858 - accuracy: 0.7286 - val_loss: 0.6477 - val_accuracy: 0.6914\n",
            "Epoch 332/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6089 - accuracy: 0.7052 - val_loss: 0.6696 - val_accuracy: 0.6953\n",
            "Epoch 333/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5892 - accuracy: 0.7157 - val_loss: 0.6531 - val_accuracy: 0.6875\n",
            "Epoch 334/500\n",
            "128/128 [==============================] - 1s 11ms/step - loss: 0.5906 - accuracy: 0.7155 - val_loss: 0.6587 - val_accuracy: 0.6982\n",
            "Epoch 335/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5796 - accuracy: 0.7222 - val_loss: 0.7162 - val_accuracy: 0.6758\n",
            "Epoch 336/500\n",
            "128/128 [==============================] - 1s 11ms/step - loss: 0.6091 - accuracy: 0.7145 - val_loss: 0.6686 - val_accuracy: 0.6846\n",
            "Epoch 337/500\n",
            "128/128 [==============================] - 1s 11ms/step - loss: 0.5734 - accuracy: 0.7228 - val_loss: 0.6643 - val_accuracy: 0.6846\n",
            "Epoch 338/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6103 - accuracy: 0.7199 - val_loss: 0.6449 - val_accuracy: 0.6934\n",
            "Epoch 339/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5709 - accuracy: 0.7290 - val_loss: 0.6573 - val_accuracy: 0.7266\n",
            "Epoch 340/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6007 - accuracy: 0.7168 - val_loss: 0.6646 - val_accuracy: 0.6895\n",
            "Epoch 341/500\n",
            "128/128 [==============================] - 1s 11ms/step - loss: 0.5810 - accuracy: 0.7307 - val_loss: 0.6537 - val_accuracy: 0.6895\n",
            "Epoch 342/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5894 - accuracy: 0.7199 - val_loss: 0.6856 - val_accuracy: 0.7109\n",
            "Epoch 343/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6069 - accuracy: 0.7197 - val_loss: 0.6925 - val_accuracy: 0.7002\n",
            "Epoch 344/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5840 - accuracy: 0.7229 - val_loss: 0.6562 - val_accuracy: 0.7461\n",
            "Epoch 345/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5772 - accuracy: 0.7291 - val_loss: 0.6714 - val_accuracy: 0.6836\n",
            "Epoch 346/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5915 - accuracy: 0.7150 - val_loss: 0.6831 - val_accuracy: 0.6777\n",
            "Epoch 347/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5849 - accuracy: 0.7259 - val_loss: 0.6576 - val_accuracy: 0.6914\n",
            "Epoch 348/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6090 - accuracy: 0.7147 - val_loss: 0.6441 - val_accuracy: 0.6973\n",
            "Epoch 349/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5904 - accuracy: 0.7221 - val_loss: 0.6628 - val_accuracy: 0.6943\n",
            "Epoch 350/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6211 - accuracy: 0.7177 - val_loss: 0.6763 - val_accuracy: 0.6895\n",
            "Epoch 351/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5958 - accuracy: 0.7118 - val_loss: 0.6565 - val_accuracy: 0.6963\n",
            "Epoch 352/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5944 - accuracy: 0.7178 - val_loss: 0.6519 - val_accuracy: 0.6924\n",
            "Epoch 353/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5800 - accuracy: 0.7230 - val_loss: 0.6532 - val_accuracy: 0.7090\n",
            "Epoch 354/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5933 - accuracy: 0.7256 - val_loss: 0.6442 - val_accuracy: 0.6992\n",
            "Epoch 355/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5803 - accuracy: 0.7202 - val_loss: 0.6476 - val_accuracy: 0.7207\n",
            "Epoch 356/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6069 - accuracy: 0.7124 - val_loss: 0.6526 - val_accuracy: 0.7021\n",
            "Epoch 357/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5847 - accuracy: 0.7216 - val_loss: 0.6420 - val_accuracy: 0.7119\n",
            "Epoch 358/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5741 - accuracy: 0.7236 - val_loss: 0.6414 - val_accuracy: 0.7158\n",
            "Epoch 359/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5864 - accuracy: 0.7232 - val_loss: 0.6530 - val_accuracy: 0.7148\n",
            "Epoch 360/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5913 - accuracy: 0.7217 - val_loss: 0.6464 - val_accuracy: 0.6904\n",
            "Epoch 361/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5605 - accuracy: 0.7329 - val_loss: 0.6745 - val_accuracy: 0.6777\n",
            "Epoch 362/500\n",
            "128/128 [==============================] - 1s 11ms/step - loss: 0.5997 - accuracy: 0.7194 - val_loss: 0.6425 - val_accuracy: 0.6982\n",
            "Epoch 363/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5601 - accuracy: 0.7349 - val_loss: 0.6510 - val_accuracy: 0.7041\n",
            "Epoch 364/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5463 - accuracy: 0.7435 - val_loss: 0.6605 - val_accuracy: 0.7363\n",
            "Epoch 365/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5617 - accuracy: 0.7311 - val_loss: 0.6853 - val_accuracy: 0.6953\n",
            "Epoch 366/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5967 - accuracy: 0.7268 - val_loss: 0.6661 - val_accuracy: 0.6797\n",
            "Epoch 367/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5643 - accuracy: 0.7377 - val_loss: 0.6694 - val_accuracy: 0.6953\n",
            "Epoch 368/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5917 - accuracy: 0.7193 - val_loss: 0.6912 - val_accuracy: 0.6846\n",
            "Epoch 369/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5882 - accuracy: 0.7222 - val_loss: 0.6963 - val_accuracy: 0.6807\n",
            "Epoch 370/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5623 - accuracy: 0.7363 - val_loss: 0.6758 - val_accuracy: 0.6973\n",
            "Epoch 371/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5735 - accuracy: 0.7235 - val_loss: 0.6992 - val_accuracy: 0.6768\n",
            "Epoch 372/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5634 - accuracy: 0.7314 - val_loss: 0.7925 - val_accuracy: 0.6504\n",
            "Epoch 373/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5595 - accuracy: 0.7426 - val_loss: 0.6382 - val_accuracy: 0.7568\n",
            "Epoch 374/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5831 - accuracy: 0.7247 - val_loss: 0.6372 - val_accuracy: 0.7510\n",
            "Epoch 375/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5501 - accuracy: 0.7511 - val_loss: 0.8290 - val_accuracy: 0.6484\n",
            "Epoch 376/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5889 - accuracy: 0.7204 - val_loss: 0.6870 - val_accuracy: 0.6699\n",
            "Epoch 377/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5664 - accuracy: 0.7435 - val_loss: 0.6200 - val_accuracy: 0.7715\n",
            "Epoch 378/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5512 - accuracy: 0.7418 - val_loss: 0.6330 - val_accuracy: 0.7285\n",
            "Epoch 379/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5794 - accuracy: 0.7469 - val_loss: 0.6548 - val_accuracy: 0.7139\n",
            "Epoch 380/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5592 - accuracy: 0.7451 - val_loss: 0.6700 - val_accuracy: 0.6943\n",
            "Epoch 381/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5655 - accuracy: 0.7240 - val_loss: 0.7072 - val_accuracy: 0.6846\n",
            "Epoch 382/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5508 - accuracy: 0.7407 - val_loss: 0.6972 - val_accuracy: 0.6885\n",
            "Epoch 383/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5686 - accuracy: 0.7336 - val_loss: 0.6760 - val_accuracy: 0.6895\n",
            "Epoch 384/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5452 - accuracy: 0.7441 - val_loss: 0.7017 - val_accuracy: 0.6895\n",
            "Epoch 385/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5469 - accuracy: 0.7495 - val_loss: 0.6371 - val_accuracy: 0.7119\n",
            "Epoch 386/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5377 - accuracy: 0.7455 - val_loss: 0.6526 - val_accuracy: 0.7119\n",
            "Epoch 387/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5901 - accuracy: 0.7264 - val_loss: 0.7116 - val_accuracy: 0.6846\n",
            "Epoch 388/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5690 - accuracy: 0.7363 - val_loss: 0.7817 - val_accuracy: 0.6973\n",
            "Epoch 389/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5640 - accuracy: 0.7388 - val_loss: 0.6534 - val_accuracy: 0.6934\n",
            "Epoch 390/500\n",
            "128/128 [==============================] - 1s 11ms/step - loss: 0.5682 - accuracy: 0.7314 - val_loss: 0.7251 - val_accuracy: 0.6807\n",
            "Epoch 391/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5647 - accuracy: 0.7320 - val_loss: 0.6349 - val_accuracy: 0.7002\n",
            "Epoch 392/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5095 - accuracy: 0.7671 - val_loss: 0.7030 - val_accuracy: 0.6982\n",
            "Epoch 393/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5402 - accuracy: 0.7527 - val_loss: 0.7520 - val_accuracy: 0.6895\n",
            "Epoch 394/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5535 - accuracy: 0.7394 - val_loss: 0.6959 - val_accuracy: 0.6914\n",
            "Epoch 395/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5177 - accuracy: 0.7619 - val_loss: 0.6836 - val_accuracy: 0.7051\n",
            "Epoch 396/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5349 - accuracy: 0.7550 - val_loss: 0.7269 - val_accuracy: 0.7021\n",
            "Epoch 397/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5677 - accuracy: 0.7442 - val_loss: 0.6783 - val_accuracy: 0.6826\n",
            "Epoch 398/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5599 - accuracy: 0.7395 - val_loss: 0.8122 - val_accuracy: 0.6934\n",
            "Epoch 399/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5667 - accuracy: 0.7322 - val_loss: 0.6575 - val_accuracy: 0.6738\n",
            "Epoch 400/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5439 - accuracy: 0.7537 - val_loss: 0.6471 - val_accuracy: 0.7207\n",
            "Epoch 401/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5186 - accuracy: 0.7676 - val_loss: 0.6132 - val_accuracy: 0.7402\n",
            "Epoch 402/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5509 - accuracy: 0.7559 - val_loss: 0.8118 - val_accuracy: 0.6494\n",
            "Epoch 403/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5822 - accuracy: 0.7420 - val_loss: 0.7460 - val_accuracy: 0.6797\n",
            "Epoch 404/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5482 - accuracy: 0.7554 - val_loss: 0.8591 - val_accuracy: 0.6963\n",
            "Epoch 405/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5505 - accuracy: 0.7574 - val_loss: 0.7649 - val_accuracy: 0.6914\n",
            "Epoch 406/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5741 - accuracy: 0.7256 - val_loss: 0.6817 - val_accuracy: 0.6943\n",
            "Epoch 407/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5673 - accuracy: 0.7236 - val_loss: 0.7030 - val_accuracy: 0.6865\n",
            "Epoch 408/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5644 - accuracy: 0.7271 - val_loss: 0.6732 - val_accuracy: 0.6816\n",
            "Epoch 409/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5782 - accuracy: 0.7265 - val_loss: 0.6547 - val_accuracy: 0.7021\n",
            "Epoch 410/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5511 - accuracy: 0.7347 - val_loss: 0.6676 - val_accuracy: 0.7109\n",
            "Epoch 411/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5420 - accuracy: 0.7398 - val_loss: 1.5521 - val_accuracy: 0.5850\n",
            "Epoch 412/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5568 - accuracy: 0.7377 - val_loss: 0.6750 - val_accuracy: 0.7061\n",
            "Epoch 413/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5329 - accuracy: 0.7626 - val_loss: 0.6374 - val_accuracy: 0.7100\n",
            "Epoch 414/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5474 - accuracy: 0.7535 - val_loss: 0.5551 - val_accuracy: 0.7490\n",
            "Epoch 415/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5281 - accuracy: 0.7671 - val_loss: 0.6260 - val_accuracy: 0.7979\n",
            "Epoch 416/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.4397 - accuracy: 0.8079 - val_loss: 0.4879 - val_accuracy: 0.7695\n",
            "Epoch 417/500\n",
            "128/128 [==============================] - 1s 11ms/step - loss: 0.5424 - accuracy: 0.7671 - val_loss: 0.6147 - val_accuracy: 0.7461\n",
            "Epoch 418/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.4038 - accuracy: 0.8305 - val_loss: 0.6742 - val_accuracy: 0.7422\n",
            "Epoch 419/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.2659 - accuracy: 0.9021 - val_loss: 0.7056 - val_accuracy: 0.7012\n",
            "Epoch 420/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5245 - accuracy: 0.7971 - val_loss: 0.4709 - val_accuracy: 0.7695\n",
            "Epoch 421/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.4532 - accuracy: 0.8135 - val_loss: 0.5448 - val_accuracy: 0.8135\n",
            "Epoch 422/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.2776 - accuracy: 0.8965 - val_loss: 0.7737 - val_accuracy: 0.7695\n",
            "Epoch 423/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.1552 - accuracy: 0.9374 - val_loss: 1.0708 - val_accuracy: 0.7178\n",
            "Epoch 424/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.1053 - accuracy: 0.9723 - val_loss: 0.6056 - val_accuracy: 0.7715\n",
            "Epoch 425/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.0760 - accuracy: 0.9756 - val_loss: 0.6761 - val_accuracy: 0.6924\n",
            "Epoch 426/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.0718 - accuracy: 0.9735 - val_loss: 0.6114 - val_accuracy: 0.7568\n",
            "Epoch 427/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.0613 - accuracy: 0.9796 - val_loss: 2.4085 - val_accuracy: 0.6660\n",
            "Epoch 428/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.8086 - accuracy: 0.6933 - val_loss: 0.7268 - val_accuracy: 0.6816\n",
            "Epoch 429/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5352 - accuracy: 0.7466 - val_loss: 0.6864 - val_accuracy: 0.7041\n",
            "Epoch 430/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5308 - accuracy: 0.7503 - val_loss: 0.6904 - val_accuracy: 0.6885\n",
            "Epoch 431/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5736 - accuracy: 0.7318 - val_loss: 0.6783 - val_accuracy: 0.6924\n",
            "Epoch 432/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5560 - accuracy: 0.7276 - val_loss: 0.6783 - val_accuracy: 0.6904\n",
            "Epoch 433/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5152 - accuracy: 0.7565 - val_loss: 0.7279 - val_accuracy: 0.6758\n",
            "Epoch 434/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5430 - accuracy: 0.7402 - val_loss: 0.6784 - val_accuracy: 0.6953\n",
            "Epoch 435/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5289 - accuracy: 0.7496 - val_loss: 0.6661 - val_accuracy: 0.7002\n",
            "Epoch 436/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5363 - accuracy: 0.7389 - val_loss: 0.6775 - val_accuracy: 0.6875\n",
            "Epoch 437/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5170 - accuracy: 0.7498 - val_loss: 0.6887 - val_accuracy: 0.6943\n",
            "Epoch 438/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5223 - accuracy: 0.7549 - val_loss: 0.7323 - val_accuracy: 0.6680\n",
            "Epoch 439/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5397 - accuracy: 0.7336 - val_loss: 0.6873 - val_accuracy: 0.6963\n",
            "Epoch 440/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5341 - accuracy: 0.7395 - val_loss: 0.6867 - val_accuracy: 0.6855\n",
            "Epoch 441/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5356 - accuracy: 0.7374 - val_loss: 0.6713 - val_accuracy: 0.7031\n",
            "Epoch 442/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5229 - accuracy: 0.7435 - val_loss: 0.6770 - val_accuracy: 0.6992\n",
            "Epoch 443/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5118 - accuracy: 0.7596 - val_loss: 0.6873 - val_accuracy: 0.6973\n",
            "Epoch 444/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5259 - accuracy: 0.7409 - val_loss: 0.6696 - val_accuracy: 0.6963\n",
            "Epoch 445/500\n",
            "128/128 [==============================] - 1s 11ms/step - loss: 0.5247 - accuracy: 0.7452 - val_loss: 0.7470 - val_accuracy: 0.6748\n",
            "Epoch 446/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5237 - accuracy: 0.7438 - val_loss: 0.6532 - val_accuracy: 0.7061\n",
            "Epoch 447/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5540 - accuracy: 0.7274 - val_loss: 0.6768 - val_accuracy: 0.6963\n",
            "Epoch 448/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5473 - accuracy: 0.7341 - val_loss: 0.8105 - val_accuracy: 0.6865\n",
            "Epoch 449/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5617 - accuracy: 0.7294 - val_loss: 0.6966 - val_accuracy: 0.6768\n",
            "Epoch 450/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5418 - accuracy: 0.7262 - val_loss: 0.6634 - val_accuracy: 0.7051\n",
            "Epoch 451/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5586 - accuracy: 0.7325 - val_loss: 1.0576 - val_accuracy: 0.6143\n",
            "Epoch 452/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6056 - accuracy: 0.7063 - val_loss: 0.6820 - val_accuracy: 0.7031\n",
            "Epoch 453/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5368 - accuracy: 0.7361 - val_loss: 0.6609 - val_accuracy: 0.6973\n",
            "Epoch 454/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5255 - accuracy: 0.7468 - val_loss: 0.6553 - val_accuracy: 0.7100\n",
            "Epoch 455/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5182 - accuracy: 0.7523 - val_loss: 0.6904 - val_accuracy: 0.7031\n",
            "Epoch 456/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5348 - accuracy: 0.7448 - val_loss: 0.6723 - val_accuracy: 0.7061\n",
            "Epoch 457/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5124 - accuracy: 0.7461 - val_loss: 0.6667 - val_accuracy: 0.7002\n",
            "Epoch 458/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5287 - accuracy: 0.7353 - val_loss: 0.6789 - val_accuracy: 0.7070\n",
            "Epoch 459/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5115 - accuracy: 0.7449 - val_loss: 0.6734 - val_accuracy: 0.6992\n",
            "Epoch 460/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5013 - accuracy: 0.7571 - val_loss: 0.6740 - val_accuracy: 0.7139\n",
            "Epoch 461/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.4929 - accuracy: 0.7570 - val_loss: 0.6989 - val_accuracy: 0.7021\n",
            "Epoch 462/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5483 - accuracy: 0.7365 - val_loss: 2.0787 - val_accuracy: 0.4883\n",
            "Epoch 463/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5844 - accuracy: 0.7188 - val_loss: 0.6967 - val_accuracy: 0.6875\n",
            "Epoch 464/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5549 - accuracy: 0.7298 - val_loss: 0.6886 - val_accuracy: 0.6924\n",
            "Epoch 465/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5361 - accuracy: 0.7398 - val_loss: 0.6720 - val_accuracy: 0.7002\n",
            "Epoch 466/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5429 - accuracy: 0.7378 - val_loss: 0.6848 - val_accuracy: 0.6924\n",
            "Epoch 467/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5328 - accuracy: 0.7401 - val_loss: 0.6886 - val_accuracy: 0.6982\n",
            "Epoch 468/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5163 - accuracy: 0.7469 - val_loss: 0.7064 - val_accuracy: 0.6924\n",
            "Epoch 469/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5183 - accuracy: 0.7418 - val_loss: 0.7214 - val_accuracy: 0.6836\n",
            "Epoch 470/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5308 - accuracy: 0.7411 - val_loss: 0.6944 - val_accuracy: 0.7051\n",
            "Epoch 471/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5170 - accuracy: 0.7428 - val_loss: 0.7182 - val_accuracy: 0.6855\n",
            "Epoch 472/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5115 - accuracy: 0.7494 - val_loss: 0.7121 - val_accuracy: 0.6797\n",
            "Epoch 473/500\n",
            "128/128 [==============================] - 1s 11ms/step - loss: 0.5462 - accuracy: 0.7373 - val_loss: 0.7029 - val_accuracy: 0.6953\n",
            "Epoch 474/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5094 - accuracy: 0.7477 - val_loss: 0.6767 - val_accuracy: 0.7031\n",
            "Epoch 475/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5142 - accuracy: 0.7541 - val_loss: 0.6986 - val_accuracy: 0.6953\n",
            "Epoch 476/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5168 - accuracy: 0.7435 - val_loss: 0.6732 - val_accuracy: 0.7012\n",
            "Epoch 477/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5506 - accuracy: 0.7322 - val_loss: 0.6894 - val_accuracy: 0.6982\n",
            "Epoch 478/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5081 - accuracy: 0.7542 - val_loss: 0.6738 - val_accuracy: 0.7051\n",
            "Epoch 479/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5181 - accuracy: 0.7488 - val_loss: 0.6941 - val_accuracy: 0.6934\n",
            "Epoch 480/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5353 - accuracy: 0.7337 - val_loss: 0.6953 - val_accuracy: 0.6992\n",
            "Epoch 481/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5238 - accuracy: 0.7395 - val_loss: 0.6840 - val_accuracy: 0.7168\n",
            "Epoch 482/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5039 - accuracy: 0.7574 - val_loss: 0.7009 - val_accuracy: 0.7012\n",
            "Epoch 483/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5153 - accuracy: 0.7454 - val_loss: 0.6728 - val_accuracy: 0.7168\n",
            "Epoch 484/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5240 - accuracy: 0.7419 - val_loss: 0.6819 - val_accuracy: 0.6992\n",
            "Epoch 485/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5215 - accuracy: 0.7454 - val_loss: 0.7060 - val_accuracy: 0.6943\n",
            "Epoch 486/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5045 - accuracy: 0.7531 - val_loss: 0.6846 - val_accuracy: 0.7021\n",
            "Epoch 487/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5152 - accuracy: 0.7413 - val_loss: 0.7028 - val_accuracy: 0.6924\n",
            "Epoch 488/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5004 - accuracy: 0.7551 - val_loss: 0.6889 - val_accuracy: 0.6973\n",
            "Epoch 489/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5083 - accuracy: 0.7544 - val_loss: 0.7040 - val_accuracy: 0.6934\n",
            "Epoch 490/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5183 - accuracy: 0.7488 - val_loss: 0.7052 - val_accuracy: 0.6973\n",
            "Epoch 491/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.4951 - accuracy: 0.7608 - val_loss: 0.6870 - val_accuracy: 0.6992\n",
            "Epoch 492/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5219 - accuracy: 0.7387 - val_loss: 0.6905 - val_accuracy: 0.6982\n",
            "Epoch 493/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5104 - accuracy: 0.7473 - val_loss: 0.6877 - val_accuracy: 0.7051\n",
            "Epoch 494/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5027 - accuracy: 0.7518 - val_loss: 0.6989 - val_accuracy: 0.6943\n",
            "Epoch 495/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.4999 - accuracy: 0.7586 - val_loss: 0.6956 - val_accuracy: 0.7080\n",
            "Epoch 496/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5144 - accuracy: 0.7475 - val_loss: 0.7412 - val_accuracy: 0.7324\n",
            "Epoch 497/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5287 - accuracy: 0.7446 - val_loss: 0.6936 - val_accuracy: 0.7090\n",
            "Epoch 498/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5051 - accuracy: 0.7519 - val_loss: 0.6981 - val_accuracy: 0.6963\n",
            "Epoch 499/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.5198 - accuracy: 0.7437 - val_loss: 0.6958 - val_accuracy: 0.7178\n",
            "Epoch 500/500\n",
            "128/128 [==============================] - 1s 11ms/step - loss: 0.4984 - accuracy: 0.7541 - val_loss: 0.7114 - val_accuracy: 0.7109\n",
            "\n",
            "Accuracy: 71.09%\n",
            "Loss: 0.7113502621650696 \n",
            "\n",
            "NEXT SET OF HYPERPARAMETERS IS: \n",
            " num_conv_layers: 3 \n",
            " num_conv_nodes: 102 \n",
            " num_dense_layers: 2 \n",
            " num_dense_nodes: 152 \n",
            "\n",
            "Epoch 1/500\n",
            "128/128 [==============================] - 2s 8ms/step - loss: 1.5092 - accuracy: 0.4578 - val_loss: 2.6554 - val_accuracy: 0.1045\n",
            "Epoch 2/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.8503 - accuracy: 0.6223 - val_loss: 2.7579 - val_accuracy: 0.1074\n",
            "Epoch 3/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.8177 - accuracy: 0.6311 - val_loss: 2.9042 - val_accuracy: 0.1387\n",
            "Epoch 4/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.8131 - accuracy: 0.6336 - val_loss: 1.8727 - val_accuracy: 0.2910\n",
            "Epoch 5/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.7497 - accuracy: 0.6655 - val_loss: 0.7927 - val_accuracy: 0.6172\n",
            "Epoch 6/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.7595 - accuracy: 0.6677 - val_loss: 0.8073 - val_accuracy: 0.6475\n",
            "Epoch 7/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6930 - accuracy: 0.7140 - val_loss: 0.8352 - val_accuracy: 0.5986\n",
            "Epoch 8/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6874 - accuracy: 0.7136 - val_loss: 1.8656 - val_accuracy: 0.5342\n",
            "Epoch 9/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5998 - accuracy: 0.7500 - val_loss: 0.9066 - val_accuracy: 0.6152\n",
            "Epoch 10/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7313 - accuracy: 0.6833 - val_loss: 1.6740 - val_accuracy: 0.5244\n",
            "Epoch 11/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7188 - accuracy: 0.6809 - val_loss: 5.2814 - val_accuracy: 0.3389\n",
            "Epoch 12/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7728 - accuracy: 0.6729 - val_loss: 1.5578 - val_accuracy: 0.3906\n",
            "Epoch 13/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6831 - accuracy: 0.7087 - val_loss: 1.3330 - val_accuracy: 0.4717\n",
            "Epoch 14/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.7110 - accuracy: 0.7005 - val_loss: 1.8973 - val_accuracy: 0.3086\n",
            "Epoch 15/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7434 - accuracy: 0.6738 - val_loss: 1.3527 - val_accuracy: 0.4473\n",
            "Epoch 16/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7460 - accuracy: 0.6699 - val_loss: 3.3764 - val_accuracy: 0.2168\n",
            "Epoch 17/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7538 - accuracy: 0.6554 - val_loss: 0.9536 - val_accuracy: 0.5596\n",
            "Epoch 18/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7369 - accuracy: 0.6689 - val_loss: 0.8410 - val_accuracy: 0.6045\n",
            "Epoch 19/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.7228 - accuracy: 0.6667 - val_loss: 0.9920 - val_accuracy: 0.5947\n",
            "Epoch 20/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.7331 - accuracy: 0.6536 - val_loss: 1.8810 - val_accuracy: 0.3574\n",
            "Epoch 21/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.7615 - accuracy: 0.6364 - val_loss: 2.1302 - val_accuracy: 0.3232\n",
            "Epoch 22/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.7252 - accuracy: 0.6645 - val_loss: 0.9592 - val_accuracy: 0.5674\n",
            "Epoch 23/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7325 - accuracy: 0.6530 - val_loss: 0.7996 - val_accuracy: 0.6211\n",
            "Epoch 24/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7291 - accuracy: 0.6443 - val_loss: 0.7772 - val_accuracy: 0.6475\n",
            "Epoch 25/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.7125 - accuracy: 0.6567 - val_loss: 0.8222 - val_accuracy: 0.6172\n",
            "Epoch 26/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.7240 - accuracy: 0.6557 - val_loss: 0.7546 - val_accuracy: 0.6426\n",
            "Epoch 27/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.7220 - accuracy: 0.6601 - val_loss: 1.0254 - val_accuracy: 0.5088\n",
            "Epoch 28/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7326 - accuracy: 0.6444 - val_loss: 0.7332 - val_accuracy: 0.6318\n",
            "Epoch 29/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7209 - accuracy: 0.6514 - val_loss: 0.8035 - val_accuracy: 0.6367\n",
            "Epoch 30/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6649 - accuracy: 0.6865 - val_loss: 1.6173 - val_accuracy: 0.5166\n",
            "Epoch 31/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6892 - accuracy: 0.6684 - val_loss: 0.8108 - val_accuracy: 0.6201\n",
            "Epoch 32/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6931 - accuracy: 0.6677 - val_loss: 0.7611 - val_accuracy: 0.6318\n",
            "Epoch 33/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7115 - accuracy: 0.6639 - val_loss: 2.1774 - val_accuracy: 0.3555\n",
            "Epoch 34/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7194 - accuracy: 0.6498 - val_loss: 0.8000 - val_accuracy: 0.6074\n",
            "Epoch 35/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.7251 - accuracy: 0.6491 - val_loss: 0.7026 - val_accuracy: 0.6699\n",
            "Epoch 36/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.7018 - accuracy: 0.6675 - val_loss: 0.7244 - val_accuracy: 0.6602\n",
            "Epoch 37/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6899 - accuracy: 0.6638 - val_loss: 0.9669 - val_accuracy: 0.5713\n",
            "Epoch 38/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.7109 - accuracy: 0.6530 - val_loss: 0.7151 - val_accuracy: 0.6660\n",
            "Epoch 39/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6939 - accuracy: 0.6731 - val_loss: 0.7381 - val_accuracy: 0.6436\n",
            "Epoch 40/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6932 - accuracy: 0.6613 - val_loss: 0.7880 - val_accuracy: 0.6172\n",
            "Epoch 41/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6844 - accuracy: 0.6679 - val_loss: 0.7244 - val_accuracy: 0.6396\n",
            "Epoch 42/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6944 - accuracy: 0.6597 - val_loss: 0.7127 - val_accuracy: 0.6816\n",
            "Epoch 43/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6905 - accuracy: 0.6743 - val_loss: 0.8234 - val_accuracy: 0.6240\n",
            "Epoch 44/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6970 - accuracy: 0.6529 - val_loss: 0.7533 - val_accuracy: 0.6436\n",
            "Epoch 45/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6772 - accuracy: 0.6665 - val_loss: 0.7634 - val_accuracy: 0.6260\n",
            "Epoch 46/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.7100 - accuracy: 0.6744 - val_loss: 0.7117 - val_accuracy: 0.6719\n",
            "Epoch 47/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.7025 - accuracy: 0.6647 - val_loss: 0.6906 - val_accuracy: 0.6748\n",
            "Epoch 48/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6913 - accuracy: 0.6668 - val_loss: 0.7536 - val_accuracy: 0.6338\n",
            "Epoch 49/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.7026 - accuracy: 0.6542 - val_loss: 0.7242 - val_accuracy: 0.6699\n",
            "Epoch 50/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6838 - accuracy: 0.6772 - val_loss: 0.7271 - val_accuracy: 0.6484\n",
            "Epoch 51/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6861 - accuracy: 0.6645 - val_loss: 1.6844 - val_accuracy: 0.4951\n",
            "Epoch 52/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6735 - accuracy: 0.6737 - val_loss: 0.6827 - val_accuracy: 0.6855\n",
            "Epoch 53/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6785 - accuracy: 0.6635 - val_loss: 0.8745 - val_accuracy: 0.5957\n",
            "Epoch 54/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6878 - accuracy: 0.6710 - val_loss: 0.7076 - val_accuracy: 0.6729\n",
            "Epoch 55/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6822 - accuracy: 0.6720 - val_loss: 0.6879 - val_accuracy: 0.6807\n",
            "Epoch 56/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6533 - accuracy: 0.6918 - val_loss: 0.7357 - val_accuracy: 0.6484\n",
            "Epoch 57/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6927 - accuracy: 0.6567 - val_loss: 0.8503 - val_accuracy: 0.6133\n",
            "Epoch 58/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6751 - accuracy: 0.6713 - val_loss: 0.7323 - val_accuracy: 0.6475\n",
            "Epoch 59/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.7031 - accuracy: 0.6639 - val_loss: 0.7037 - val_accuracy: 0.6709\n",
            "Epoch 60/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6954 - accuracy: 0.6642 - val_loss: 0.6865 - val_accuracy: 0.6885\n",
            "Epoch 61/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6886 - accuracy: 0.6663 - val_loss: 0.7096 - val_accuracy: 0.6680\n",
            "Epoch 62/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6616 - accuracy: 0.6806 - val_loss: 14.3580 - val_accuracy: 0.3896\n",
            "Epoch 63/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6801 - accuracy: 0.6986 - val_loss: 1.6818 - val_accuracy: 0.4854\n",
            "Epoch 64/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6686 - accuracy: 0.7124 - val_loss: 2.0681 - val_accuracy: 0.3916\n",
            "Epoch 65/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6422 - accuracy: 0.7239 - val_loss: 5.3653 - val_accuracy: 0.1855\n",
            "Epoch 66/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6371 - accuracy: 0.7169 - val_loss: 6.6810 - val_accuracy: 0.1523\n",
            "Epoch 67/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6227 - accuracy: 0.7352 - val_loss: 0.9639 - val_accuracy: 0.5615\n",
            "Epoch 68/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6706 - accuracy: 0.6913 - val_loss: 0.9474 - val_accuracy: 0.6064\n",
            "Epoch 69/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6597 - accuracy: 0.6981 - val_loss: 1.1426 - val_accuracy: 0.5039\n",
            "Epoch 70/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6916 - accuracy: 0.6649 - val_loss: 0.8551 - val_accuracy: 0.5850\n",
            "Epoch 71/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6962 - accuracy: 0.6612 - val_loss: 1.4006 - val_accuracy: 0.5322\n",
            "Epoch 72/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6797 - accuracy: 0.6737 - val_loss: 0.7265 - val_accuracy: 0.6562\n",
            "Epoch 73/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6602 - accuracy: 0.6851 - val_loss: 0.7334 - val_accuracy: 0.6582\n",
            "Epoch 74/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6719 - accuracy: 0.6670 - val_loss: 0.8428 - val_accuracy: 0.6045\n",
            "Epoch 75/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6760 - accuracy: 0.6807 - val_loss: 0.6934 - val_accuracy: 0.6758\n",
            "Epoch 76/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6609 - accuracy: 0.6877 - val_loss: 1.0827 - val_accuracy: 0.5420\n",
            "Epoch 77/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6924 - accuracy: 0.6699 - val_loss: 0.7113 - val_accuracy: 0.6689\n",
            "Epoch 78/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6483 - accuracy: 0.6864 - val_loss: 0.6924 - val_accuracy: 0.6719\n",
            "Epoch 79/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6823 - accuracy: 0.6726 - val_loss: 0.7077 - val_accuracy: 0.6748\n",
            "Epoch 80/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6624 - accuracy: 0.6834 - val_loss: 0.6843 - val_accuracy: 0.6807\n",
            "Epoch 81/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6614 - accuracy: 0.6771 - val_loss: 0.7138 - val_accuracy: 0.6748\n",
            "Epoch 82/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6722 - accuracy: 0.6830 - val_loss: 0.7011 - val_accuracy: 0.6709\n",
            "Epoch 83/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6667 - accuracy: 0.6770 - val_loss: 0.7787 - val_accuracy: 0.6377\n",
            "Epoch 84/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6789 - accuracy: 0.6771 - val_loss: 0.7876 - val_accuracy: 0.6465\n",
            "Epoch 85/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6569 - accuracy: 0.6864 - val_loss: 0.7709 - val_accuracy: 0.6191\n",
            "Epoch 86/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6552 - accuracy: 0.6898 - val_loss: 0.6813 - val_accuracy: 0.6807\n",
            "Epoch 87/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6614 - accuracy: 0.6825 - val_loss: 0.6855 - val_accuracy: 0.6865\n",
            "Epoch 88/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6417 - accuracy: 0.6918 - val_loss: 0.6717 - val_accuracy: 0.6885\n",
            "Epoch 89/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6633 - accuracy: 0.6802 - val_loss: 0.7723 - val_accuracy: 0.6182\n",
            "Epoch 90/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6429 - accuracy: 0.6968 - val_loss: 0.6799 - val_accuracy: 0.6943\n",
            "Epoch 91/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6588 - accuracy: 0.6898 - val_loss: 2.0134 - val_accuracy: 0.4990\n",
            "Epoch 92/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6398 - accuracy: 0.6910 - val_loss: 0.6771 - val_accuracy: 0.6855\n",
            "Epoch 93/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6289 - accuracy: 0.6963 - val_loss: 0.6756 - val_accuracy: 0.6768\n",
            "Epoch 94/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6610 - accuracy: 0.6792 - val_loss: 0.6824 - val_accuracy: 0.6787\n",
            "Epoch 95/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6399 - accuracy: 0.7008 - val_loss: 0.7366 - val_accuracy: 0.6357\n",
            "Epoch 96/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6328 - accuracy: 0.6999 - val_loss: 0.9649 - val_accuracy: 0.6055\n",
            "Epoch 97/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6012 - accuracy: 0.7162 - val_loss: 1.3282 - val_accuracy: 0.4824\n",
            "Epoch 98/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6572 - accuracy: 0.6907 - val_loss: 0.7643 - val_accuracy: 0.6504\n",
            "Epoch 99/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6397 - accuracy: 0.6972 - val_loss: 0.7187 - val_accuracy: 0.6533\n",
            "Epoch 100/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6442 - accuracy: 0.7008 - val_loss: 0.7092 - val_accuracy: 0.6641\n",
            "Epoch 101/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6206 - accuracy: 0.7037 - val_loss: 0.7236 - val_accuracy: 0.6543\n",
            "Epoch 102/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6297 - accuracy: 0.7047 - val_loss: 0.6926 - val_accuracy: 0.6582\n",
            "Epoch 103/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6150 - accuracy: 0.7087 - val_loss: 0.7003 - val_accuracy: 0.6660\n",
            "Epoch 104/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6085 - accuracy: 0.7178 - val_loss: 0.6756 - val_accuracy: 0.6865\n",
            "Epoch 105/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6231 - accuracy: 0.7051 - val_loss: 0.6648 - val_accuracy: 0.6836\n",
            "Epoch 106/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6300 - accuracy: 0.6939 - val_loss: 0.6941 - val_accuracy: 0.6758\n",
            "Epoch 107/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6077 - accuracy: 0.7118 - val_loss: 0.6999 - val_accuracy: 0.6787\n",
            "Epoch 108/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6267 - accuracy: 0.6930 - val_loss: 0.6677 - val_accuracy: 0.6846\n",
            "Epoch 109/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6354 - accuracy: 0.6942 - val_loss: 0.7350 - val_accuracy: 0.6455\n",
            "Epoch 110/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6162 - accuracy: 0.7064 - val_loss: 0.6308 - val_accuracy: 0.7070\n",
            "Epoch 111/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6130 - accuracy: 0.7071 - val_loss: 0.6420 - val_accuracy: 0.6973\n",
            "Epoch 112/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5925 - accuracy: 0.7272 - val_loss: 0.7263 - val_accuracy: 0.6582\n",
            "Epoch 113/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6106 - accuracy: 0.7118 - val_loss: 0.6720 - val_accuracy: 0.6865\n",
            "Epoch 114/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6048 - accuracy: 0.7119 - val_loss: 0.6921 - val_accuracy: 0.6689\n",
            "Epoch 115/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6221 - accuracy: 0.7049 - val_loss: 0.6617 - val_accuracy: 0.6973\n",
            "Epoch 116/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6006 - accuracy: 0.7248 - val_loss: 0.6722 - val_accuracy: 0.6758\n",
            "Epoch 117/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6072 - accuracy: 0.7086 - val_loss: 0.6432 - val_accuracy: 0.6885\n",
            "Epoch 118/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6077 - accuracy: 0.7049 - val_loss: 0.6345 - val_accuracy: 0.6973\n",
            "Epoch 119/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5959 - accuracy: 0.7189 - val_loss: 0.7082 - val_accuracy: 0.6816\n",
            "Epoch 120/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5830 - accuracy: 0.7254 - val_loss: 0.6246 - val_accuracy: 0.6934\n",
            "Epoch 121/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5788 - accuracy: 0.7206 - val_loss: 0.6987 - val_accuracy: 0.6709\n",
            "Epoch 122/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6067 - accuracy: 0.7052 - val_loss: 0.6508 - val_accuracy: 0.6973\n",
            "Epoch 123/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5871 - accuracy: 0.7179 - val_loss: 0.6490 - val_accuracy: 0.6904\n",
            "Epoch 124/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5711 - accuracy: 0.7305 - val_loss: 0.6184 - val_accuracy: 0.7109\n",
            "Epoch 125/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5868 - accuracy: 0.7223 - val_loss: 0.6257 - val_accuracy: 0.6973\n",
            "Epoch 126/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5676 - accuracy: 0.7318 - val_loss: 0.6529 - val_accuracy: 0.6904\n",
            "Epoch 127/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5779 - accuracy: 0.7149 - val_loss: 0.6612 - val_accuracy: 0.6836\n",
            "Epoch 128/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5658 - accuracy: 0.7367 - val_loss: 0.6248 - val_accuracy: 0.6992\n",
            "Epoch 129/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6069 - accuracy: 0.7104 - val_loss: 0.6231 - val_accuracy: 0.7051\n",
            "Epoch 130/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5743 - accuracy: 0.7322 - val_loss: 0.6153 - val_accuracy: 0.7041\n",
            "Epoch 131/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5687 - accuracy: 0.7321 - val_loss: 0.6716 - val_accuracy: 0.6895\n",
            "Epoch 132/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5853 - accuracy: 0.7227 - val_loss: 0.7947 - val_accuracy: 0.6504\n",
            "Epoch 133/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5618 - accuracy: 0.7369 - val_loss: 0.6201 - val_accuracy: 0.7080\n",
            "Epoch 134/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5550 - accuracy: 0.7370 - val_loss: 0.6132 - val_accuracy: 0.7080\n",
            "Epoch 135/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5847 - accuracy: 0.7165 - val_loss: 0.6269 - val_accuracy: 0.7031\n",
            "Epoch 136/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5789 - accuracy: 0.7333 - val_loss: 0.6194 - val_accuracy: 0.7031\n",
            "Epoch 137/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5705 - accuracy: 0.7173 - val_loss: 0.6102 - val_accuracy: 0.7207\n",
            "Epoch 138/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5609 - accuracy: 0.7216 - val_loss: 0.6240 - val_accuracy: 0.7061\n",
            "Epoch 139/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5540 - accuracy: 0.7349 - val_loss: 0.6063 - val_accuracy: 0.7139\n",
            "Epoch 140/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5589 - accuracy: 0.7377 - val_loss: 0.5991 - val_accuracy: 0.7100\n",
            "Epoch 141/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5553 - accuracy: 0.7373 - val_loss: 0.6090 - val_accuracy: 0.7139\n",
            "Epoch 142/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5872 - accuracy: 0.7229 - val_loss: 0.6477 - val_accuracy: 0.6963\n",
            "Epoch 143/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5728 - accuracy: 0.7176 - val_loss: 0.6018 - val_accuracy: 0.7109\n",
            "Epoch 144/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5549 - accuracy: 0.7314 - val_loss: 0.6534 - val_accuracy: 0.6855\n",
            "Epoch 145/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5831 - accuracy: 0.7125 - val_loss: 0.6007 - val_accuracy: 0.7158\n",
            "Epoch 146/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5561 - accuracy: 0.7350 - val_loss: 0.6380 - val_accuracy: 0.7090\n",
            "Epoch 147/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5373 - accuracy: 0.7395 - val_loss: 0.8196 - val_accuracy: 0.6328\n",
            "Epoch 148/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5280 - accuracy: 0.7518 - val_loss: 0.6017 - val_accuracy: 0.7188\n",
            "Epoch 149/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5327 - accuracy: 0.7416 - val_loss: 0.6870 - val_accuracy: 0.6729\n",
            "Epoch 150/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5310 - accuracy: 0.7350 - val_loss: 0.6186 - val_accuracy: 0.7031\n",
            "Epoch 151/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5320 - accuracy: 0.7428 - val_loss: 0.5880 - val_accuracy: 0.7129\n",
            "Epoch 152/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5536 - accuracy: 0.7366 - val_loss: 0.6528 - val_accuracy: 0.6865\n",
            "Epoch 153/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5497 - accuracy: 0.7359 - val_loss: 0.6248 - val_accuracy: 0.7061\n",
            "Epoch 154/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5375 - accuracy: 0.7393 - val_loss: 0.5969 - val_accuracy: 0.7090\n",
            "Epoch 155/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5378 - accuracy: 0.7378 - val_loss: 0.5860 - val_accuracy: 0.7168\n",
            "Epoch 156/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5525 - accuracy: 0.7344 - val_loss: 0.5866 - val_accuracy: 0.7246\n",
            "Epoch 157/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5735 - accuracy: 0.7179 - val_loss: 0.6934 - val_accuracy: 0.6748\n",
            "Epoch 158/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5432 - accuracy: 0.7300 - val_loss: 0.6095 - val_accuracy: 0.7119\n",
            "Epoch 159/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5377 - accuracy: 0.7430 - val_loss: 0.6785 - val_accuracy: 0.6758\n",
            "Epoch 160/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5644 - accuracy: 0.7292 - val_loss: 0.8038 - val_accuracy: 0.6279\n",
            "Epoch 161/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5243 - accuracy: 0.7413 - val_loss: 0.5875 - val_accuracy: 0.7129\n",
            "Epoch 162/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5352 - accuracy: 0.7433 - val_loss: 0.5882 - val_accuracy: 0.7129\n",
            "Epoch 163/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5124 - accuracy: 0.7513 - val_loss: 0.5798 - val_accuracy: 0.7100\n",
            "Epoch 164/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5484 - accuracy: 0.7373 - val_loss: 0.6031 - val_accuracy: 0.7109\n",
            "Epoch 165/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5403 - accuracy: 0.7443 - val_loss: 0.6270 - val_accuracy: 0.7109\n",
            "Epoch 166/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5289 - accuracy: 0.7408 - val_loss: 0.5842 - val_accuracy: 0.7246\n",
            "Epoch 167/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5468 - accuracy: 0.7266 - val_loss: 0.7446 - val_accuracy: 0.6660\n",
            "Epoch 168/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5344 - accuracy: 0.7452 - val_loss: 0.5959 - val_accuracy: 0.7119\n",
            "Epoch 169/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5282 - accuracy: 0.7493 - val_loss: 0.5960 - val_accuracy: 0.7266\n",
            "Epoch 170/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5445 - accuracy: 0.7313 - val_loss: 0.7542 - val_accuracy: 0.6660\n",
            "Epoch 171/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5243 - accuracy: 0.7458 - val_loss: 0.5818 - val_accuracy: 0.7158\n",
            "Epoch 172/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5421 - accuracy: 0.7370 - val_loss: 0.5725 - val_accuracy: 0.7158\n",
            "Epoch 173/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5430 - accuracy: 0.7370 - val_loss: 0.6206 - val_accuracy: 0.6982\n",
            "Epoch 174/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5151 - accuracy: 0.7490 - val_loss: 3.7767 - val_accuracy: 0.4258\n",
            "Epoch 175/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6067 - accuracy: 0.7104 - val_loss: 0.6286 - val_accuracy: 0.6963\n",
            "Epoch 176/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5351 - accuracy: 0.7429 - val_loss: 9.2904 - val_accuracy: 0.2451\n",
            "Epoch 177/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5683 - accuracy: 0.7262 - val_loss: 0.7275 - val_accuracy: 0.6543\n",
            "Epoch 178/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5426 - accuracy: 0.7294 - val_loss: 0.6341 - val_accuracy: 0.7021\n",
            "Epoch 179/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5392 - accuracy: 0.7389 - val_loss: 0.6194 - val_accuracy: 0.7129\n",
            "Epoch 180/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5153 - accuracy: 0.7502 - val_loss: 0.8702 - val_accuracy: 0.6367\n",
            "Epoch 181/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5217 - accuracy: 0.7460 - val_loss: 0.5966 - val_accuracy: 0.7236\n",
            "Epoch 182/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5307 - accuracy: 0.7386 - val_loss: 0.5936 - val_accuracy: 0.7148\n",
            "Epoch 183/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5455 - accuracy: 0.7294 - val_loss: 0.6334 - val_accuracy: 0.7002\n",
            "Epoch 184/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5271 - accuracy: 0.7472 - val_loss: 0.6054 - val_accuracy: 0.7207\n",
            "Epoch 185/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5095 - accuracy: 0.7559 - val_loss: 1.9331 - val_accuracy: 0.5498\n",
            "Epoch 186/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5183 - accuracy: 0.7436 - val_loss: 0.6822 - val_accuracy: 0.6885\n",
            "Epoch 187/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5156 - accuracy: 0.7523 - val_loss: 1.2238 - val_accuracy: 0.6094\n",
            "Epoch 188/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5186 - accuracy: 0.7482 - val_loss: 0.8512 - val_accuracy: 0.6250\n",
            "Epoch 189/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4943 - accuracy: 0.7609 - val_loss: 0.6301 - val_accuracy: 0.7021\n",
            "Epoch 190/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5248 - accuracy: 0.7375 - val_loss: 0.6152 - val_accuracy: 0.7129\n",
            "Epoch 191/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5285 - accuracy: 0.7384 - val_loss: 0.5784 - val_accuracy: 0.7227\n",
            "Epoch 192/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5009 - accuracy: 0.7567 - val_loss: 0.6006 - val_accuracy: 0.7148\n",
            "Epoch 193/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4958 - accuracy: 0.7625 - val_loss: 0.5812 - val_accuracy: 0.7227\n",
            "Epoch 194/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5008 - accuracy: 0.7526 - val_loss: 0.5755 - val_accuracy: 0.7139\n",
            "Epoch 195/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5227 - accuracy: 0.7444 - val_loss: 0.7031 - val_accuracy: 0.6836\n",
            "Epoch 196/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5256 - accuracy: 0.7501 - val_loss: 0.6067 - val_accuracy: 0.7080\n",
            "Epoch 197/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5335 - accuracy: 0.7360 - val_loss: 0.5760 - val_accuracy: 0.7129\n",
            "Epoch 198/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5096 - accuracy: 0.7445 - val_loss: 0.5662 - val_accuracy: 0.7217\n",
            "Epoch 199/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5091 - accuracy: 0.7515 - val_loss: 0.6017 - val_accuracy: 0.7109\n",
            "Epoch 200/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5236 - accuracy: 0.7443 - val_loss: 0.5718 - val_accuracy: 0.7236\n",
            "Epoch 201/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5037 - accuracy: 0.7545 - val_loss: 0.5994 - val_accuracy: 0.7256\n",
            "Epoch 202/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5237 - accuracy: 0.7401 - val_loss: 0.5941 - val_accuracy: 0.7178\n",
            "Epoch 203/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5073 - accuracy: 0.7427 - val_loss: 0.6373 - val_accuracy: 0.6973\n",
            "Epoch 204/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5344 - accuracy: 0.7310 - val_loss: 0.6109 - val_accuracy: 0.7100\n",
            "Epoch 205/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5040 - accuracy: 0.7465 - val_loss: 0.6106 - val_accuracy: 0.7197\n",
            "Epoch 206/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5051 - accuracy: 0.7557 - val_loss: 0.8674 - val_accuracy: 0.6348\n",
            "Epoch 207/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5182 - accuracy: 0.7517 - val_loss: 0.6675 - val_accuracy: 0.6787\n",
            "Epoch 208/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5482 - accuracy: 0.7295 - val_loss: 0.7707 - val_accuracy: 0.6465\n",
            "Epoch 209/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5091 - accuracy: 0.7552 - val_loss: 0.5716 - val_accuracy: 0.7227\n",
            "Epoch 210/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5375 - accuracy: 0.7343 - val_loss: 0.5828 - val_accuracy: 0.7217\n",
            "Epoch 211/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5271 - accuracy: 0.7361 - val_loss: 0.5734 - val_accuracy: 0.7227\n",
            "Epoch 212/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5070 - accuracy: 0.7603 - val_loss: 0.6159 - val_accuracy: 0.7139\n",
            "Epoch 213/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5163 - accuracy: 0.7486 - val_loss: 0.6383 - val_accuracy: 0.7178\n",
            "Epoch 214/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5352 - accuracy: 0.7329 - val_loss: 0.5874 - val_accuracy: 0.7217\n",
            "Epoch 215/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5004 - accuracy: 0.7549 - val_loss: 0.5872 - val_accuracy: 0.7197\n",
            "Epoch 216/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4929 - accuracy: 0.7547 - val_loss: 0.5773 - val_accuracy: 0.7344\n",
            "Epoch 217/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5054 - accuracy: 0.7620 - val_loss: 0.5880 - val_accuracy: 0.7178\n",
            "Epoch 218/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4870 - accuracy: 0.7597 - val_loss: 0.7881 - val_accuracy: 0.6729\n",
            "Epoch 219/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5245 - accuracy: 0.7449 - val_loss: 0.5791 - val_accuracy: 0.7285\n",
            "Epoch 220/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5047 - accuracy: 0.7517 - val_loss: 0.5803 - val_accuracy: 0.7314\n",
            "Epoch 221/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4982 - accuracy: 0.7543 - val_loss: 0.5973 - val_accuracy: 0.7207\n",
            "Epoch 222/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5038 - accuracy: 0.7504 - val_loss: 0.6119 - val_accuracy: 0.7080\n",
            "Epoch 223/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5211 - accuracy: 0.7426 - val_loss: 0.5989 - val_accuracy: 0.7148\n",
            "Epoch 224/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5143 - accuracy: 0.7503 - val_loss: 2.0470 - val_accuracy: 0.5615\n",
            "Epoch 225/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5129 - accuracy: 0.7522 - val_loss: 0.6268 - val_accuracy: 0.7012\n",
            "Epoch 226/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5146 - accuracy: 0.7472 - val_loss: 0.8069 - val_accuracy: 0.6768\n",
            "Epoch 227/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5015 - accuracy: 0.7544 - val_loss: 0.5767 - val_accuracy: 0.7197\n",
            "Epoch 228/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4920 - accuracy: 0.7518 - val_loss: 0.9611 - val_accuracy: 0.6191\n",
            "Epoch 229/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5525 - accuracy: 0.7292 - val_loss: 0.8060 - val_accuracy: 0.6553\n",
            "Epoch 230/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5281 - accuracy: 0.7377 - val_loss: 0.9838 - val_accuracy: 0.6211\n",
            "Epoch 231/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5813 - accuracy: 0.7188 - val_loss: 0.6290 - val_accuracy: 0.7188\n",
            "Epoch 232/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5079 - accuracy: 0.7564 - val_loss: 0.5808 - val_accuracy: 0.7188\n",
            "Epoch 233/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5197 - accuracy: 0.7491 - val_loss: 0.5722 - val_accuracy: 0.7148\n",
            "Epoch 234/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4786 - accuracy: 0.7725 - val_loss: 0.6564 - val_accuracy: 0.6973\n",
            "Epoch 235/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5049 - accuracy: 0.7523 - val_loss: 0.5637 - val_accuracy: 0.7246\n",
            "Epoch 236/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4962 - accuracy: 0.7641 - val_loss: 0.7202 - val_accuracy: 0.6904\n",
            "Epoch 237/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5148 - accuracy: 0.7536 - val_loss: 0.5921 - val_accuracy: 0.7285\n",
            "Epoch 238/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5085 - accuracy: 0.7499 - val_loss: 0.6980 - val_accuracy: 0.6934\n",
            "Epoch 239/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5207 - accuracy: 0.7511 - val_loss: 0.5775 - val_accuracy: 0.7217\n",
            "Epoch 240/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4998 - accuracy: 0.7532 - val_loss: 0.5977 - val_accuracy: 0.7139\n",
            "Epoch 241/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5095 - accuracy: 0.7461 - val_loss: 0.8287 - val_accuracy: 0.6758\n",
            "Epoch 242/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5200 - accuracy: 0.7384 - val_loss: 0.8811 - val_accuracy: 0.6357\n",
            "Epoch 243/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5257 - accuracy: 0.7343 - val_loss: 0.5917 - val_accuracy: 0.7197\n",
            "Epoch 244/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4792 - accuracy: 0.7570 - val_loss: 0.6897 - val_accuracy: 0.6777\n",
            "Epoch 245/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5038 - accuracy: 0.7552 - val_loss: 0.6102 - val_accuracy: 0.7090\n",
            "Epoch 246/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5160 - accuracy: 0.7445 - val_loss: 0.5691 - val_accuracy: 0.7246\n",
            "Epoch 247/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.5049 - accuracy: 0.7528 - val_loss: 0.7219 - val_accuracy: 0.6914\n",
            "Epoch 248/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5157 - accuracy: 0.7491 - val_loss: 0.5750 - val_accuracy: 0.7363\n",
            "Epoch 249/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5093 - accuracy: 0.7505 - val_loss: 0.5850 - val_accuracy: 0.7314\n",
            "Epoch 250/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5236 - accuracy: 0.7417 - val_loss: 0.5804 - val_accuracy: 0.7324\n",
            "Epoch 251/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4974 - accuracy: 0.7564 - val_loss: 0.5692 - val_accuracy: 0.7256\n",
            "Epoch 252/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4982 - accuracy: 0.7509 - val_loss: 0.5817 - val_accuracy: 0.7148\n",
            "Epoch 253/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4982 - accuracy: 0.7507 - val_loss: 0.5641 - val_accuracy: 0.7295\n",
            "Epoch 254/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5023 - accuracy: 0.7542 - val_loss: 0.5708 - val_accuracy: 0.7266\n",
            "Epoch 255/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5093 - accuracy: 0.7526 - val_loss: 0.9280 - val_accuracy: 0.6680\n",
            "Epoch 256/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5114 - accuracy: 0.7450 - val_loss: 0.5776 - val_accuracy: 0.7217\n",
            "Epoch 257/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5151 - accuracy: 0.7511 - val_loss: 0.6874 - val_accuracy: 0.7002\n",
            "Epoch 258/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4997 - accuracy: 0.7544 - val_loss: 0.6006 - val_accuracy: 0.7227\n",
            "Epoch 259/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5137 - accuracy: 0.7505 - val_loss: 0.6025 - val_accuracy: 0.7158\n",
            "Epoch 260/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5042 - accuracy: 0.7621 - val_loss: 0.6300 - val_accuracy: 0.7080\n",
            "Epoch 261/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5103 - accuracy: 0.7439 - val_loss: 0.5739 - val_accuracy: 0.7520\n",
            "Epoch 262/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5133 - accuracy: 0.7506 - val_loss: 0.5760 - val_accuracy: 0.7266\n",
            "Epoch 263/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5082 - accuracy: 0.7583 - val_loss: 0.5742 - val_accuracy: 0.7227\n",
            "Epoch 264/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5160 - accuracy: 0.7448 - val_loss: 0.5757 - val_accuracy: 0.7373\n",
            "Epoch 265/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4935 - accuracy: 0.7523 - val_loss: 0.5714 - val_accuracy: 0.7285\n",
            "Epoch 266/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4988 - accuracy: 0.7564 - val_loss: 0.5672 - val_accuracy: 0.7402\n",
            "Epoch 267/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5053 - accuracy: 0.7454 - val_loss: 0.6554 - val_accuracy: 0.7041\n",
            "Epoch 268/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4988 - accuracy: 0.7482 - val_loss: 0.5766 - val_accuracy: 0.7236\n",
            "Epoch 269/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4972 - accuracy: 0.7612 - val_loss: 0.5977 - val_accuracy: 0.7188\n",
            "Epoch 270/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4899 - accuracy: 0.7523 - val_loss: 0.5743 - val_accuracy: 0.7227\n",
            "Epoch 271/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4994 - accuracy: 0.7571 - val_loss: 0.5661 - val_accuracy: 0.7275\n",
            "Epoch 272/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4810 - accuracy: 0.7588 - val_loss: 0.5903 - val_accuracy: 0.7178\n",
            "Epoch 273/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5019 - accuracy: 0.7532 - val_loss: 0.5763 - val_accuracy: 0.7295\n",
            "Epoch 274/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4796 - accuracy: 0.7690 - val_loss: 0.5671 - val_accuracy: 0.7383\n",
            "Epoch 275/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4864 - accuracy: 0.7570 - val_loss: 0.5768 - val_accuracy: 0.7314\n",
            "Epoch 276/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4929 - accuracy: 0.7588 - val_loss: 0.5740 - val_accuracy: 0.7256\n",
            "Epoch 277/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4889 - accuracy: 0.7543 - val_loss: 0.5849 - val_accuracy: 0.7354\n",
            "Epoch 278/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5291 - accuracy: 0.7399 - val_loss: 0.5637 - val_accuracy: 0.7373\n",
            "Epoch 279/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4912 - accuracy: 0.7521 - val_loss: 0.5683 - val_accuracy: 0.7344\n",
            "Epoch 280/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4974 - accuracy: 0.7510 - val_loss: 0.5693 - val_accuracy: 0.7227\n",
            "Epoch 281/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4892 - accuracy: 0.7588 - val_loss: 0.5900 - val_accuracy: 0.7207\n",
            "Epoch 282/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5116 - accuracy: 0.7526 - val_loss: 0.5644 - val_accuracy: 0.7363\n",
            "Epoch 283/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5039 - accuracy: 0.7538 - val_loss: 0.5725 - val_accuracy: 0.7217\n",
            "Epoch 284/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4874 - accuracy: 0.7653 - val_loss: 0.5721 - val_accuracy: 0.7197\n",
            "Epoch 285/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4996 - accuracy: 0.7471 - val_loss: 0.5705 - val_accuracy: 0.7354\n",
            "Epoch 286/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5047 - accuracy: 0.7536 - val_loss: 0.6105 - val_accuracy: 0.7178\n",
            "Epoch 287/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5191 - accuracy: 0.7495 - val_loss: 0.6222 - val_accuracy: 0.7188\n",
            "Epoch 288/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4940 - accuracy: 0.7550 - val_loss: 0.5702 - val_accuracy: 0.7246\n",
            "Epoch 289/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5052 - accuracy: 0.7438 - val_loss: 0.5660 - val_accuracy: 0.7383\n",
            "Epoch 290/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5112 - accuracy: 0.7453 - val_loss: 0.5646 - val_accuracy: 0.7305\n",
            "Epoch 291/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4877 - accuracy: 0.7628 - val_loss: 0.5664 - val_accuracy: 0.7256\n",
            "Epoch 292/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4877 - accuracy: 0.7627 - val_loss: 0.5778 - val_accuracy: 0.7363\n",
            "Epoch 293/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4842 - accuracy: 0.7687 - val_loss: 0.5689 - val_accuracy: 0.7275\n",
            "Epoch 294/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4879 - accuracy: 0.7600 - val_loss: 0.5689 - val_accuracy: 0.7285\n",
            "Epoch 295/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4869 - accuracy: 0.7561 - val_loss: 0.5680 - val_accuracy: 0.7393\n",
            "Epoch 296/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5130 - accuracy: 0.7504 - val_loss: 0.5678 - val_accuracy: 0.7393\n",
            "Epoch 297/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5093 - accuracy: 0.7473 - val_loss: 0.5720 - val_accuracy: 0.7246\n",
            "Epoch 298/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5046 - accuracy: 0.7497 - val_loss: 0.5721 - val_accuracy: 0.7373\n",
            "Epoch 299/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5067 - accuracy: 0.7600 - val_loss: 0.5775 - val_accuracy: 0.7275\n",
            "Epoch 300/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4975 - accuracy: 0.7540 - val_loss: 0.5709 - val_accuracy: 0.7266\n",
            "Epoch 301/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4846 - accuracy: 0.7680 - val_loss: 0.5703 - val_accuracy: 0.7393\n",
            "Epoch 302/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4967 - accuracy: 0.7555 - val_loss: 0.5645 - val_accuracy: 0.7383\n",
            "Epoch 303/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5335 - accuracy: 0.7369 - val_loss: 0.5755 - val_accuracy: 0.7246\n",
            "Epoch 304/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4947 - accuracy: 0.7513 - val_loss: 0.5777 - val_accuracy: 0.7227\n",
            "Epoch 305/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5192 - accuracy: 0.7414 - val_loss: 0.5741 - val_accuracy: 0.7373\n",
            "Epoch 306/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5147 - accuracy: 0.7486 - val_loss: 0.5791 - val_accuracy: 0.7236\n",
            "Epoch 307/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5052 - accuracy: 0.7518 - val_loss: 0.5835 - val_accuracy: 0.7344\n",
            "Epoch 308/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5132 - accuracy: 0.7483 - val_loss: 0.5666 - val_accuracy: 0.7266\n",
            "Epoch 309/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5093 - accuracy: 0.7432 - val_loss: 0.5657 - val_accuracy: 0.7246\n",
            "Epoch 310/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4982 - accuracy: 0.7563 - val_loss: 0.5707 - val_accuracy: 0.7236\n",
            "Epoch 311/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4844 - accuracy: 0.7636 - val_loss: 0.5759 - val_accuracy: 0.7227\n",
            "Epoch 312/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4997 - accuracy: 0.7585 - val_loss: 0.5806 - val_accuracy: 0.7275\n",
            "Epoch 313/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5036 - accuracy: 0.7550 - val_loss: 0.5749 - val_accuracy: 0.7402\n",
            "Epoch 314/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4935 - accuracy: 0.7507 - val_loss: 0.5666 - val_accuracy: 0.7314\n",
            "Epoch 315/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5020 - accuracy: 0.7451 - val_loss: 0.5716 - val_accuracy: 0.7363\n",
            "Epoch 316/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5011 - accuracy: 0.7526 - val_loss: 0.5675 - val_accuracy: 0.7256\n",
            "Epoch 317/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4908 - accuracy: 0.7635 - val_loss: 0.5708 - val_accuracy: 0.7393\n",
            "Epoch 318/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5047 - accuracy: 0.7546 - val_loss: 0.5865 - val_accuracy: 0.7236\n",
            "Epoch 319/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5033 - accuracy: 0.7521 - val_loss: 0.5763 - val_accuracy: 0.7383\n",
            "Epoch 320/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5155 - accuracy: 0.7383 - val_loss: 0.5736 - val_accuracy: 0.7891\n",
            "Epoch 321/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4877 - accuracy: 0.7716 - val_loss: 0.5959 - val_accuracy: 0.7197\n",
            "Epoch 322/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4901 - accuracy: 0.7589 - val_loss: 0.5809 - val_accuracy: 0.7217\n",
            "Epoch 323/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4796 - accuracy: 0.7602 - val_loss: 0.5703 - val_accuracy: 0.7256\n",
            "Epoch 324/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5111 - accuracy: 0.7502 - val_loss: 0.5827 - val_accuracy: 0.7363\n",
            "Epoch 325/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5039 - accuracy: 0.7468 - val_loss: 0.5784 - val_accuracy: 0.7246\n",
            "Epoch 326/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4962 - accuracy: 0.7537 - val_loss: 0.5726 - val_accuracy: 0.7266\n",
            "Epoch 327/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5108 - accuracy: 0.7440 - val_loss: 0.5769 - val_accuracy: 0.7256\n",
            "Epoch 328/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5099 - accuracy: 0.7450 - val_loss: 0.5726 - val_accuracy: 0.7383\n",
            "Epoch 329/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5070 - accuracy: 0.7440 - val_loss: 0.5710 - val_accuracy: 0.7295\n",
            "Epoch 330/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4819 - accuracy: 0.7606 - val_loss: 0.5739 - val_accuracy: 0.7227\n",
            "Epoch 331/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4969 - accuracy: 0.7486 - val_loss: 1.9060 - val_accuracy: 0.6152\n",
            "Epoch 332/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5556 - accuracy: 0.7376 - val_loss: 0.5979 - val_accuracy: 0.7324\n",
            "Epoch 333/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5082 - accuracy: 0.7531 - val_loss: 0.7633 - val_accuracy: 0.6768\n",
            "Epoch 334/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5016 - accuracy: 0.7488 - val_loss: 0.5879 - val_accuracy: 0.7197\n",
            "Epoch 335/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5131 - accuracy: 0.7521 - val_loss: 2.0723 - val_accuracy: 0.6104\n",
            "Epoch 336/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.6649 - accuracy: 0.7194 - val_loss: 0.7330 - val_accuracy: 0.6689\n",
            "Epoch 337/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5533 - accuracy: 0.7338 - val_loss: 0.7738 - val_accuracy: 0.6611\n",
            "Epoch 338/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5473 - accuracy: 0.7282 - val_loss: 0.8912 - val_accuracy: 0.6250\n",
            "Epoch 339/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5119 - accuracy: 0.7514 - val_loss: 0.8228 - val_accuracy: 0.6748\n",
            "Epoch 340/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5064 - accuracy: 0.7487 - val_loss: 0.8041 - val_accuracy: 0.6416\n",
            "Epoch 341/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5144 - accuracy: 0.7543 - val_loss: 0.6256 - val_accuracy: 0.7002\n",
            "Epoch 342/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5039 - accuracy: 0.7577 - val_loss: 0.5711 - val_accuracy: 0.7334\n",
            "Epoch 343/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4830 - accuracy: 0.7593 - val_loss: 0.5724 - val_accuracy: 0.7207\n",
            "Epoch 344/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4899 - accuracy: 0.7616 - val_loss: 2.7331 - val_accuracy: 0.5273\n",
            "Epoch 345/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5516 - accuracy: 0.7313 - val_loss: 1.8181 - val_accuracy: 0.6133\n",
            "Epoch 346/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5758 - accuracy: 0.7305 - val_loss: 0.6392 - val_accuracy: 0.7158\n",
            "Epoch 347/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5196 - accuracy: 0.7378 - val_loss: 0.6779 - val_accuracy: 0.6953\n",
            "Epoch 348/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5208 - accuracy: 0.7435 - val_loss: 0.7477 - val_accuracy: 0.6572\n",
            "Epoch 349/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4966 - accuracy: 0.7567 - val_loss: 0.6180 - val_accuracy: 0.7197\n",
            "Epoch 350/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5212 - accuracy: 0.7479 - val_loss: 0.6315 - val_accuracy: 0.7109\n",
            "Epoch 351/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4988 - accuracy: 0.7491 - val_loss: 0.7174 - val_accuracy: 0.6729\n",
            "Epoch 352/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4888 - accuracy: 0.7566 - val_loss: 0.5899 - val_accuracy: 0.7705\n",
            "Epoch 353/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5043 - accuracy: 0.7595 - val_loss: 0.5794 - val_accuracy: 0.7227\n",
            "Epoch 354/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5048 - accuracy: 0.7550 - val_loss: 0.5718 - val_accuracy: 0.7227\n",
            "Epoch 355/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5278 - accuracy: 0.7444 - val_loss: 0.5867 - val_accuracy: 0.7227\n",
            "Epoch 356/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4981 - accuracy: 0.7549 - val_loss: 0.6062 - val_accuracy: 0.7158\n",
            "Epoch 357/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5188 - accuracy: 0.7482 - val_loss: 0.5928 - val_accuracy: 0.7334\n",
            "Epoch 358/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5097 - accuracy: 0.7463 - val_loss: 0.6304 - val_accuracy: 0.7178\n",
            "Epoch 359/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4972 - accuracy: 0.7562 - val_loss: 0.6030 - val_accuracy: 0.7266\n",
            "Epoch 360/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5015 - accuracy: 0.7488 - val_loss: 0.5719 - val_accuracy: 0.7354\n",
            "Epoch 361/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5002 - accuracy: 0.7590 - val_loss: 0.5759 - val_accuracy: 0.7285\n",
            "Epoch 362/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4921 - accuracy: 0.7591 - val_loss: 0.5673 - val_accuracy: 0.7227\n",
            "Epoch 363/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5069 - accuracy: 0.7472 - val_loss: 0.5846 - val_accuracy: 0.7188\n",
            "Epoch 364/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4972 - accuracy: 0.7548 - val_loss: 0.5857 - val_accuracy: 0.7266\n",
            "Epoch 365/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5117 - accuracy: 0.7543 - val_loss: 0.5820 - val_accuracy: 0.7266\n",
            "Epoch 366/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5017 - accuracy: 0.7509 - val_loss: 0.5733 - val_accuracy: 0.7314\n",
            "Epoch 367/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4767 - accuracy: 0.7661 - val_loss: 0.5840 - val_accuracy: 0.7363\n",
            "Epoch 368/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5028 - accuracy: 0.7519 - val_loss: 0.5793 - val_accuracy: 0.7207\n",
            "Epoch 369/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4898 - accuracy: 0.7471 - val_loss: 0.5710 - val_accuracy: 0.7285\n",
            "Epoch 370/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4797 - accuracy: 0.7584 - val_loss: 0.5714 - val_accuracy: 0.7373\n",
            "Epoch 371/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4939 - accuracy: 0.7547 - val_loss: 0.5757 - val_accuracy: 0.7227\n",
            "Epoch 372/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4990 - accuracy: 0.7484 - val_loss: 0.5741 - val_accuracy: 0.7256\n",
            "Epoch 373/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5180 - accuracy: 0.7389 - val_loss: 0.5858 - val_accuracy: 0.7217\n",
            "Epoch 374/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4754 - accuracy: 0.7634 - val_loss: 0.5781 - val_accuracy: 0.7393\n",
            "Epoch 375/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5220 - accuracy: 0.7431 - val_loss: 0.5748 - val_accuracy: 0.7246\n",
            "Epoch 376/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4963 - accuracy: 0.7612 - val_loss: 0.5748 - val_accuracy: 0.7256\n",
            "Epoch 377/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5089 - accuracy: 0.7541 - val_loss: 8.4790 - val_accuracy: 0.3027\n",
            "Epoch 378/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5914 - accuracy: 0.7199 - val_loss: 0.6419 - val_accuracy: 0.7051\n",
            "Epoch 379/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5089 - accuracy: 0.7506 - val_loss: 0.5783 - val_accuracy: 0.7217\n",
            "Epoch 380/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4973 - accuracy: 0.7531 - val_loss: 0.6076 - val_accuracy: 0.7188\n",
            "Epoch 381/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4970 - accuracy: 0.7575 - val_loss: 0.5719 - val_accuracy: 0.7246\n",
            "Epoch 382/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5047 - accuracy: 0.7427 - val_loss: 0.5729 - val_accuracy: 0.7236\n",
            "Epoch 383/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4936 - accuracy: 0.7619 - val_loss: 0.5690 - val_accuracy: 0.7217\n",
            "Epoch 384/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4986 - accuracy: 0.7535 - val_loss: 0.5810 - val_accuracy: 0.7334\n",
            "Epoch 385/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5038 - accuracy: 0.7459 - val_loss: 0.5742 - val_accuracy: 0.7236\n",
            "Epoch 386/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4976 - accuracy: 0.7703 - val_loss: 0.5759 - val_accuracy: 0.7227\n",
            "Epoch 387/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5250 - accuracy: 0.7392 - val_loss: 0.6063 - val_accuracy: 0.7500\n",
            "Epoch 388/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4979 - accuracy: 0.7560 - val_loss: 0.5784 - val_accuracy: 0.7207\n",
            "Epoch 389/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4960 - accuracy: 0.7517 - val_loss: 0.5703 - val_accuracy: 0.7314\n",
            "Epoch 390/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5001 - accuracy: 0.7488 - val_loss: 0.5895 - val_accuracy: 0.7227\n",
            "Epoch 391/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5110 - accuracy: 0.7525 - val_loss: 0.5711 - val_accuracy: 0.7383\n",
            "Epoch 392/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5021 - accuracy: 0.7538 - val_loss: 0.5739 - val_accuracy: 0.7246\n",
            "Epoch 393/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4938 - accuracy: 0.7537 - val_loss: 0.5716 - val_accuracy: 0.7393\n",
            "Epoch 394/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5106 - accuracy: 0.7427 - val_loss: 0.5763 - val_accuracy: 0.7227\n",
            "Epoch 395/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5067 - accuracy: 0.7512 - val_loss: 0.5879 - val_accuracy: 0.7227\n",
            "Epoch 396/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4942 - accuracy: 0.7527 - val_loss: 0.5782 - val_accuracy: 0.7256\n",
            "Epoch 397/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5234 - accuracy: 0.7336 - val_loss: 0.5816 - val_accuracy: 0.7314\n",
            "Epoch 398/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5022 - accuracy: 0.7429 - val_loss: 0.5817 - val_accuracy: 0.7246\n",
            "Epoch 399/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5070 - accuracy: 0.7480 - val_loss: 0.5789 - val_accuracy: 0.7363\n",
            "Epoch 400/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5041 - accuracy: 0.7493 - val_loss: 0.5784 - val_accuracy: 0.7236\n",
            "Epoch 401/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5204 - accuracy: 0.7482 - val_loss: 0.5759 - val_accuracy: 0.7275\n",
            "Epoch 402/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5209 - accuracy: 0.7438 - val_loss: 0.6562 - val_accuracy: 0.6973\n",
            "Epoch 403/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5146 - accuracy: 0.7501 - val_loss: 4.2649 - val_accuracy: 0.3848\n",
            "Epoch 404/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6740 - accuracy: 0.6997 - val_loss: 0.6992 - val_accuracy: 0.6973\n",
            "Epoch 405/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5184 - accuracy: 0.7430 - val_loss: 0.6123 - val_accuracy: 0.7168\n",
            "Epoch 406/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5242 - accuracy: 0.7422 - val_loss: 0.6029 - val_accuracy: 0.7197\n",
            "Epoch 407/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5124 - accuracy: 0.7498 - val_loss: 0.5878 - val_accuracy: 0.7217\n",
            "Epoch 408/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5187 - accuracy: 0.7390 - val_loss: 0.5820 - val_accuracy: 0.7363\n",
            "Epoch 409/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5068 - accuracy: 0.7511 - val_loss: 0.6575 - val_accuracy: 0.7002\n",
            "Epoch 410/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5106 - accuracy: 0.7472 - val_loss: 0.6222 - val_accuracy: 0.7148\n",
            "Epoch 411/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5041 - accuracy: 0.7490 - val_loss: 0.5809 - val_accuracy: 0.7344\n",
            "Epoch 412/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4965 - accuracy: 0.7538 - val_loss: 0.5859 - val_accuracy: 0.7217\n",
            "Epoch 413/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5150 - accuracy: 0.7458 - val_loss: 0.5803 - val_accuracy: 0.7363\n",
            "Epoch 414/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4852 - accuracy: 0.7632 - val_loss: 0.5871 - val_accuracy: 0.7227\n",
            "Epoch 415/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4907 - accuracy: 0.7523 - val_loss: 0.6130 - val_accuracy: 0.7168\n",
            "Epoch 416/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5093 - accuracy: 0.7485 - val_loss: 0.5780 - val_accuracy: 0.7295\n",
            "Epoch 417/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5011 - accuracy: 0.7593 - val_loss: 0.5750 - val_accuracy: 0.7275\n",
            "Epoch 418/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4833 - accuracy: 0.7565 - val_loss: 0.5777 - val_accuracy: 0.7227\n",
            "Epoch 419/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.5075 - accuracy: 0.7495 - val_loss: 0.5794 - val_accuracy: 0.7236\n",
            "Epoch 420/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4951 - accuracy: 0.7581 - val_loss: 0.5683 - val_accuracy: 0.7256\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00420: early stopping\n",
            "\n",
            "Accuracy: 72.56%\n",
            "Loss: 0.5682880282402039 \n",
            "\n",
            "NEXT SET OF HYPERPARAMETERS IS: \n",
            " num_conv_layers: 2 \n",
            " num_conv_nodes: 227 \n",
            " num_dense_layers: 3 \n",
            " num_dense_nodes: 247 \n",
            "\n",
            "Epoch 1/500\n",
            "128/128 [==============================] - 2s 12ms/step - loss: 2.8733 - accuracy: 0.3268 - val_loss: 2.7342 - val_accuracy: 0.1152\n",
            "Epoch 2/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 1.0237 - accuracy: 0.5572 - val_loss: 3.5874 - val_accuracy: 0.1113\n",
            "Epoch 3/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.9206 - accuracy: 0.5728 - val_loss: 3.9484 - val_accuracy: 0.1104\n",
            "Epoch 4/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.8495 - accuracy: 0.6058 - val_loss: 2.8314 - val_accuracy: 0.2129\n",
            "Epoch 5/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.8314 - accuracy: 0.6088 - val_loss: 0.8869 - val_accuracy: 0.6123\n",
            "Epoch 6/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.8010 - accuracy: 0.6118 - val_loss: 0.9320 - val_accuracy: 0.5840\n",
            "Epoch 7/500\n",
            "128/128 [==============================] - 1s 11ms/step - loss: 0.8179 - accuracy: 0.6038 - val_loss: 0.9545 - val_accuracy: 0.5439\n",
            "Epoch 8/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7998 - accuracy: 0.6335 - val_loss: 0.8168 - val_accuracy: 0.6123\n",
            "Epoch 9/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7719 - accuracy: 0.6344 - val_loss: 0.7833 - val_accuracy: 0.6309\n",
            "Epoch 10/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7483 - accuracy: 0.6438 - val_loss: 0.7577 - val_accuracy: 0.6426\n",
            "Epoch 11/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7467 - accuracy: 0.6415 - val_loss: 0.7757 - val_accuracy: 0.6514\n",
            "Epoch 12/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7421 - accuracy: 0.6500 - val_loss: 0.7563 - val_accuracy: 0.6309\n",
            "Epoch 13/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7331 - accuracy: 0.6495 - val_loss: 0.7641 - val_accuracy: 0.6270\n",
            "Epoch 14/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7367 - accuracy: 0.6564 - val_loss: 0.7616 - val_accuracy: 0.6611\n",
            "Epoch 15/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7485 - accuracy: 0.6531 - val_loss: 0.7317 - val_accuracy: 0.6416\n",
            "Epoch 16/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7472 - accuracy: 0.6533 - val_loss: 0.7285 - val_accuracy: 0.6797\n",
            "Epoch 17/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7840 - accuracy: 0.6429 - val_loss: 0.7350 - val_accuracy: 0.6475\n",
            "Epoch 18/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7403 - accuracy: 0.6500 - val_loss: 0.7014 - val_accuracy: 0.7080\n",
            "Epoch 19/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7192 - accuracy: 0.6926 - val_loss: 0.7645 - val_accuracy: 0.6240\n",
            "Epoch 20/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6947 - accuracy: 0.6941 - val_loss: 0.6641 - val_accuracy: 0.6914\n",
            "Epoch 21/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7213 - accuracy: 0.6862 - val_loss: 0.7655 - val_accuracy: 0.6494\n",
            "Epoch 22/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6715 - accuracy: 0.7073 - val_loss: 0.7796 - val_accuracy: 0.6514\n",
            "Epoch 23/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6400 - accuracy: 0.7215 - val_loss: 0.7173 - val_accuracy: 0.6455\n",
            "Epoch 24/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6556 - accuracy: 0.7258 - val_loss: 0.9358 - val_accuracy: 0.5303\n",
            "Epoch 25/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7137 - accuracy: 0.6828 - val_loss: 0.8422 - val_accuracy: 0.6201\n",
            "Epoch 26/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6746 - accuracy: 0.7104 - val_loss: 1.7061 - val_accuracy: 0.3311\n",
            "Epoch 27/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7231 - accuracy: 0.6839 - val_loss: 1.1213 - val_accuracy: 0.4424\n",
            "Epoch 28/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6996 - accuracy: 0.6814 - val_loss: 2.2582 - val_accuracy: 0.2520\n",
            "Epoch 29/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6792 - accuracy: 0.6915 - val_loss: 1.7972 - val_accuracy: 0.3096\n",
            "Epoch 30/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7434 - accuracy: 0.6708 - val_loss: 2.8611 - val_accuracy: 0.2705\n",
            "Epoch 31/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6530 - accuracy: 0.7185 - val_loss: 0.9659 - val_accuracy: 0.5615\n",
            "Epoch 32/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6674 - accuracy: 0.6946 - val_loss: 1.2712 - val_accuracy: 0.4355\n",
            "Epoch 33/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7543 - accuracy: 0.6371 - val_loss: 0.8589 - val_accuracy: 0.6387\n",
            "Epoch 34/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7408 - accuracy: 0.6631 - val_loss: 4.4359 - val_accuracy: 0.3398\n",
            "Epoch 35/500\n",
            "128/128 [==============================] - 1s 11ms/step - loss: 0.7103 - accuracy: 0.6651 - val_loss: 1.0602 - val_accuracy: 0.4854\n",
            "Epoch 36/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7303 - accuracy: 0.6531 - val_loss: 0.9913 - val_accuracy: 0.4980\n",
            "Epoch 37/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7162 - accuracy: 0.6538 - val_loss: 0.8426 - val_accuracy: 0.6260\n",
            "Epoch 38/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7039 - accuracy: 0.6576 - val_loss: 0.7596 - val_accuracy: 0.6523\n",
            "Epoch 39/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7313 - accuracy: 0.6460 - val_loss: 0.7167 - val_accuracy: 0.6777\n",
            "Epoch 40/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7261 - accuracy: 0.6476 - val_loss: 0.7611 - val_accuracy: 0.6436\n",
            "Epoch 41/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7212 - accuracy: 0.6541 - val_loss: 0.7564 - val_accuracy: 0.6406\n",
            "Epoch 42/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7081 - accuracy: 0.6455 - val_loss: 0.8362 - val_accuracy: 0.5859\n",
            "Epoch 43/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7131 - accuracy: 0.6554 - val_loss: 0.7153 - val_accuracy: 0.6602\n",
            "Epoch 44/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7127 - accuracy: 0.6589 - val_loss: 0.9172 - val_accuracy: 0.5498\n",
            "Epoch 45/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6904 - accuracy: 0.6556 - val_loss: 1.6022 - val_accuracy: 0.4551\n",
            "Epoch 46/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6975 - accuracy: 0.6529 - val_loss: 0.8219 - val_accuracy: 0.5986\n",
            "Epoch 47/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6996 - accuracy: 0.6546 - val_loss: 0.7443 - val_accuracy: 0.6416\n",
            "Epoch 48/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7311 - accuracy: 0.6585 - val_loss: 0.7536 - val_accuracy: 0.6455\n",
            "Epoch 49/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7101 - accuracy: 0.6695 - val_loss: 0.7052 - val_accuracy: 0.6738\n",
            "Epoch 50/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6986 - accuracy: 0.6681 - val_loss: 0.7200 - val_accuracy: 0.6621\n",
            "Epoch 51/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7092 - accuracy: 0.6555 - val_loss: 0.7023 - val_accuracy: 0.6865\n",
            "Epoch 52/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7181 - accuracy: 0.6664 - val_loss: 0.7281 - val_accuracy: 0.6504\n",
            "Epoch 53/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7001 - accuracy: 0.6568 - val_loss: 0.7279 - val_accuracy: 0.6602\n",
            "Epoch 54/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7030 - accuracy: 0.6643 - val_loss: 0.7133 - val_accuracy: 0.6631\n",
            "Epoch 55/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7091 - accuracy: 0.6647 - val_loss: 0.7523 - val_accuracy: 0.6396\n",
            "Epoch 56/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6837 - accuracy: 0.6742 - val_loss: 0.7569 - val_accuracy: 0.6318\n",
            "Epoch 57/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6909 - accuracy: 0.6599 - val_loss: 5.7949 - val_accuracy: 0.3574\n",
            "Epoch 58/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7072 - accuracy: 0.6451 - val_loss: 0.7371 - val_accuracy: 0.6367\n",
            "Epoch 59/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6913 - accuracy: 0.6692 - val_loss: 0.7026 - val_accuracy: 0.6797\n",
            "Epoch 60/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6879 - accuracy: 0.6687 - val_loss: 0.7353 - val_accuracy: 0.6602\n",
            "Epoch 61/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6899 - accuracy: 0.6705 - val_loss: 0.8356 - val_accuracy: 0.6260\n",
            "Epoch 62/500\n",
            "128/128 [==============================] - 1s 11ms/step - loss: 0.7047 - accuracy: 0.6579 - val_loss: 0.7242 - val_accuracy: 0.6572\n",
            "Epoch 63/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6684 - accuracy: 0.6830 - val_loss: 0.7622 - val_accuracy: 0.6270\n",
            "Epoch 64/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6942 - accuracy: 0.6777 - val_loss: 0.6909 - val_accuracy: 0.6885\n",
            "Epoch 65/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7058 - accuracy: 0.6647 - val_loss: 0.7122 - val_accuracy: 0.6680\n",
            "Epoch 66/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7230 - accuracy: 0.6416 - val_loss: 0.7447 - val_accuracy: 0.6533\n",
            "Epoch 67/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6858 - accuracy: 0.6651 - val_loss: 0.7243 - val_accuracy: 0.6533\n",
            "Epoch 68/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6924 - accuracy: 0.6592 - val_loss: 0.7237 - val_accuracy: 0.6650\n",
            "Epoch 69/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6941 - accuracy: 0.6623 - val_loss: 0.7349 - val_accuracy: 0.6426\n",
            "Epoch 70/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6904 - accuracy: 0.6745 - val_loss: 0.7744 - val_accuracy: 0.6279\n",
            "Epoch 71/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6937 - accuracy: 0.6704 - val_loss: 0.7061 - val_accuracy: 0.6826\n",
            "Epoch 72/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6963 - accuracy: 0.6660 - val_loss: 0.7361 - val_accuracy: 0.6396\n",
            "Epoch 73/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6791 - accuracy: 0.6722 - val_loss: 0.7182 - val_accuracy: 0.6377\n",
            "Epoch 74/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6825 - accuracy: 0.6716 - val_loss: 0.7094 - val_accuracy: 0.6719\n",
            "Epoch 75/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6827 - accuracy: 0.6630 - val_loss: 0.7256 - val_accuracy: 0.6475\n",
            "Epoch 76/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6852 - accuracy: 0.6638 - val_loss: 0.7072 - val_accuracy: 0.6709\n",
            "Epoch 77/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6976 - accuracy: 0.6525 - val_loss: 0.6967 - val_accuracy: 0.6699\n",
            "Epoch 78/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6921 - accuracy: 0.6574 - val_loss: 0.7245 - val_accuracy: 0.6650\n",
            "Epoch 79/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6764 - accuracy: 0.6725 - val_loss: 0.6953 - val_accuracy: 0.6719\n",
            "Epoch 80/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6793 - accuracy: 0.6743 - val_loss: 0.7028 - val_accuracy: 0.6670\n",
            "Epoch 81/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6749 - accuracy: 0.6747 - val_loss: 0.7009 - val_accuracy: 0.6650\n",
            "Epoch 82/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6995 - accuracy: 0.6451 - val_loss: 0.7093 - val_accuracy: 0.6611\n",
            "Epoch 83/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6977 - accuracy: 0.6603 - val_loss: 0.6888 - val_accuracy: 0.6807\n",
            "Epoch 84/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6781 - accuracy: 0.6711 - val_loss: 0.7087 - val_accuracy: 0.6836\n",
            "Epoch 85/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6744 - accuracy: 0.6771 - val_loss: 0.7105 - val_accuracy: 0.6729\n",
            "Epoch 86/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6592 - accuracy: 0.6835 - val_loss: 0.7092 - val_accuracy: 0.6680\n",
            "Epoch 87/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6550 - accuracy: 0.6777 - val_loss: 0.7144 - val_accuracy: 0.6572\n",
            "Epoch 88/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6745 - accuracy: 0.6699 - val_loss: 0.7779 - val_accuracy: 0.6250\n",
            "Epoch 89/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6863 - accuracy: 0.6721 - val_loss: 0.7194 - val_accuracy: 0.6699\n",
            "Epoch 90/500\n",
            "128/128 [==============================] - 1s 11ms/step - loss: 0.7011 - accuracy: 0.6614 - val_loss: 0.6920 - val_accuracy: 0.6650\n",
            "Epoch 91/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6800 - accuracy: 0.6554 - val_loss: 0.8729 - val_accuracy: 0.6143\n",
            "Epoch 92/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6979 - accuracy: 0.6623 - val_loss: 0.7077 - val_accuracy: 0.6670\n",
            "Epoch 93/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6666 - accuracy: 0.6845 - val_loss: 0.6994 - val_accuracy: 0.6719\n",
            "Epoch 94/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6740 - accuracy: 0.6717 - val_loss: 0.7209 - val_accuracy: 0.6484\n",
            "Epoch 95/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6759 - accuracy: 0.6816 - val_loss: 0.6983 - val_accuracy: 0.6680\n",
            "Epoch 96/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6843 - accuracy: 0.6655 - val_loss: 0.6870 - val_accuracy: 0.6846\n",
            "Epoch 97/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6899 - accuracy: 0.6719 - val_loss: 0.7184 - val_accuracy: 0.6611\n",
            "Epoch 98/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6748 - accuracy: 0.6774 - val_loss: 0.7191 - val_accuracy: 0.6699\n",
            "Epoch 99/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6693 - accuracy: 0.6766 - val_loss: 0.7166 - val_accuracy: 0.6631\n",
            "Epoch 100/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6767 - accuracy: 0.6703 - val_loss: 0.7007 - val_accuracy: 0.6992\n",
            "Epoch 101/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6823 - accuracy: 0.6710 - val_loss: 0.6775 - val_accuracy: 0.6953\n",
            "Epoch 102/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6801 - accuracy: 0.6675 - val_loss: 0.6970 - val_accuracy: 0.6660\n",
            "Epoch 103/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6785 - accuracy: 0.6689 - val_loss: 0.7366 - val_accuracy: 0.6592\n",
            "Epoch 104/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6918 - accuracy: 0.6710 - val_loss: 0.6934 - val_accuracy: 0.6777\n",
            "Epoch 105/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6696 - accuracy: 0.6720 - val_loss: 0.7212 - val_accuracy: 0.6582\n",
            "Epoch 106/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6825 - accuracy: 0.6604 - val_loss: 0.6942 - val_accuracy: 0.6865\n",
            "Epoch 107/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6690 - accuracy: 0.6766 - val_loss: 0.6986 - val_accuracy: 0.6621\n",
            "Epoch 108/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6961 - accuracy: 0.6620 - val_loss: 0.6964 - val_accuracy: 0.6719\n",
            "Epoch 109/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6510 - accuracy: 0.6826 - val_loss: 0.6917 - val_accuracy: 0.6719\n",
            "Epoch 110/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6622 - accuracy: 0.6757 - val_loss: 0.7337 - val_accuracy: 0.6445\n",
            "Epoch 111/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6560 - accuracy: 0.6823 - val_loss: 0.7093 - val_accuracy: 0.6602\n",
            "Epoch 112/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6679 - accuracy: 0.6790 - val_loss: 0.7119 - val_accuracy: 0.6680\n",
            "Epoch 113/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6986 - accuracy: 0.6633 - val_loss: 0.7334 - val_accuracy: 0.6543\n",
            "Epoch 114/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6860 - accuracy: 0.6662 - val_loss: 0.7021 - val_accuracy: 0.6797\n",
            "Epoch 115/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6709 - accuracy: 0.6688 - val_loss: 0.7052 - val_accuracy: 0.6689\n",
            "Epoch 116/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.7024 - accuracy: 0.6510 - val_loss: 0.7523 - val_accuracy: 0.6367\n",
            "Epoch 117/500\n",
            "128/128 [==============================] - 1s 10ms/step - loss: 0.6730 - accuracy: 0.6641 - val_loss: 0.6841 - val_accuracy: 0.6865\n",
            "Epoch 118/500\n",
            "128/128 [==============================] - 1s 11ms/step - loss: 0.6612 - accuracy: 0.6728 - val_loss: 0.6754 - val_accuracy: 0.6943\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00118: early stopping\n",
            "\n",
            "Accuracy: 69.43%\n",
            "Loss: 0.6754029393196106 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWd32Qtz7irF"
      },
      "source": [
        "## 6.6.Second stage results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4a6yHTkAy4m"
      },
      "source": [
        "**Results of the 2nd stage of optimization:**\r\n",
        "*  Convergence plot (visualization of the fitness function after *n* iterations of the optimizer);\r\n",
        "*  Objective plot (visualization of the influence of each search-space dimension on the objective function);\r\n",
        "*  The history of analyzed hyperparameters sets;\r\n",
        "*  The time spent in the optimization process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 966
        },
        "id": "BnO2ZKnTsp2c",
        "outputId": "d510f858-8e12-47cf-ea92-ad613830c4d8"
      },
      "source": [
        "# Visualização das métricas:\r\n",
        "\r\n",
        "skopt.plots.plot_convergence(result)\r\n",
        "plt.savefig('/content/drive/My Drive/MESTRADO - UFES/optimizer convergence B.png', transparent=True, bbox_inches='tight', dpi=600)\r\n",
        "\r\n",
        "\r\n",
        "skopt.plots.plot_objective(result,\r\n",
        "                           levels=100,\r\n",
        "                           n_points=100,\r\n",
        "                           n_samples=250,\r\n",
        "                           size=2.25,\r\n",
        "                           zscale='linear',\r\n",
        "                           dimensions=None,\r\n",
        "                           sample_source='random',\r\n",
        "                           minimum='result',\r\n",
        "                           n_minimum_search=None,\r\n",
        "                           plot_dims=None,\r\n",
        "                           show_points=True,\r\n",
        "                           cmap='viridis_r')\r\n",
        "plt.savefig('/content/drive/My Drive/MESTRADO - UFES/space exploration B.png', transparent=True, bbox_inches='tight', dpi=600)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqkAAAKdCAYAAAAAzfB8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzde3ycZZn/8c83aUtoORRogVKoRUUUUE4RwcMuB0WLBVxFARVZdCkUVFBWFHVFBP2pIC54KJQFxV2WgwpKu1ROKgq7gG05U1QU1B7oQUqBlLRNcv3+eO6h0zRpZiaTPM8k3/frNa/M88w9931NCOmV+6iIwMzMzMysSJryDsDMzMzMrDsnqWZmZmZWOE5SzczMzKxwnKSamZmZWeE4STUzMzOzwnGSamZmZmaF4yTVzMzMzArHSaoBIOlpSePyjsPMzMwMnKTaIJHUnHcMZmZm1jicpFZI0mRJCyRdIekxSbdJ2lzSryW1pjLjJD2dnv+zpJ9Juj31Un5c0qclPSDpXknbbqKtV0u6Q9JDkuZLepUyF0p6VNIjko5NZQ9OMfxE0hOSrkll3yXpx2V1HixpdoWf9WeS5qXPOS3d+6ikfy8rc7Kkb6fnH5Z0v6QHJV1eSkglvSjpW5IeAg6S9HVJj0t6WNJF1f43MDMzs+HDSWp1dgO+FxF7As8B7+uj/F7Ae4E3Al8FVkfEvsD/AR/ZxPuuSe3sDbwZWJLq2QfYG3g7cKGkCan8vsCZwB7AK4G3AHcAb5I0JpU5Friuws/50YjYH2gFPilpO+AG4EhJI1OZk4CrJL0u1f2WiNgH6AQ+lMqMAe5Ln2MB8E/AnhHxBuCCCmMxMzOzYchJanWeiogH0/N5wOQ+yv8qIl6IiOXAKmBWuv9Ib++VtCUwMSJuAoiI9ohYDbwVuDYiOiNiKXAXWfILcH9ELIyILuBBYHJEdAC/IEssRwDvBn5e4ef8ZOr9vBfYBdgtIl4EfglMlfRaYGREPAIcBuwP/E7Sg+n6lameTuCn6fkqoB24UtJ7gdUVxmJmZmbD0Ii8A2gwa8qedwKbAx2sT/ZbNlG+q+y6i/p+77vHVar7OuDjwLPA3Ih4oa+KJB1M1lN7UESslvRr1n+u/wA+DzwB/KD0FuDqiDinh+raI6ITICI6JB1AlsQek+I6tNIPaGZmZsOLe1L772mynkTIkq9+SYnkQknvAZC0maTRwG+BYyU1SxoP/ANwfx/V3QXsB5xM5UP9WwMrU4L6WuDAstjuI+tZ/SBwbbp9J3CMpO1TvNtKekX3SiVtAWwdEbcAnyKbtmBmZmbWIyep/XcRMF3SA0C9tnA6gWzI/WHgf4EdgZuAh4GHyIbdz46IZzZVSerFnA1MSV8r8QtghKQFwNfJhvzL3QDcExErUxuPA18Ebkvx3g5MYGNbArNTmbuBT1cYj5mZmQ1Dioi8Y7AGknYI+HZE3Jl3LGZmZjZ0uSfVKiJprKQ/AC85QTUzM7OB5p7UHEn6Htl2UeUuiYgf9FS+Tm3eB2zW7fYJaaW+mZmZWSG4J7UbSbtI+lXadP4xSWdsouwbJXVIqnXB1NeBlcAoYCTwg+4JqqStJc1KG/s/JumkWhqS1CLpfrKV+iOBn0fEPunxSCrz6bLN9u/saQFUNW2VxXxeL+U+UPZ9/u9a2jIzM7OhyT2p3aQN8idExPy0Z+k84D1pgVB5uWayRULtwFUR8ZOBaEvS58lWxX82rer/PbBjRKytsi0BYyLixbQh/93AGRFxb1mZQ8g2318taTpwcEQcW8PnqqSt3cgWYR0aESslbR8Ry6pty8zMzIYm96R2ExFLImJ+ev4C2UlJE3so+gmyjeprTqwqbCuALVPitwXZnqcdNbQVaUN+yHpSR6a6y8v8Kh0cANmq/p2rbafStsi2xfpe2S4BTlDNzMzsZU5SN0HSZLIjR+/rdn8i2RGfMwa6LeC7wOuAxWQnVZ2RTpaqpY3mdCrUMuD2tO9pbz4GzKmlnQrbeg3wGkn3SLpX0rtqbcvMzMyGHiepvUibz/8UODMinu/28r8Dn601WayyrXeSHXW6E7AP8F1JW9XSTjpSdR+yHtIDJO3VSzwfBlqBC2tpp8K2RgC7AQcDxwNXSBpba3tmZmY2tDhJ7UGaR/lT4JqIuLGHIq3AdZKeJjtl6vulE6IGoK2TgBvTEPqTwFPAa2tpqyQingN+BWzUeynp7cAXgKMiYk331+vY1kLg5ohYFxFPAX8gS1rNzMzMnKR2l+Z+XgksiIiLeyoTEbtGxOSImAz8BDgtIn42EG0BfyU77x5JOwC7A3+uoa3xpZ5KSZsD7wCe6FZmX+BysgS15jmilbQF/IysFxVJ48iG/6v+XGZmZjY0jcg7gAJ6C9mxpI+kOZUAnwcmAUTEZYPc1vnADyU9AohsmsGKGtqaAFyddiVoAm6IiNmSvgLMjYibyYb3twB+nOXP/DUijhqgtm4FDpf0ONAJfCYi/l5DW2ZmZjYEeQsqMzMzMyscD/ebmZmZWeE4STUzMzOzwnGSamZmZmaF4yTVzMzMzArHSaqZmZmZFY6T1CpImua2Gq89MzMzazxOUqszmMnVUG0rj/bMzMyswThJNTMzM7PCafjN/MeNGxeTJ08elLaWL1/O+PHj3VYDtTdv3rwVETF4H87MzMzqouGPRZ08eTJz587NOwwrKEl/yTsGMzMzq56H+83MzMyscJykmpmZmVnhOEk1MzMzs8JxkmpmZmZmheMk1czMzMwKx0mqmZmZmRWOk1QzMzMzKxwnqWZmZmZWOE5SzczMzKxwnKSamZmZWeE4STUzMzOzwnGSamZmZmaF4yTVzMzMzArHSaqZmZmZFY6TVDMzMzMrHCepZmZmZlY4TlLNzMzMrHCcpJqZmZlZ4ThJtWFN0lWSlkl6tOzetpJul/TH9HWbXt57YirzR0knDl7UZmZmQ5+TVBvufgi8q9u9zwF3RsRuwJ3pegOStgXOBd4EHACc21sym8rvIOlKSXPS9R6SPlafj2BmZjb0OEm1YS0ifgM82+320cDV6fnVwHt6eOs7gdsj4tmIWAnczsbJbrkfArcCO6XrPwBn1hi2mZnZkOck1WxjO0TEkvT8GWCHHspMBP5Wdr0w3evNuIi4AegCiIgOoLMOsZqZmQ1JDZmkSpomaa6kucuXL887HCu2caWflfSYVs2bIyKAqEMcbZK2K9Ul6UBgVR3qNTMzG5JG5B1ALSJiJjAToLW1tR4JhA1dKyKitcr3LJU0ISKWSJoALOuhzCLg4LLrnYFfb6LOTwM3A6+SdA8wHjimyrjMzMyGjYbsSTUbYDcDpdX6JwI/76HMrcDhkrZJC6YOT/d6FBHzgX8E3gycAuwZEQ/XNWozM7MhxEmqDWuSrgX+D9hd0sK04v7rwDsk/RF4e7pGUquk/wCIiGeB84HfpcdX0r3e2jkd2CIiHouIR4EtJJ02kJ/NzMyskSmbcte4WltbY+7cuXmHYQUlaV4Nw/0DEceDEbFPt3sPRMS+ecVkZmZWZO5JNRsczZJUupDUDIzKMR4zM7NCa8iFU2YN6BfA9ZIuT9enpHtmZmbWAyepZoPjs2SJ6fR0fTvwH/mFY2ZmVmxOUs0GQUR0ATPSw8zMzPrgJNVsEEh6C/Bl4BVk/9+J7KyAV+YZl5mZWVE5STUbHFcCnwLm4eNQzczM+uQk1WxwrIqIOXkHYWZm1iicpJoNjl9JuhC4EVhTuplOojIzM7NunKSaDY43pa/lBwsEcGgOsZiZmRWek1SzQRARh+Qdg5mZWSPxiVNmg0DSDpKulDQnXe8h6WN5x2VmZlZUTlLNBscPgVuBndL1H4Azc4vGzMys4Jykmg2OcRFxA9AFEBEdeCsqMzOzXjlJNRscbZK2I1sshaQDgVX5hmRmZlZcXjhlNjg+DdwMvErSPcB44Jh8QzIzMysuJ6lmgyAi5kv6R2B3siNRfx8R63IOy8zMrLCcpJoNIEnv7eWl10giIm4c1IDMzMwahJNUs4F1ZPq6PfBm4Jfp+hDgf8lOoDIzM7NuvHDKhjVJu0t6sOzxvKQzu5U5WNKqsjJfqrT+iDgpIk4CRgJ7RMT7IuJ9wJ7pnpmZmfXAPak2rEXE74F9ACQ1A4uAm3oo+tuImNqPpnaJiCVl10uBSf2oz8zMbEhzkmq23mHAnyLiLwNQ952SbgWuTdfHAncMQDtmZmZDgpNUs/WOY30S2d1Bkh4CFgP/GhGPVVNxRHw8LaJ6W7o1MyJ66rE1MzMzQBGRdwxVkzQNmAYwadKk/f/yl4Ho+LKhQNJfgBVlt2ZGxMweyo0iS0D3jIil3V7bCuiKiBclHQFcEhG7DWTcZmZmw11DJqnlWltbY+7cuXmHYQUlaV5EtFZQ7mjg9Ig4vIKyTwOtEbGir7Jl73kv8A2yVf5Kj4iIrSqtw8zMbDjxcL9Z5nh6GeqXtCOwNCJC0gFku2L8vcr6vwkcGREL+hemmZnZ8OAk1YY9SWOAdwCnlN07FSAiLiM7vnS6pA7gJeC4qH4IYqkTVDMzs8o5SbVhLyLagO263bus7Pl3ge/2s5m5kq4HfgasKavbm/mbmZn1wEmq2eDYClgNlM95DXzilJmZWY+cpJoNgnTqlJmZmVXIx6KaDQJJr5F0p6RH0/UbJH0x77jMzMyKykmq2eC4AjgHWAcQEQ+THR5gZmZmPXCSajY4RkfE/d3udeQSiZmZWQNwkmo2OFZIehXZYikkHQMsyTckMzOz4vLCKbPBcTowE3itpEXAU8CH8g3JzMysuJykmg2CiPgz8PZ0cEBTRLyQd0xmZmZF5uF+s0EgaTtJlwK/BX4t6RJJ2/X1PjMzs+HKSarZ4LgOWA68j+yY1eXA9blGZGZmVmAe7jcbHBMi4vyy6wskHZtbNGZmZgXnnlSzwXGbpOMkNaXHB4Bb8w7KzMysqJykmg2Ok4H/BtYCa8iG/0+R9IKk53ONzMzMrIA83G82CCJiy7xjMDMzayTuSTUbBMp8WNK/petdJB2Qd1xmZmZF5STVbHB8HzgI+GC6fhH4Xn7hmJmZFZuH+80Gx5siYj9JDwBExEpJo/IOyszMrKjck2o2ONZJagYCQNJ4oCvfkMzMzIrLSaoNe5KelvSIpAclze3hdUm6VNKTkh6WtF8NzVwK3ARsL+mrwN3A1/oZupmZ2ZDl4X6zzCERsaKX16YAu6XHm4AZ6WvFIuIaSfOAwwAB74mIBf2I18zMbEhzkmrWt6OBH0VEAPdKGitpQkQs6euNkrYtu1wGXFv+WkQ8W/9wzczMGp+TVLNsnuhtkgK4PCJmdnt9IvC3suuF6V6fSSowL9UvYBKwMj0fC/wV2LV/oZuZmQ1NDZmkSpoGTAOYNGlSztFYwY3rNs90Zg9J6FsjYpGk7YHbJT0REb+pR+MRsSuApCuAmyLilnQ9BXhPPdowMzMbihpy4VREzIyI1ohoHT9+fN7hWLGtKP2spEf3BJWIWJS+LiNb3NR9k/1FwC5l1zune9U4sJSgprbmAG+usg4zM7NhoyGTVLN6kTRG0pal58DhwKPdit0MfCSt8j8QWFXJfNRuFkv6oqTJ6fEFYHG/P4CZmdkQ1ZDD/WZ1tANwkyTI/n/474j4haRTASLiMuAW4AjgSWA1cFIN7RwPnEvWUxvAb9I9MzMz64GyBcuNq7W1NebO3WhrSzMAJM2LiNa84zAzM7PqeLjfzMzMzArHSaqZmZmZFY6TVDMzMzMrHC+cMhtAkr5DtlCqRxHxyUEMx8zMrGE4STUbWF7VZ2ZmVgMnqWYDKCKuzjsGMzOzRuQk1WwQSBoPfBbYA2gp3Y+IQ3MLyszMrMC8cMpscFwDLAB2Bc4DngZ+l2dAZmZmReYk1WxwbBcRVwLrIuKuiPgo4F5UMzOzXni432xwrEtfl0h6N7AY2DbHeMzMzArNSarZ4LhA0tbAWcB3gK2AT+UbkpmZWXE5STUbBBExOz1dBRySZyxmZmaNwEmq2QCSdHZEfLO3Tf29mb+ZmVnPnKSaDawF6as39TczM6uCk1SzARQRs9LT1RHx4/LXJL0/h5DMzMwagregMhsc51R4z8zMzHBPqtmAkjQFOAKYKOnSspe2AjryicrMzKz4nKSaDazFZPNRjwLmld1/AW9BZWZm1isnqTZsSdoF+BGwA9nK+5kRcUm3MgcDPweeSrdujIivVNpGRDwk6VHgnRFxdV0CNzMzGwacpNpw1gGcFRHzJW0JzJN0e0Q83q3cbyNiaq2NRESnpF0kjYqItf2K2MzMbJhwkmrDVkQsAZak5y9IWgBMBLonqfXwFHCPpJuBtrIYLh6AtszMzBqek1QzQNJkYF/gvh5ePkjSQ2TzS/81Ih6roYk/pUcTsGWNYZqZmQ0bDZmkSpoGTAOYNGlSztFYwY2TVL6R/syImFleQNIWwE+BMyPi+W7vnw+8IiJelHQE8DNgt2qDiIjzqn2PmZnZcKaIjU5qbCitra0xd64P87GeSZoXEa2beH0kMBu4tZKhd0lPA60RsaLKOMYDZwN7Ai2l+xFxaDX1mJmZDRfezN+GLUkCrgQW9JagStoxlUPSAWT/z/y9huauAZ4AdgXOA54GfldDPWZmZsNCQw73m9XJW4ATgEckPZjufR6YBBARlwHHANMldQAvAcdFbcMP20XElZLOiIi7gLskOUk1MzPrhZNUG7Yi4m5AfZT5LvDdOjS3Ln1dIundZIuwtq1DvWZmZkOSk1SzwXGBpK2Bs4DvkB2L6hOnzMzMeuEk1WwASWoBTgVeTbYH65URcUi+UZmZmRWfF06ZDayrgVbgEWAK8K18wzEzM2sM7kk1G1h7RMTrASRdCdyfczxmZmYNwT2pZgOrtGCKiOjIMxAzM7NG4p5Us4G1t6TSKVYCNk/XAiIitsovNDMzs+Jykmo2gCKiOe8YzMzMGpGH+83MzMyscJykmpmZmVnhOEk1MzMzs8Jp+DmpK15cw1V3P5V3GABokwdsDj/+dpiZmVmtGj5JXbKqna/MfjzvMMysziS9GBFb5B2HmZnlo+GT1D0mbMWvv3R43mEQRN4hFEoU5Nux7TfyjsDMzMxq0fBJanOT2Hr0yLzDMKuapMnAHOBu4M3AIuDodO9fI2KupHHA3IiYLOmfgfcAY4DdgIuAUcAJwBrgiIh4tpe2Xg1cBowHOoH3A38Gvkl2XGsAF0TE9ZIOBr4MrAD2AuYBHwbeCXwsIt6f6jw4xTm1lzZfBC4BpgIvAUdHxNL0ua8CxgHLgZMi4q+SdgX+G9gC+Hm3uj4DfADYDLgpIs6VNAa4AdgZaAbOj4jre/l2m5lZg/HCKbN87QZ8LyL2BJ4D3tdH+b2A9wJvBL4KrI6IfYH/Az6yifddk9rZmywhXpLq2QfYG3g7cKGkCan8vsCZwB7AK4G3AHcAb0rJIcCxwHWbaHMMcG9q8zfAyen+d4CrI+INKa5L0/1LgBnpGNklpUokHU72fTogxbu/pH8A3gUsjoi9I2Iv4BebiMXMzBqMk1SzfD0VEQ+m5/OAyX2U/1VEvBARy4FVwKx0/5He3itpS2BiRNwEEBHtEbEaeCtwbUR0RsRS4C6y5Bfg/ohYGBFdwIPA5HSs6y+AIyWNAN5Ntx7PbtYCs3v4bAeR9ZgC/GeKA7JE+Nqy+yWHp8cDwHzgtWRJ6yPAOyR9Q9LbImLVJmIxM7MG0/DD/WYNbk3Z805gc6CD9X9AtmyifFfZdRf1/f+5e1yluq8DPg48SzYN4YVN1LEu4uXZyeV1bEpPs5kF/L+IuHyjF6T9gCOACyTdGRFfqaANMzNrAO5JNSuep4H90/Nj+ltZSiQXSnoPgKTNJI0GfgscK6lZ0njgH4D7+6juLmA/sqH7TQ31b8r/Asel5x9KcQDc0+1+ya3ARyVtkeKfKGl7STuRTXf4L+DCFJeZmQ0RTlJtWJP0Lkm/l/SkpM/18Ppmkq5Pr9+XFv0MtIuA6ZIeIFtcVA8nAJ+U9DBZkrgjcBPwMPAQ8Evg7Ih4ZlOVREQn2RD+FNYP5VfrE8BJKZYTgDPS/TOA0yU9Akwsa/M2sukB/5de+wmwJfB64H5JDwLnAhfUGI+ZmRWQoih7BdWotbU15s6dm3cYVlCS5kVEay+vNQN/AN4BLAR+BxwfEY+XlTkNeENEnCrpOOCfIuLYQQjdzMxsWHNPqg1nBwBPRsSfI2It2fD10d3KHA1cnZ7/BDhM8tliZmZmA80Lp2w4mwj8rex6IfCm3spERIekVcB2ZHuIFo6k75Gtki93SUT8YADbvI9s/9JyJ0TEIwPVppmZDX0N2ZMqaZqkuZLmLl++PO9wrNjGlX5W0mNaJW9Ki4keACal611TMjYJuELSqP4EJelTkh6T9KikayW1lNpI81+vr6YNSVdJWgb8Y0TsExH7ALeT7Q7wKUk3SRpbVv6c1M7vJb2z0volPdrt/ieArYGRwG1lbU+tsv5dJP1K0uPp+3JGur+tpNsl/TF93Sbdl6RLUxsPp1X+NbVR9vpZkiIdoFBTG2ZmVj8NmaRGxMyIaI2I1vHjx+cdjhXbitLPSnrMLHttEbBL2fXO6R5ki3gWAO2pzDfINptfBSwFPlZrQJImAp8EWtMm9M1kq9q/AXw7Il4NrKyyjR+SbW5f7nZgr7Rp/h+Ac1L7e6T29kzv+X6an1tV/ZIOIZsOsXc6jOCiftTfAZwVEXsAB5ItoNoD+BxwZ0TsBtyZriFbuLVbekwDZvRR/6baQNIuZHux/rWsfC1tmJlZnTRkkmpWJ78Ddks9mKPIEqubJe1MtlH9f5AlpCcCh6b3/JJsjup7+tn2CGDztCn+aLITlg4lm/dKtW1ExG/I9i4tv3db2oAf4F6yJByyxPK6iFgTEU8BT5LNz62qfmA68PWIWJPKLOtH/UsiYn56/gLZHwgT2XBOcPn35GjgR5G5Fxir9adlVdsGwLeBs9lwn9aq2zAzs/pxkmrDVkrgPk62D+cC4IaIeAy4jWx7pS6y+agTgK3Ijgn9HNnc1Yk91Vlhu4vIeh3/SpacriI7kem5sqSyX2304KPAnPS8p7m4tbT1GuBtaYrCXZJKp1X1q/60zde+wH3ADhFROiL1GWCHerch6WhgUUQ81K1Yvb5PZmZWg4ZfODVv3rwVkv6SdxxWWK/Y1IsRcQtwS+la0lTg1xHxbUkHkyWqHyM7g/6AVGaXnuqqVJpXeTSwK/Ac8GM2HqqvG0lfIBvqvqbOVY8AtiUbOn8jcIOkV/anQmUb9v8UODMini/fSCEiQlK/98wrb4Ps+/J5sqF+MzMrkIZPUiPCk1Ktnt4CHCXpCLJFR1uRzUUdK2lE6uksn7tai7cDT0XEcgBJN6Z269kGqe5/BqYCh5UdUbqpubjVWAjcmOq9X1IX2eEDNdUvaSRZ8nhNRNyYbi+VNCEilqSh9tKUgrq0Ien1ZH8sPJQS4p2B+ZIOqLUNMzOrDw/3m5WJiHMiYueImEw2R/WXEfEh4FesP6L0RODn/Wjmr8CBkkanPVcPAx6vcxtIehfZPMujImJ12Us3A8cpO01rV7KFQX0dh9qTnwGHpLZeA4wi25qr6vrT9+FKYEFEXNwt1hPT8/Lvyc3AR9IK/AOBVWXTAipuIyIeiYjtI2Jy+m++ENgvnbxVdRtmZlY/Dd+TajZIPgtcJ+kC4AGyZKcmEXGfpJ8A88mGmx8AZgL/U2sbkq4FDibbcmsh2TGh55DtX3p76iW8NyJOjYjHJN1Alhh3AKen406rrf8q4Kq0LdVa4MTUq1p1/WQ9yScAjyg75hSyYfivk00j+BjwF+AD6bVbgCPIFmWtBk7qo/5e20hTPnpSSxtmZlYnDX8sqpmZmZkNPR7uNzMzM7PCcZJqZmZmZoXjJNXMzMzMCsdJqpmZmZkVjpNUMzMzMyscJ6lmFZA0rZHrH4w2XL+ZmdWTk1Szygx0AjMYCVKjf4ZGr9/MzKrgJNXMzMzMCqfhN/OXFAB77TWC5iblHY4VRGdX8OijHQBERFU/GOPGjYvJkydvcG/58uWMHz++tmAWL4addtpkkX7VX6GBbmO41D9v3rwVETGw/7HMzGxoHIt69JEtXHHZtnmHYQUz6VWLaW+v/n2TJ09m7ty59QtEyhJVGxIk/SXvGMzMhoMhkaS+FGJxh2cu2HorlnXUlKCamZlZMQyJJPW22S/xuW80MXqME1XLbDF+BJu1iDXtjT2dxczMbLgaEknq6w6fyB0d+8OqvCOxoli7uoM17U/nHUamwed9m5mZ5aHhk9QtttyJ7fUJ7jk770isSNa2Pw/MyjuMzMyZMM27GzWKl9Z20tHVxZYtI/MOxcxsWBvQJFXSLsCPgB2AAGZGxCVlr58FXASMj4gVkgRcAhwBrAb+OSLmb7KN1Wtouf/JgfoI1qBGxLq8Q1jvlFOcpDaQr8x+nD8vf5HrTzko71DMzIa1ge5J7QDOioj5krYE5km6PSIeTwns4cBfy8pPAXZLjzcBM9LXXr3Q+XceXHkrr2/yPyi2noAd2IWl/C3vUKzBLH9hDYueeynvMMzMhr0BTVIjYgmwJD1/QdICYCLwOPBt4Gzg52VvORr4UWSbt94raaykCameXi3lb4z99Gk0jdpsQD6HNaZtOZClX/v0A3nHYY2ls6uL1Ws78w7DzGzYG7Q5qZImA/sC90k6GlgUEQ9lI/wvmwgbdH0tTPc2maQCvO3oBxg52nPIbEMLvkZX3jFw8815R2BV6OgK2tZ05B2GmdmwNyhJqqQtgJ8CZ5JNAfg82VB/rfVNo+yc7TcfMZYpE/7Q3zBtiGlpWseMvIMA2H//vCOwKnR2BWs6uujo7GJEs7e1MzPLy4AnqZJGkiWo10TEjZJeD+wKlHpRdwbmSzoAWATsUvb2ndO9DUTETGAmwG6vGxUXf38s8NyAfg6zmk2c6G2oGkhHV/bfqm1tJ1tv7iTVzCwvA726X8CVwIKIuBggIh4Bti8r8zTQmlb33wx8XNJ1ZAumVvU1H/XJJ9bxpY8v5Zvf226gPoaZDSOdKUldvbaDrTf3FCIzs7wMdE/qW4ATgEckPZjufT4ibuml/C1k2089SbYF1UmVNHLb7Jf4xkXrGOMTp8ysn17uSfW8VDOzXA306v67yXUujeQAACAASURBVHYD2lSZyWXPAzi92namHtnCdluIbCtWsw3kf2DFySfnHYFVobMrW2vXtsYr/M3M8pT/P+D9tNdeI7jqsm3zDsMKaJ83PgOwd95xMHNm3hFYFTo6S3NS3ZNqZpanhk9SI6A98t9lyIpl2bIOFi8uyM/F/vvDvHl5R2EVenlOqntSzcxy1fCTOB97rIPTpntlv21o++1HMGGngvx4z9/kyb5WMJ1d7kk1MyuChu9JBZg9q53zLwwvnLIN7LPfKJYsbs87DGsw6xdOuSfVzCxPQyKrmzK1xQmqbaCtrYs5swuSoE6YkHcEVoXyLajMzCw/DZ/ZvW7PEVwyY5u8w7CCGTOmiSlTW/IOI7N4cd4RWBU60ur+F2vcgkrSVZKWSXq07N62km6X9Mf0tcdfWpJOTGX+KOnEmgIwMxsiGj5JbW7a5A5XNoylP14eyjsOvvzlvCOwKqzvSa15uP+HwLu63fsccGdE7Abcma43IGlb4Fyyg0wOAM7tLZlN5XeQdKWkOel6D0kfqzVoM7Oiafgk1aw3Z0xfCUXYguq88/KOwKrQ2c/N/CPiN8Cz3W4fDVydnl8NvKeHt74TuD0ino2IlcDtbJzslvshcCuwU7r+A3BmTUGbmRWQk1Qbkgo1J9UaSkf/e1J7skPZEc/PADv0UGYi8Ley64XpXm/GRcQNQBdARHQAXu1lZkOGk9QhZvkyL/aA6uekSpomaa6kucuXLx/AyKzoSj2pm5iTOq70s5Ie06qpP52sV4/j8dokbVeqS9KBwKo61GtmVggNvwWVFLTIx6ECvOmAZSxZ3MWEnZq47/7t8w4nd81VTFeOiJnATIDW1tb6/kDNnVvX6mxgdfS9un9FRLRWWe1SSRMiYomkCcCyHsosAg4uu94Z+PUm6vw0cDPwKkn3AOOBY6qMy8yssBo+SR1BE9s2jco7jNwtXdbBknTC0pLFXaxb0cQO2zf8f96avdjWxexZHu636nUOzD6pNwMnAl9PX3/eQ5lbga+VLZY6HDintwojYr6kfwR2BwT8PiLW1TNoM7M8VZzFSHo/8IuIeEHSF4H9gAsiItfjdLoIVoeHuLccDy0t0N6efd1yPMP6+9I0ev33I3etrdn5vVZ4EdHvhVOSriXrER0naSHZiv2vAzek1fd/AT6QyrYCp0bEv0TEs5LOB36XqvpKRHRfgFXezunANRHxWLreRtLxEfH9mgI3MyuYarra/i0ifizprcDbgQuBGWTbpeSmi6A9CnJGe47a2rpeTsja2+HvL3YM6wMOli3rKEaCag2llKBC7QunIuL4Xl46rIeyc4F/Kbu+CriqwqZOjojvlb13paSTASepZjYkVJOkln5jvxuYGRH/I+mCAYipKhGiPbxXavPoZqZMbWHO7HamTG2heXQz7cO4826r8SPZcUITzyzxHzBWuY6yJLWt+CdONUtSWoiFpGbAc5/MbMiopqttkaTLgWOBWyRt1tf7Je0i6VeSHpf0mKQz0v0eT19R5lJJT0p6WNJ+tX4ws9/cvwMUYTP/c8/NOwKrUKkndUSTWF3fOakD4RfA9ZIOk3QYcG26Z2Y2JFSTpH6AbGL/OyPiOWBb4DN9vKcDOCsi9gAOBE6XtAe9n74yBdgtPaaRTSewCpTvCzpndjttbe5BTPLvDvOJUw2j1JO61eYjWdvZxdqOQv9/9FngV8D09LgTODvXiMzM6qjiJDUiVpNtm/LWdKsD+GMf71lSWlgVES8AC8g2p+7t9JWjgR9F5l5gbNquxfpQvi/olKktw3o+auHstFPfZawQSj2pW7VkM6E2sQ1V7iKiKyJmRMQx6XF5RBS++9fMrFLVrO4/F2gl2+7kB8BI4L+At1T4/snAvsB99H76Sm8nrizB+nTJjG342kVdTlA3lP83Y4l/fBtFR1fWc7rV5iMBaFvbydjReUbUO0lvAb4MvILsd7nIzgp4ZZ5xmZnVSzULp/6JLMks9YwulrRlJW+UtAXwU+DMiHheWr/QKSJCqm43/nTCyzSAnSY2V/PWIc8J6npnTF8J2c+sWUXW96SmJLXGbagGyZXAp4B5+DhUMxuCqklS15YnlJLGVPImSSPJEtRrIuLGdLu301cWAbuUvX3ndG8D5acDvf4No4bxGnbrTfkc3dzt5/V/jaKjM/t1Mmaz7I/f9nWFzv1WRcScvIMwMxso1XS73ZBW949Ne/HdAVyxqTco6zK9ElgQEReXvVQ6fQU2PH3lZuAjaZX/gWS/hD1WalUbM6aJlpa8o0jmzcs7AqtQqSd1sxHNG1wX1K8kXSjpIEn7lR55B2VmVi8V96RGxEWS3gE8TzYv9UsRcXsfb3sLcALwiKQH073P08vpK8AtwBHAk8Bq4KS+4pKClupmC9gwUKjN/KdNg5kz847CKtDxcpKa/f1e8CS1dJBKa9m9AA7NIRYzs7qr6nD3lJT2lZiWl7+bbDJ/T3o6fSWA06uLyZv528a2Gj+SpiboKsIOQldc4SS1QbzckzoyS1I7CpykRsQhecdgZjaQ+kxSJb1A9td5jyJiq7pGVKUuYHWXF0/ZhlYs6yhGgmoNpZGG+yXtAHwN2CkipqQ9qA+KiCtzDs3MrC76TFIjYksASeeTbQX1n2S9ox8Cct/DtAvRHlV1CNswsMX4EWw/oZllSwq98MUKppSUjhpR/J5U4Idk2wF+IV3/AbiebB2AmVnDqya7Oyoi9i67niHpIeBLdY6pKk0ELSr0NjGWk9vuncA+r1iY/7GoizbaoMIKqrRP6vo5qYXujh8XETdIOgcgIjok+a8yMxsyqlnd3ybpQ5KaJTVJ+hDQNlCBmdVJ/n/BeHV/w9h4uD/PaPrUJmk70nSs0o4o+YZkZlY/1fSkfhC4JD0A7k73cuWt6603o5s6oQg/IkcdBVHoYWNLNl7dX+gs9dNk2/a9StI9wHjgmHxDMjOrn2q2oHoaOHrgQqldSkbMNuATp6xaDba6f76kfyTbElDA7yNiXc5hmZnVTcVJqqSdge+Q7X0K8FvgjIhYOBCBmfVHoU6csobR0QCr+yW9t5eXXiOJspP9zMwaWjXD/T8A/ht4f7r+cLr3jnoHZdZfY8Y0MWVqSzES1csvzzsCq1Bnt4VTpWNSC+bI9HV74M3AL9P1IcD/Ak5SzWxIqGa+3viI+EFEdKTHD8nmQJnZpkyblncEVqFSUtqfE6ck7S7pwbLH85LO7FbmYEmryspUvEtKRJwUEScBI4E9IuJ9EfE+YM90z8xsSKimJ/Xvkj4MXJuujwf+Xv+QzPqvUMP9khdONYj1c1Kz4f5a5qRGxO+BfQAkNQOLgJt6KPrbiJhaY6gAu0TEkrLrpcCkftRnZlYo1SSpHyWbk/ptsi1P/hc4aSCCqkZ0BS1yAmAbatlCTD2yhdmzCpKoWkMoJaWjmuu2uv8w4E8R8Zf+VtSDOyXdyvqOg2OBOwagHTOzXFSzuv8vwFEDGEtNHnusg1NOfY5LZmyTdyjWTd5/PNxxuxNUq84ArO4/jvVJZHcHpQNRFgP/GhGPVVNxRHw8LaJ6W7o1MyJ66rE1M2tI1azuHw+cDEwuf19EfLT+YVVnzux2/u2bYvSY/LfEtPVW59j2355eS3sVOaqkacA0gEmT6jxiOrU/I7o2mDbeJ7XHJHWcpLll1zMjYmb3QpJGkf1hf04PdcwHXhERL0o6AvgZsFu18aaV/F4oZWZDUjXD/T8n23bqDqBwG5M+17U5a7ucpFpm7ebVrR9JScZMgNbW1vp2Ac+aVdfqbOCsX92/yS2oVkREawXVTQHmR8TS7i9ExPNlz2+R9H1J4yJiRaWxpl7Ub5Ct8ld6RERsVWkdZmZFVk2SOjoiPjtgkfTD2969FTG6hTZPTbWkZdwoxk0YwYol+Z+KypFHOlFtEN17Uvs53H88vQz1S9oRWBoRIekAsp1Wql2I+k3gyIhY0J8gzcyKqpqux9lpWKpQXrlHC5/7zi55h2EFdPU9uwM8lHcczJ6ddwRWoe5zUmvdzF/SGLI9pG8su3eqpFPT5THAo2lO6qXAcRFVbwGx1AmqmQ1l1fSkngF8XtIaYB0VDC1JugqYCiyLiL3K7n8COJ1s2sD/RMTZ6f45wMfS/U9GxK19BdXS3MnE5lVVfAwbZgp9+LoVS2mf1NLq/lp7UiOiDdiu273Lyp5/F/hujWGWzJV0Pdl81jVldXuOqpkNCdWs7t9yU69L2rOH1ak/JPtF/KOycocARwN7R8QaSdun+3uQrYTdE9gJuEPSayJik/Nfm4DRTYWbImsFcMb0lQD75h2HNY6u1Jk5ormJEU2qxxZUA2krsvWJh5fdC7yQysyGiGp6Uvvyn8B+5Tci4jeSJncrNx34ekSsSWWWpftHA9el+09JehI4APi/OsZow0ShNvP3Rv4No9Rz2izR3KR6bEE1YNKpU2ZmQ1Y9l8OrwnKvAd4m6T5Jd0l6Y7o/EfhbWbmF6Z5Voa2t0D0/g2bMmCamTG3JO4zMzI12J7KCKs1BbW5S1pPaWdwkVdJrJN0p6dF0/QZJX8w7LjOzeqlnklrpb/MRwLbAgcBngBskVZrgAtmelpLmSpr77LNOykrOmL6SfV+7tDTMPeylAx4eyDsOTjkl7wisQqU5qSOaRFPBe1KBK8j2YF0HEBEPk02ZMjMbEvLYWHQhcGNk7idb2DKO7Hzr8mX6O6d7G4mImRHRGhGt227rvVFhw+HtObPb3aO6nr8RVrHOri4kaCr1pBY7SR2dfoeWK8Cea2Zm9VHPDG9theV+BhwC2XAVMApYAdwMHCdpM0m7kp2+0v0X8EYK/o/IoCkf3p4ytYUxPn3LrGodXcGIpmxgp7mpic5izydeIelVpFEsSccAS/INycysfirOZCTNkvTBtP/fRiLiwB7ecy3ZwqfdJS2U9DHgKuCVaR7VdcCJqVf1MeAG4HHgF8Dpfa3sB1jwWIeHt5NLZmzDA0/sUBrmtqK4+ea8I7AKdXYFzSlJLfqcVLJt/C4HXitpEXAmcOqm32Jm1jiqWd1/EXAs8P8k/Y4swZwdEb0uoY6I43t56cO9lP8q8NUqYgKy4e2Lv9Xp3kOgZQtR+fRgGxT77593BFahrCc1+z3SAKv7/wy8PXUcNEXEC3nHZGZWT9Xsk3oXcJekZuBQ4GSyXtHcz4meMrWF5tHNtBf33xMbziZO9DZUDWKDntTmYu+TKmk74FzgrUBIuhv4SkRUe7yqmVkhVbVPqqTNgSPJelT3A64eiKCqseeeI7j8srG49zDT1tblHmWzGnV0dZXNSS12TyrZaNZvgPel6w8B1wNvzy0iM7M6qmZO6g3AArJe1O8Cr4qITwxUYFa906Y/x+t2X8Zp05/LOxSzhrTRnNRiJ6kTIuL8iHgqPS4Adsg7KDOzeqmmJ/VK4PhKFjMNpsce6+AT05/jisu2zTuUXL3Y1sXsWdn04Nmz2rn0W7CFe1SL4eST847AKtTRueHq/oL3pN4m6TiyBacAxwC35hiPmVldVTMn9VZJb07HnI4ou/+jAYirKj+f1c7XL+oY1sPcI0bD1CNbmD2rnalHtjBiNLRHcefTDSs+caphdHYFzc2lJLXwW9ydTLai/7/I5js1A22STgEiInJfL2Bm1h8VJ6mS/hN4FfAgUOpNDSD3JNULpzIXf38bzr8wm5M63L8XhbL//jBvXt5RWAU2XN1f7J7UiNgy7xjMzAZSNcP9rcAeEcVapvy6PUd4X9Ayw7k3ubDmz887AqvQxnNSizsakY6T/hCwa0ScL2kXsnmqfR6CYmbWCKrJaB4FdhyoQGpV+gfFzKy/uq/uL/hw//eBg4APpusXge/lF46ZWX1V05M6Dnhc0v3AmtLNiDiq7lGZDSUTJuQdgVWoe0/qus7i9qQCb4qI/SQ9ABARKyWNyjsoM7N6qSZJ/fJABWH1431SC2jx4rwjsAp1dsUGPakvrSt0T+q6dLhKAEgaDxQ6qzYzq0bF2Uw6ceoJYMv0WJDuWUGcMX0l+752KWdMX5l3KFbuy1/OOwKrUEed9kmV9LSkRyQ9KGluD69L0qWSnpT0sKT9amjmUuAmYHtJXwXuBr5WU8BmZgVUzer+DwAXAr8GBHxH0mci4icDFFuFcQUtKnRvx6Boa+tizuxsn9Q5s9u5+Fud7lEtivPOc6LaIDq7r+7v7NfvlkMiYkUvr00BdkuPNwEz0teKRcQ1kuYBh5H9Tn5PRCzoR7xmZoVSzXD/F4A3RsQyeHlo6Q4g1yQ1QrSHF081j25mytQW5sxu95ZcG2rJOwBrHPXqSa3A0cCP0m4p90oaK2lCRCzp642Syk8uWQZcW/5aRDxb/3DNzAZfNUlqUylBTf5OdbsD2AC7ZMY2fO0iz0ktee0rlgDsmXcc1jg6u4LNRqWe1GbRUfsWVEF2IlQAl0dE9xMdJgJ/K7temO71maQC81L9AiYBK9PzscBfgV1rDdrMrEiqSVJ/IelW1v/Vfiwwp/4hVcfD/Rvq9PcDgMcfX0s1+YWkacA0gEmTJtU3mLkbTUm0girvSW1Wrz2p47rNM53ZQxL61ohYJGl74HZJT0TEb+oRY0TsCiDpCuCmiLglXU8B3lOPNszMiqCahVOfAS4H3pAeMyPi7E29R9JVkpZJerTs3oWSnkiLBW6SNLbstXPSQoLfS3pn9R9neDtt+nO8bvdlnDb9ubxDyd3o0dVNAYmImRHRGhGt48ePH6CorOg6y/ZJHdGk3k6cWlH6WUmPjc69jYhF6esyssVNB3QrsgjYpex653SvGgeWEtTU1hzgzVXWYWZWWNUsnNoVuCUibkzXm0uaHBFPb+JtPwS+y4ZHp94OnBMRHZK+AZwDfFbSHsBxZMOzOwF3SHpNRHSyCZ6Tmmlr62L2rGzh1OxZ7Zx/YQzrYf8tx1UzSDDAWluhWAe1WS86OoMmrd+CqquGOamSxpBNj3ohPT8c+Eq3YjcDH5d0HdmCqVWVzEftZrGkLwL/la4/BHi/MzMbMqrJYn7MhnvwdaZ7vUrDW892u3dbRHSky3vJehAgW0hwXUSsiYingCfZuPfBejFmTBM7Tsj+c+44oWlYJ6iQfT+mTPWaKatOZ1cwojn1pDb32pPalx2AuyU9BNwP/E9E/ELSqZJOTWVuAf5M9nvuCuC0Gto5HhhP1lN7Y3p+fC0Bm5kVUTXdTSMiYm3pIiLW1uF0k48C16fnE8mS1pLSQoJN8pzUTFtbF88syf6GeGZJF52rvQXV5ZeNZdLOzzyQdxzWOLITp0pbUNW2uj8i/gzs3cP9y8qeB3B67ZFCWsV/Rn/qMDMrsmqymOWSXj4CVdLRQG97APZJ0heADuCaGt47TdJcSXOf/bsPWIGs53DCTtl/zgk7uSe1TP4/IOeem3cEVqGOshOnRjQ11dqTamZmdVBNT+qpwDWSvpuuFwIn1NKopH8GpgKHpR4FqGIhQVqoMBPgDXuP9L8iZD2pSxZn+diSxV0+HrVIvJF/w+gsX90/sPukmplZH6pZ3f+niDgQ2APYIyLeHBF/Kr0u6cRK6pH0LuBs4KiIWF320s3AcZI2S4u0diObz2UVGDOmialHZnMwpx7Z4gS1SHbaKe8IrEIdG63uz78j3sxsuKp6CXREvNjLS2cAV5ffkHQtcDDZvoILgXPJVvNvRrZ3IMC9EXFqRDwm6QbgcbJpAKf3tbI/i8er+0su/v42nH9h1oPq06Zeln+2vqTaRduWl0boSZX0HbLN/HsUEZ8cxHDMzAZMPffp2ShTjIieVppe2VsFEfFV4Kt1jGnYcQ/qemdMXwmwb95xWOPYcE5qzav7B5pPhzCzYaGeSWouv829ut960tbWxZzZ7XmHkdlvv7wjsAp1dq5f3d/UJCKgqytoairOaE1EXN13KTOzxjegPalmeWlrK9Bcwnnz8o7AKtRRvk9qSkw7uoJRBUpSSySNBz5Ltk7g5U2BI+LQ3IIyM6ujeo4N31PHusz6ZfvtR9BSlL38p03LOwKrUGeUz0nNfj12Ffe0sGuABcCuwHnA08Dv8gzIzKyeqjkWdSzwEWBy+ftKk/Qj4uP1Ds6sVm1tXbQXZLSfK66AmRsd724F1NltTipQ1HmpANtFxJWSzoiIu4C7JDlJNbMho5qe1FvIEtRHgHllDyuQQg1z56h8Sy6zSkTERqv7IZunWlDr0tclkt4taV9g2zwDMjOrp2rmpLZExKcHLBLrt9OmP8fsWe1MPbKF788Ym3c4ufv+jLHMnuVjUa0ype2mXu5JbS71pBb2D78LJG0NnAV8B9gK+FS+IZmZ1U81Sep/SjoZmA2sKd1M50fnxvukZtraupg9Kxvfnj2rnfMvDG9Hlck/w1jU48FpVjClYf3SXNSXe1ILOtwfEbPT01XAIXnGYmY2EKpJUtcCFwJfYP12UwG8st5BWfXGjGliytQW5sxuZ8pUnzhVJv9vxLx5PnWqAaztzP6eGdnD6v4ikXR2RHyzt039vZm/mQ0V1SSpZwGvjogVAxWM9c8lM7bhaxd1OUFNCrOZ/1FHQXFXiFuyanU2xXOrzUcC63tUC9iTuiB99ab+ZjakVZOkPgmsHqhArD6coGYKtZm/NYRVL2VJ6tiXk9TsftF6UiNiVnq6OiJ+XP6apPfnEJKZ2YCoJqNpAx6UdLmkS0uPgQrMrD9K0x/MKlVKUrfeqCc1/2nNvTinwntmZg2pmp7Un6WHWUO4ZMY2zJm9JP/V/ZdfnncEVoGXk9TRWZI64uWFU7mF1CNJU4AjgIndOgq2AjryicrMrP4qTlJ9XrQ1qPxTDJ841RCeW10a7h8FrF/dX8AtqBaTzUc9ig33qn4Bb0FlZkNINSdOPUXPK0lzXd1fwEUNVizVjBYMDMkLpxpA9+H+ETVuQSVpF+BHwA5kvzNnRsQl3cocDPwceCrdujEivlJJ/RHxkKRHgXe688DMhrJq/gFvLXveAryfApxusuCxDs6YvpJLZmyTdyhWMP9wwFKAvfOOwxrDcy+tZdSIJlpGbrhPag0LpzqAsyJivqQtgXmSbo+Ix7uV+21ETK0l1ojolLSLpFERsbaWOszMiq6a4f6/d7v175LmAV+qpWFJnwL+hayn4RHgJGACcB2wHdkw1gmV/AKeM7udi7/V6ZXtZKva/X2AZcs6eGZJ4YZprcCef2kdW28+Eqm0T2ptW1BFxBJgSXr+gqQFwESge5LaX08B90i6mWxha6n9i+vcjplZLirOZiTtV/ZolXQqNQ6lSpoIfBJojYi9gGbgOOAbwLcj4tXASuBjldQ39UhvXg/Zsaiv230Zp01/Lu9Qcleon4epNXWW2SBblZLUkpd7Ujtrn6ohaTLZXr339fDyQZIekjRH0p41VP8nshMAm4Atyx5mZkNCNUnmt1g/J7UDeJpsyL8/bW8uaR0wmqzn4VDgg+n1q4EvAzP6qqgzGPZHoxbxWNQW5TcP84kn1lVVXtI0YBrApEmT6hvMrFl9l7HcPbd63ct7pAKMaO51Tuo4SeUb6c+MiJndC0naAvgpcGZEPN/t5fnAKyLiRUlHkO2csls18UbEedWUNzNrNNVkMVOAK4E7gXuARWS9n1WLiEXARcBfyZLTVWTD+89FRGkLlYVkQ2R9mjO7nba24T20W74vaFGORW0P5faYvPuoqmKNiJkR0RoRrePHj6/vN+LII+tbnw2IXntSN17dv6L0s5IePSWoI8kS1Gsi4sbur0fE8xHxYnp+CzBS0rhq4pU0XtKFkm6R9MvSo5o6zMyKrJpM5mfAkcA64MX0aNvkO3ohaRvgaGBXYCdgDPCuKt4/TdLcUm9GUZKyvF0yYxseeGIHLyIrmtmz847AKvDc6nUv75EK0KyaV/eL7A/6Bb3ND5W0YyqHpAPIfhd3n/ffl2uAJ8h+j55HNrr1uyrrMDMrrGqG+3eOiIoTyT68HXgq4v+zd+fxUZXn//9f7ywQwr4LAQQFVFYRxLWtS93FpdpWa92rYqvVaj+1tv1WW9vfx1bbWnf5iLWLVm3FCtS1arViXQCR1QUBEQj7Hggkmev3xzkDQ8gkM5NJziS5no/HPDLnzH3OfZ1hSK65z73YGgBJk4CjgE6SCsLW1D4ErbV7CVsuJgAMGVZonpTt5sl6IN6y7EujulRtTtqSmna3laOAC4E5kmaF+34E9AMwsweBc4GrJVUC24HzzNKep6yrmU2UdJ2ZvQ68LsmTVOdcs5FOkvqWpOFmNicL9S4FDpdUTPAL+niCyalfI/jl/QRwMcE8grVSntgWy89CSK65+d/7uvH81GXRrzjlcl5lVYwtOyr3SFLjfVJj6Y/ufxOotZO8md0L3Jt2oHuKd7wulXQawST/kU8L6Jxz2ZJOkno0cEk4qf8Ogl/CZmYj0q3UzN6R9HeCwQOVwPsELaP/BJ6Q9Itw38R0z+1cNdF3VvaJ/HPe5vKgK/weA6cyb0ltLL+Q1BG4EbiHYFlUX3HKOddspJOknpLNis3sFuCWarsXAWPTOU8eUJxXla2wXPMTff+HCRN8adQct3FbMB3zHn1SM5wntaFJKgLGAwMJBpdONLNjo43KOeeyL+U/4Gb2WU2PhgzOufq47uoN5AdzVEbrqquijsDVIb4kaqc2u2eFyOGW1D8SrAA4h6Dx4DfRhuOccw0j+nXNnWsAZWUxnp9azoCoA3FNQjxJ7VDDwKmqvaegitoQMxsOIGki8G7E8TjnXIOI/laocw2gbds8vnVMKx9F4lIST1I7No0+qbtWqkiYV9o555odT1Jd8xEzVBbb9bh9dHoT+jeYyZOjjiASpZu2Rx1CylZv3gFAl7a7PzO7W1JzLkkdKWlz+NgCjIg/l1R9ZSvnnGuymvztfskiXX7T5RKjw0Nb6XB3GcqlsXSjR0cdQaN7e9E6zpvwNn+45FCOPbBH1OHU6Z3F69m3a3GNSWplVW79fjEzn3PPOdcieEuqaz7yxeYb27PmyS5U9Mqhj3ZJSqv7Bj/AngAAIABJREFUNisvz18FwL2vLYw4krpVVsV4Z9E6jty/6x77c7gl1TnnWoQm35JqMShSDiUkLnpHFLHlwrZ0+fWWqCNpsd74eA2tCvKY8dkG3luynkP7527v4HkrNrNlRyVH7t9tj/0F8SmofJ5b55yLRJNPUufOreSy8eu5/4FOUYeSE8rKYr40KtDjlXJ25MGanBuY3fyt2LidT1Zv5cYTBjNx2mL++NaSlJPUVZvLWbNlB8NKOjZwlLtN+3QtAIfvV3NL6qbtFSxdt41+XYsbLSbnnHPN5Hb/1CnllJV5NvLtqzdy0AGr+fbVG6MOJVL5pVWozLjhC61YGnUwAFdcEXUEjeqNj9cAcPKwfThteC9eWbCa7Ttr7yRsZjw/p5QTf/cGX3ngLTZtq6i1fLZUVMV4fs5KDujZnu7tW+/xWnx0/wP//pQz73sz7eVRnXPO1U+Tb0kFOH1cUYtvPSwrizF1SjkQJO133NmCW1SrYPGTnbl/5Jp6nWb6kvXsrIrtdRs4bRMmsH1nFe8tWc+SdWUsXlvGpu0V5Et0a9+acSN6M6R3hxoPXby2jKXrt9G5uJBhvTuSl1frkvC12rqjkrIdlfTsUJS0zNqtO3jr03Uc0LM9B+zTPqXzlldUUbajkg5tClm1uZx7Xl1I3y5tGNijHacN78Vj7yzl3x+t5pThvQDYUl7B8o3bGdyjPTurYrwwdyUT31zMnOWb6N+1mE3rKnhhXilfP7TfrjpiMWP28k1s21nJqL6dadMq+dihzeUVvLVwHcs2bCM/TxzSrzPDS/Z+78yMmyfNYc7yTfzu6yP3Ok9i+Q3bKli2Ybu3piaQ1B+YambDIqr/EmCMmV0TRf3OuYbX5JPUoUML/FY/wbygp48rYuqU8haftFf1yacYdr0f6dpZGePz9du45A/vUdwqn7dvPr5eyeHGISM48YLfsnpLMM1Rcat8Ohe3oipmrCvbwYOvf8oZI3tz0RH7Mnrf4Lb45vIKfvLMXKbMXkG8S2RJpzaUdG5Dn85tOOGgnpw0dJ8a41q4eitTZ6/gq2P6UtKpDRAkeRc8/A5L15Xx6o3H0LltK8orqli2YTv7d29LZcy448WPePg/i4g3GB5/YA8e+OZoWhXs/iztqKyidUE+a7bs4O8zlvHy/JXMXrZp11yiBXmiuFU+j33rcCQxdkAXurZtxdQ5pZw8bB9umTyPx99ZSmXM6NauFZvLK9lZGWO/bm359TkjOGtUCSfd9QbPzlrB1w/tx6rN5Ux4YxHPzSmldFPwb9m6II+vH9qXK76wH3277Jk0LlqzlTPvm8aW8j2nDz1y/6787usH75GgT/5gBX+fsYzvHj+Is0f1qfPfcX7pZk9SWxBJBT4PrXPRavJJqtvt/gc6tewW1Gruf6ATU6es/CDd4z5atYWz7pvG1h2VbN1Ryezlmzi4b/BFaMnaMv4+YxlXfGG/PdZ5NzOWbdjO+rKdjOjTEUmYGfe+upBrF8yhV6c2/OqcEQzt3YHu7Vsjhf0dt1Vw/78X8vg7S3l21gpuOvlALji8H5f+4T0++Hwj47+0P8cf2IPP1m3jpfkr2bCtgtc+XM2kmcsZO6AL5x7Sh85tW9GmMJ/3lqznjU/W8P7SoLvH0zOX8dRVR9CrYxsmf7CCDz4P9v/6xQ/536+M4OZJc3jm/eX0aN+aypixvmwn5x3al6+O6ctbC9fym5c/5v/9Yy63nzMcSfxt+ufcPGkOA3u047N129heUcXwko5c8cX96Nm+NZu2V1K2s5KzDi7Z1TJckJ/HKcP34an3lvH9v83m6ZnLOOeQPowd0Jm3Pl1Hj/at+dLgHhy5f9ddCfcZI3tz96uf8PB/FvHQG4vYtK2CLw7uzg9OPoBObVrx3JxSHn9nKX9++zMuOKwfvzhr+K5/g1smzwODx684jKG9O1JeUcULc1dy+/Mfcvo9b/KHSw5lWElHNpdXcNvUBYzs05Hrjh9U52dCgg9XbubkYfuk+3FqcGGL5vPAm8CRwHLgzHDf981suqRuwHQz6x+2QJ4FtAUGAXcCrYALgR3AqWa2Pkldo4FHws2XEvbnA7cDxwCtgfvM7CFJxwC3AmuBYcAM4JtmZpJuB84AKoGXzOz7kroDDwLxZvTrzWxaCu/BOOAn4XWsAy4A1gAfAUea2RpJecDHwBHhYXvVI+lWYH9gP2CppF8AfwjPmwecY2af1BWPcy47mnySOm9eJddevZH/ezB3Rw83pqJ2nqDGXTF+PcDe93Hr0KN9azoVF/I/Jx3Aj/8xl5fmreTgvp14cd5KvvvX99lRGWNzeQU/P3MYG7ft5Nq/vs+szzfuar0b278Ll39hAP+av4q/zVjGtcDT44+gIH/vf5uOxYXcfOpBXPflQfzw6Tn86oUPufOlj4IE9xuHcGp4i3xM/y6cMzpo7auKGU/PWMZt/5zPD56evetcEozo04nvnziYYSUdufbx97n4kXeZcOEYbn/+Q4aXdOTQ/l14ZNpienVswzPvL+fEIT1p0yqfooJ8jjuoBycNDZKw0ft2pqIqxt2vLmRUvyBBv/mZOYzq24nC/DxOGtqTa44bxMAe7ep8P2844QAWlG7h6ZnLOOaA7txx7gjy8rTH7fxE5xzSh//7zyJ+8c8F9OtSzGPfPYzBPXd3PTj2wB7ccOJg7njxI/7y9lLOO7Qfw0o68uK8Vfznk7XcOm7Iri4aHdsUcvGR/Tl8v65c+od3OW/C2/zp8rH85e3PWFe2gz9ccuiuAVK1GdC1LQtKc3qe/EHA+WZ2haSngHPqKD8MGAUUAQuBm8xslKTfARcBdyU57g/ANWb2hqQ7EvZfDmwys0MltQamSYonsaOAocAKYBpwlKQFwNnAgWHCGr8d9nvgd2b2pqR+wIvAQSlc/5vA4eG5vgX8wMxulPQXgoT1LuDLwAdhwvp4LfUMAY42s+2S7gF+b2aPSWoF+By1zjUiWROfXkUKZvJf9PE+tPMWRLaWxfx9IHgf9hu8EgAzS+te/ZgxY2z69OkAnD/hbdZu3cHLN3yJr9w/jY3bKxhe0pGps0t58fovcP+/P2XyrBWcP7Yfg/dpTyxm3PPqQtZuDW7tX3vcQG645Fi0YkWd9VZWxZjwn0Vs21HFcQf14JB+nWstX15RxZotO1hftpMt5ZUMK+lAp+Ldk9FPW7iWCye+Q36eKMzP48krj2D/Hm25aOK7TP9sAx2KCvjPD47bo0U4USxmXPTIu7y7OOib+6XB3XnowtEUFab/d3pHZRXPvr+Ck4buk7S+RDsrY6zaXE6PDq1pXVBzfZu2VXD4/77CGSN786tzR3DWfdPYtL2Cl7/3xRq/EJRu2s55E95m5aZydlTGuO74QXzvhMG1xnHehP9y9MBuLCjdwtwVmzh56D786LQhM8xsTGpX3vDCltSXzWxQuH0TUEiQlCVrST3KzK4Iyy8FjjCz5ZIuA0aY2fU11NMJmG1m/cLtEcDjZjZM0t+BEcC2sHhH4CpgJ/BjMzshPOYBgkT1CYJW1RnAVIK+rTslrSZIZuO6AweY2dYa4rmEsE+qpOHAb4BeBK2ei83sZEl9gWfN7BBJTwB/MbOpyeoBvg+Ymf0srOMbwI+BPwGTvBXVucbV5FtSAc4cV+SJGUHL4bNTyjlzXJG3LGfJCUN68vOp83lvyXpmfb6Ra44bxEVH7MurC1Zz+j1vUl4R49rjBnLjiQfsOuYbh/XjvcXradMqn1H9OkMKCSoEt8a/fczAlGMrKsynb5fivfplxh01sBs/Pm0Id7/yCQ9+czTD+wTTOj162Vh+NGkOxx7YvdaEMS9P/OZrIxl3z5sc0q8zvz//4KQJY11aF+TztUP7ply+VUFe0uuK61hcyFmjSpg0cxnHHtiDWZ9v5NZxQ2pMUAF6dWzDny87jK899F8O7tsppdv8T1wZ3Bm+55VP+OecUh56Y1HK19DIdiQ8rwLaENxGj78Z1UfLJZaPJWzHyOzvgoBrzezFPXYGt/urx1ZgZpWSxgLHA+cC1wDHhfEebmbpdia/B/itmU1O6GKAmX0uaZWk44CxBK2qJKsn7IZTFt82s8clvQOcBjwn6SozezXN2JxzGYo0SQ37MU0HlpvZ6ZIGEHzD7krwDftCM9tZ2zmGDi3gngc6UW4tewqqsrIYz4aDhJ6dUs7td1a26L6pBcWZD5xKdMbBvbn9+Q+54alZxAyOOaA73dq15smrjuDxdz9jw7YKrjluz8SyMD+PIwcmzAhw663BIwKXHz2AS4/sv8cAq3atC7j7/FEpHd+zQxFv3nTcHoOncsnlR/fnmfeXMf4vM2jbKn9Xl4hk+nUt5o0fHEthvnb1C07Fgb2CPrYH7tOez+oVcaNaAowG3iVIBOvFzDZK2ijpaDN7k90JHwS3y6+W9KqZVUgaTNA3tkaS2gHFZvacpGlAPPt/CbgWuCMsd7CZzUohvI4J9V1c7bWHgb8Afzaz+FxoKdUjaT9gkZndHXYLGAF4kupcI4n6L891wIKE7V8R9BMaCGwg6OfkUhAf3Q8+JVc2dWvXmjMO7s3n67fTqbiQkX2CrnNDenfgF2cN575vHFJ36+LPftYIkSZXn5kJgJxNUAEG9mjP1GuP5oj9uvKd4wbSvqjurgStCvLSSlABDtuvC6eN6MW93zgkpfKSTpb0kaSFkn5Yw+utJT0Zvv5OeMs+2+4kSBzfB+o5j9oulwL3SZpF0Hoa9zAwH5gpaS7wELU3grQHpkqaTdCf9IZw/3eBMZJmS5oPjE8xrluBv0maQTBIK9FkoB1Bf9q4VOv5GjA3vN5hBLf9nXONJLI+qZL6AH8EfknwC2ocwWjMfcJbQUcAt5rZSXWcx04fV+TTUIV8xalAWVmMgw5YDdSvTyrA3OWbOP2eNxk3sjf3pNgCuQcJmnjfb7ebpFr7pIZ3iD4GTgCWAe8RDGqan1Dm2wR9P8dLOg8428y+3sCht0iSxhA0fnwh6licc+mJ8nb/XcAPCL5RQ3CLf2PCvHTLgJJUTjR1SjkP/7bA+6UCXVKbf73ZW/F5/W7zJxpW0pHbzhrGYQO8n69LyVhgoZktAggH7JxJ0NIYdyZhv0ng78C9kmRNfSRrjglbsa9mz64JzrkmIpIkVdLpwGozmxF2ck/3+CuBKxP3bbNK8syTVBdYvy27c3BfePi+mR+c0CrrWoQS4POE7WXAYcnKhHeONhF8Ua9+qzoSku4Djqq2+/dm9oeayjdwLJcSdA1LNM3MvlPXsWZ2O8H8rc65JiiqltSjgDMknUow6rQDwfx4nRJW+ehDko73ZjYBmADB7f5TTi+iso3Y6Gtru1CvwXX3TUyU+MWnX7+a5+90LtRNUuI3jwnh76RmI5UEsLGEiXGjJ8fOuehF0vRoZjebWR8z6w+cB7xqZhcAr7F7FOrFwLN1neugoQX8/oHa55N0LU/btnmccnryNeqrM7MJZjbGzMZ07949u8GMyZkpNV12rI1/VsJH9QR1OZA431ZNX7h3lZH0B2AA8O+aKpN0QTjAZ46ktySlvUBFwrkekbQ6HNyUrMwxkmZJmifp9XrU1VfSa5Lmh+eq3hqKAneHA8hmS0ptZFoGdSWUPVRSpaSMZjxI8bo6Spoi6YOwzKWZ1OVcS5dr98dvAm6QtJDg1tfEug5IZbUY1zK9P6PW2cucayjvAYMkDVCwStF5BCPME01m91RJnwH/quV8i4Evmdlw4DbCu0gZehQ4OdmL4YT99wNnmNlQ4Kv1qKsSuNHMhgCHA9+RNKRamVMIVssaRHAn44EGrCs+qO1XJCzp2kB1fQeYb2YjCZaK/U34WXDOpSHyJNXM/m1mp4fPF5nZWDMbaGZfNbMddR3vXE3WrK5kZWnLnjvXRSPsrnQNwdyhC4CnzGyepJ9LOiMsNhHoGn4hP5Ug+Ux2vrfMbEO4+TZBy2ymsb0BrK+lyDcIVlZaGpZfXY+6Ss1sZvh8C8F7UX0w7JnAnyzwNkGXr14NVBcEc6M+DTT0dRnQXsFcZ+0I3vPsdpR3rgVo8itOzZ1TsXZw39LGml+7G403sKG51tVY9eURrBmethkzZqyVlN3PVJrzcrqcVucoOjN7Dniu2r6fJjwvJ6GVMo15Ui8Hnk+xbCYGA4WS/k0w88rvzazec4OG1zcKeKfaSzUNMisBSrNdl6QS4GzgWODQTM+fSl3AvQSt5SsI3sevm7XwFWecy0CTT1LNLMsdCJOTNL2x1uxurnVFUV+6GvMz5VyqJB1LkKQe3YDVFBCsUnU8wdKq/5X0tpl9nOkJw9WlngauN7PN2Qkzo7ruAm4ys1i6izlkUNdJwCyCpV73B16W9J+Gvn7nmpsmn6Q651xzJ2kEwapOp5jZugasahmwzszKgDJJbwAjCRYnSJukQoJE7jEzm1RDkVQGmWWrrjHAE2GC2g04VVKlmf2jAeq6FLg9nPd2oaTFwIEES9Q651IUeZ9U55xzySlYM34ScGF9WjRT9CxwtKQCScUE87suqOOYGoX9MScCC8zst0mKTQYuCkf5Hw5sMrO0b/WnUpeZDTCz/uGsMn8Hvp1hgprKdS0laI1GUk/gAGBRunU519J5S2p6GnMuxOZaVxT1OZezJP2VYAR4N0nLgFuAQgAzexD4KcFsJ/eHrYCVmXaXqasuM1sg6QVgNhADHjazpNNV1eEo4EJgjqRZ4b4fAf0Sru05goFjC4FtBC2QDVVXtqRS123Ao5LmACLoZpATCzU415TIV+FzzjnnnHO5xm/3O+ecc865nONJqnPOOeecyzmepDrnnHPOuZzjSapzzjnnnMs5nqQ655xzzrmc40mqc87lKElXel1Np64o6nOuOfMk1TnncldjJjxeV9Osz7lmy5NU55xzzjmXc5r8ZP6SDGDAvgV07pgfdTiR2bEzxrwPK3ZtDz2wkNatWu53kCVLd7J+Y/DZNjOlc2yXznm2YWPN/y+GDyskv5a3tSoGc+ZWpFw+Vamct3qZeDmF5aoMKiuNjxZUJq2n+nnrcz21HVvTa8qDWJUxd15q8SW73lTel+qGHlTAvFrel2rWmVm3VAsDdCsutv5DhqRzCABr1qyhe/fuaR+XCa8ruvpmzJix1swaL0jnmohmsyzqx2/vG3UIkeswYBHby402RWLWa/2iDidyhb0+zei4/v0KKS6uYvmKKkp653PY6NZMmrKNs8e14ZEHu9Z5/OXj16dVPlWXjV/HM1O213reeBlgV7lyqwJgUywoc+xhqygtDTbaFIlTTmhTa7yp1JtJzNVfi8f5rfEb+OfU8l3l2hSJ7eVW6zkSr7e2OL4yrhiASVO2UdI7n+UrqnYdFy8T31/9Z7yOZ6ZsX5LWmwD0376d6dOnp3uYayEkfRZ1DM7loibfkrpf/0LzBHW3T5fsZP/+raIOI2cU9vp0jZn1SOeYQ0a2tv+8sA+rVlfSs0fwPW5rWYzWxSnWqby0yqdja1mMdm1rb8rcWhYkoPFy1ZPU1jJ2rIWd22G/8LNSV7yp1JvJsYmvxeMsN6Ngex6FyqOsLEaX7nm1nmPHtvC66ni/t5bF6Nxuz3/P6ueNb8fLJpYrVB6ti6FzybIZZjYmnfdgjGTTm/jvWtdwJKX9mXKuJWjyLakt+RZ/TTxB3cvSTA+MJ6gQJHwVFkv52HTLp3PebJTp2aOAQu0uV1e8mSaodR2b7LV2bYMkNR5XKueo6/1OPEey88a3E3/GyxWqHv+mo0dndpxzzrVgLbfTonPONZY1a6KOwDnnmhxPUp1zrqEtzbhB3znnWixPUp1zzjnnXM7xJNU555xzzuUcT1Kdc66hDRwYdQTOOdfkNFqSKqmvpNckzZc0T9J1NZTpKGmKpA/CMpc2VnzOOddgihtgPjLnnGvmGnMKqkrgRjObKak9MEPSy2Y2P6HMd4D5ZjZOUnfgI0mPmdnORozTOeeya/bsqCNwzrkmp9FaUs2s1Mxmhs+3AAuAkurFgPaSBLQD1hMkt84555xzrgWJZDJ/Sf2BUcA71V66F5gMrADaA183a4AZ0Z1zzjnnXE5r9IFTktoBTwPXm9nmai+fBMwCegMHA/dK6lDDOa6UNF3S9LXrqho8Ztf8+WfKNahu3aKOwDnnmpxGTVIlFRIkqI+Z2aQailwKTLLAQmAxcGD1QmY2wczGmNmYbl19WVRXf/6Zcg1q332jjsA555qcxhzdL2AisMDMfpuk2FLg+LB8T+AAYFHjROiccw1kwYKoI3DOuSanMfukHgVcCMyRNCvc9yOgH4CZPQjcBjwqaQ4g4CYzW9uIMTrnXPZt2xZ1BM451+Q0WpJqZm8SJJ61lVkBnNg4ETnnnHPOuVzlK04551xDKyyMOgLnnGtyPEl1zrmGNmJE1BE451yT40mqc841tBUroo7ANWGSHpG0WtLchH1dJL0s6ZPwZ+ckx14clvlE0sWNF7Vz9edJqnPONbTS0qgjcE3bo8DJ1fb9EHjFzAYBr4Tbe5DUBbgFOAwYC9ySLJkNy/eUNFHS8+H2EEmXZ+cSnEufJ6nOOedcDjOzNwiWCU90JvDH8PkfgbNqOPQk4GUzW29mG4CX2TvZTfQo8CLBgjoAHwPXZxi2c/WWUZIqaX9JrcPnx0j6rqRO2Q3NOeecc0n0NLN4E/1KoGcNZUqAzxO2l4X7kulmZk8BMQAzqwR8CT4XmUxbUp8GqiQNBCYAfYHHsxaVc841cYlL7W7cZ5+ow3G5rVv8sxI+rkznYDMzwLIQR5mkrvFzSToc2JSF8zqXkUznSY2ZWaWks4F7zOweSe9nMzDnnGvKzGwCwZd4xgwZko0EwjVfa81sTJrHrJLUy8xKJfUCVtdQZjlwTMJ2H+DftZzzBmAysL+kaUB34Nw043IuazJtSa2QdD5wMTA13OcTATrnXE18WVSXfZMJ/gYT/ny2hjIvAidK6hwOmDox3FcjM5sJfAk4ErgKGGpms7MatXNpyDRJvRQ4AvilmS2WNAD4c/bCcs455xyApL8C/wUOkLQsHHF/O3CCpE+AL4fbSBoj6WEAM1tPsNz4e+Hj5+G+ZPV8B2hnZvPMbC7QTtK3G/LanKtN2rf7JeUDPzazC+L7zGwx8KtsBuacc845MLPzk7x0fA1lpwPfSth+BHgkxaquMLP7Eo7dIOkK4P40wnUua9JuSTWzKmBfSa0aIB7nnGt+evWKOgLnUpEvSfGNsFHK/9a7yGQ6cGoRME3SZKAsvtPMfpuVqJxzrjnp3bvuMs5F7wXgSUkPhdtXhfuci0SmSeqn4SMPaJ+9cJxzrhma7WNPXJNwE0FienW4/TLwcHThuJYuoyTVzH4GIKnYzLZlN6T0xIixNbYjyhBcM2MYFRbbtV2o9McXJh6fq1pijBUWS/nfs3rd9YqloiLzY51rJGYWAx4IH85FLqMkVdIRwESgHdBP0kjgKjNr9FGAlQZrYr4ghsuuctvzM5VqYlOovJxL/oqUHzzJi1+TkpbNBUVhl7hU38d0Es90zutcSyPpKOBWYF+C/EAEawXsF2VcruXK9Hb/XQRrAk8GMLMPJH0xa1E5l+MyaV2NQmKcic/TSdTixzV0crcrmW6OioujjsC5VEwEvgfMwJdDdTkg0yQVM/s8YRAg+AfaOedqdtBBUUfgXCo2mdnzUQfhXFymzUGfSzoSMEmFkr4P1LqkiqS+kl6TNF/SPEnX1VL2UEmVknw5Nudc0/fZZ1FH4FwqXpN0h6QjJB0Sf0QdlGu5Mm1JHQ/8HighWBv4JeA7dRxTCdxoZjMltQdmSHrZzOYnFgrnZftVeE7nnGv61q6NOgLnUnFY+HNMwj4DjosgFucyTlItccWpFA8oBUrD51skLSBIcudXK3ot8DRwaIaxOeeccy5NZnZs1DE4lyjT2/1vS/qbpFNUrWNqKiT1B0YB71TbXwKcjU9/4ZxzzjUqST0lTZT0fLg9RNLlUcflWq5Mk9TBwATgIuATSf+fpMGpHCipHUFL6fVmtrnay3cBN4VztdV2jislTZc0ff16n07G1V/iZ2rtOv9MuSwbMSLqCJxLxaPAi0B8ibSPgesji8a1eBklqRZ42czOB64ALgbelfR6OIdqjSQVEiSoj5nZpBqKjAGekLQEOBe4X9JZNdQ/wczGmNmYLl2axlRALrclfqa6dfXPlMuybZGueeJcqrqZ2VNADMDMKvGZe1yEMp3MvyvwTeBCYBVBP9LJwMHA34ABNRwjgjnYFpjZb2s6r5kNSCj/KDDVzP6RSYzOOZczFi6MOgLnUlEW/n03AEmHA5uiDcm1ZJkOnPov8GfgLDNblrB/uqQHkxxzFEFSO0fSrHDfj4B+AGaW7DjnnHPONbwbCBqc9pc0DehOcFfTuUhkmqQeYGZW0wtm9qsk+98kjfUYzeySzEJzzjnnXLrCKSK/BBxA8Pf6IzOriDgs14JlmqR2k/QDYChQFN9pZj6XmnPOVdevX9QROJeUpK8keWmwJJKMIXGuwWWapD4GPAmcTjCx/8XAmmwF5ZxzzUr37lFH4JowSQcQ/M2N2w/4qZndlVDmGOBZYHG4a5KZ/TzFKsaFP3sARwKvhtvHAm8BnqS6SGSapHY1s4mSrjOz14HXJb2XzcCcc67ZmDEj6ghcE2ZmHxEMTI6vyrgceKaGov8xs9MzOP+l4blfAoaEi+8gqRfBtFTORSLTJDXeR6VU0mnACqBLdkJyzjnnXBLHA5+a2WcNcO6+8QQ1tIpwcLNzUcg0Sf2FpI7AjcA9QAfge1mLyjnnnHM1OQ/4a5LXjpD0AUHD0ffNbF6a535F0osJ5/868K/MwnSu/jJKUs1savh0E0GfFeeccwkkXQlcCTC8TZuIo3E5rpuk6QnbE8xsQvVCkloBZwA313COmcC+ZrZV0qnAP4BB6QRhZteEg6i+kBBHTd0KnGsUaSWpku4hnOS3Jmb23XpH5JxzzUCYZEwAGDNmTNLfm84Ba81sTArlTgFmmtmq6i+mRx+mAAAgAElEQVQkLjNuZs9Jul9SNzNbm04g4Uh+HyjlckK6LanT6y7inHNuD77ilMuO80lyq1/SPsAqMzNJYwmWPV+XzsnDVtRfEYzyV/gwM+tQr6idy1BaSaqZ/TGVcpLuMbNrMwvJOeeamU2+sqSrH0ltgROAqxL2jYddKzaeC1wtqRLYDpyXbNGdWvwaGGdmC7ITtXP1k+nAqboc1UDndc4551ocMysDulbb92DC83uBe+tZzSpPUF0uaagk1TnnnHNNy3RJTxIMutoR3+krTrmoeJLqnHMNbfToqCNwLhUdgG3AiQn7DB9I5SLSUEmqGui8zjnX9KzxVaNd7ouvPOVcrsirz8GSipO89Pv6nNc555qVpUujjsC5OkkaLOkVSXPD7RGSfhJ1XK7lyihJlXSkpPnAh+H2SEn3x183s0ezE55zzjnnGsn/ESwUUAFgZrMJVrhyLhKZtqT+DjiJcA42M/sA+GK2gnLOOedcoys2s3er7auMJBLnqMftfjP7vNquqnrG4pxzzdPAgVFH4Fwq1kran3BlSUnnAqXRhuRaskwHTn0u6UjAJBUC1wE+t5pzztWkOFn3fedyyncIlvI9UNJyYDFwQbQhuZYs05bU8QQf5hJgOXBwuJ2UpL6SXpM0X9I8SdfVUEaS7pa0UNJsSYdkGJ9zzuWO2bOjjsC5OpnZIjP7MtAdONDMjjazz6KOy7VcGbWkmtla0v92VQncaGYzJbUHZkh62czmJ5Q5BRgUPg4DHgh/Ouecc64BSeoK3AIcTXCn9E3g52a2LtrIXEuV6ej+X0vqIKkwnK5ijaRv1naMmZWa2czw+RaC7gEl1YqdCfzJAm8DnST1qu28FZXpLk3cvP33zfKoQ8g1+2TjJKtWVzJ3/t7v7arVe44p2FoWy0Z1KZ8nlXLVy2wti7FqdWVasWbrurKpekzV/y3SOTZN9Zq6z7kc9gSwBjgHODd8/mSkEbkWLdNftiea2WbgdGAJMBD4n1QPltQfGAW8U+2lEiBxQNYy9k5k9/DRgkq+OHZVqlU3a4P7lnLx+RsY3Nf7uQPx96HWz08qhh1ayv4Hr+ALJ6ylW99lu/YfMGY5+x+8ggPGLAfgoqvWss+gZVw2vn6NDpeNX0ffwSvqPE8q5eIxXXTV2j22Dxy1MqU6Eo+p73VlU/VrP2DMcg4ctZJhh9b92U/1/U12LMHvrvR065b2Ic5FoJeZ3WZmi8PHL4CeUQflWq5Mk9R4N4HTgL+Z2aZUD5TUDngauD5MdNMm6UpJ0yVNB1hZGmNNGq0ozVH1FtSW3qJ61x0b0yqf+Jlau253K9vq1ZUsX7F74oqqGMydX86qhP3LV1SxaMlOJk3ZBsAzU7Zn3FK3tSzGM1O213meVMptLYvtimnSlG2sWl25azuurlgTz1Gf68qm6te+aMnOPf4tamtRTfX9revYtO27b2bHOde4XpJ0nqS88PE14MWog3ItV6ZJ6lRJHwKjgVckdQfqzIrCmQCeBh4zs5rWAl4O9E3Y7hPu24OZTTCzMWY2BmCfXnl079FQK7w2DUccXVTrdktz/f90Sqt84meqW9fd/y169CigpHf+ru38PBg2pIieCftLeuezX/9WfGVcMIL77HFtaNc2s/9a7drmcfa4NnWeJ5Vy7drm7YrpK+OK6dmjYNd2XF2xJp6jPteVTdWvfb/+rfb4t+hZy++CVN/fuo5N2wKf/MQ1CVcAjwM7gR0Et/+vkrRFUkaNSs7Vh8wy69MpqQuwycyqwuVRO5jZylrKC/gjsN7Mrk9S5jTgGuBUggFTd5vZ2NriOHBIoU15qXtG19Ac/ffN8hafoCYa3Ld0uZn1SeeYUSNb2fPP7b49W6R81q+JsWLNToYNCd7bQgXJzarVlXskRVvLYrTOwmxDW8tiKSVQdZUrVN5eZbaWxdi4tZK2bfNSqiN+jmxcVzYlXleh8li2ametCWqyY9PVuWTZ+2aW1swjYySbnuHvWtf8SZoRb3Rxzu1Wn+bHA4H+khLP8adayh8FXAjMkTQr3PcjoB+AmT0IPEeQoC4EtgGX1hVEYYHSj7wZ8wR1L0m/OKWjZ48CunTfO6mpnhS1a5tHhdX/lniqCVQq5aqXadc2j9bF6f3Xz9Z1ZVP160o1Qa3p2DTl1hvhXJaEjUkXAAPM7DZJfQn6qVZfhcq5RpFRkirpz8D+wCx2rzRl1JKkmtmbQK0ZpQXNurXOt+qcc01OYWHUETiXivsJvoQdB9wGbAXuAw6NMijXcmXakjoGGGKZ9hVwzrmWZMSIqCNwLhWHmdkhkt4HMLMNklpFHZRruTK95zWXLM0/6Zxzzd6KFVFH4Jo4SUskzZE0Kz6zTbXXs7FiY4WkfII7o4SDor17i4tMpi2p3YD5kt4lGAEIgJmdkZWonHOuOSn1uYtdVhwbrvhYk2ys2Hg38AzQQ9IvCSb0/0mGsTpXb5kmqbdmMwjnnHPO1cuuFRuBtyV1ktTLzFL+hmRmj0maARxPMIbkLDPz+dNcZDJKUs3sdUn7AoPM7F/hFFT5dR3nnHPOuYwYwWT7BjxkZhOqvZ5sxcY6k9RwSsm41cBfE18zs/UZR+1cPWQ6uv8K4EqgC8Eo/xLgQYJvX8451+JJupLg9yT77+Nd+F2tulXrZzqhhiT0aDNbLqkH8LKkD83sjSzVP4MgCRbBtJAbwuedgKXAgCzV41xaMh049R2CeU83A5jZJ0CPbAXlnHNNXeIqZp06d446HJfb1sY/K+GjeoKKmS0Pf64m6DdafaGblFZsrImZDTCz/YB/AePMrJuZdQVOB15K/3Kcy45Mk9QdZrYzvhFO6O/TUTnnXE18WVRXD5LaSmoffw6cSDDLTqLJwEXhKP/DCVaETHfE3uFm9lx8w8yeB46sR+jO1UumA6del/QjoI2kE4BvA1OyF5ZzzjnnQj2BZ4IFoSgAHjezFySNh8xXbKzBCkk/Af4Sbl8A+PxpLjKZJqk/BC4H5gBXEfzneDhbQTnnnHMuYGaLgJE17H8w4Xk2Vmw8H7iFoDuBAW+E+5yLRKaj+2PA/4UP55xztenVK+oInKtTOIr/uqjjcC4urSRV0hxq6XtqZr72n3POVde7d9QROOdck5NuS+rp4c/4LYU/hz+/iQ+ccs65ms2eHXUEzjnX5KSVpJrZZwCSTjCzUQkv3SRpJkFfVeecc4kqKqKOwDnnmpxMB05J0lFmNi3cOJLMp7NyzjnnXEQk3UPtXfm+24jhOLdLpknq5cAjkjqG2xuBy7ITknPONTPFxVFH4FxtptddxLnGl+no/hnAyHiSamabEl+XdLGZ/TEL8TnnXNN30EFRR+BcUv732uWqTFtSgb2T0wTXAf6hd845gM8+izoC5+okqTtwEzAEKIrvN7PjIgvKtWgN1Y9UDXRe55xretaujToC51LxGLAAGAD8DFgCvBdlQK5la6gkda8O2JIekbRaUvX1hhPLHCNplqR5kl5voNicc845t7euZjYRqDCz183sMsBbUV1kGrMl9VHg5KQHSJ2A+4EzzGwo8NWGCc0555xzNYjPlVYq6TRJo4AuUQbkWrZ69UmtxbTqO8zsDUn9aznmG8AkM1sall/dMKE551wjG+GL8bkm4RfhgOgbgXuADsD3og3JtWQZJalhq+dFQP/Ec8TnUjOzazI47WCgUNK/gfbA783sT5nE55xzOWXbtqgjcK5OZjY1fLoJODbKWJyDzFtSnwPeBuYAsSzGMho4HmgD/FfS22b2cfWCkq4ErgToXZKfpepdS5b4merjnymXbQsXRh2Bc0lJ+oGZ/TrZpP4+mb+LSqZJapGZ3ZDVSGAZsM7MyoAySW8AI4G9klQzmwBMABg+olXSVTKcS1XiZ2rUSP9MOedalAXhT5/U3+WUTJPUP0u6ApgK7IjvNLP19YjlWeBeSQVAK+Aw4Hf1OJ9zzjnn6mBmU8Kn28zsb4mvSfJBzC4ymSapO4E7gB+z+9aAAfslO0DSX4FjgG6SlgG3AIUAZvagmS2Q9AIwm6ALwcNmlnS6KuecazL69Ys6AudScTPwtxT2OdcoMk1SbwQGmlnKM1Sb2fkplLmDIPl1zrnmo3v3qCNwLilJpwCnAiWS7k54qQNQGU1UzmWepC4EcmK4agViTVVR3QWdc64W5VYV/jR2WDDVc8/8LA2imzEjO+dxLZKkvsCfgJ4Edy0nmNnvq5U5hqDb3OJw1yQz+3mKVawg6I96BpD4Yd2CT0HlIpRpkloGzJL0Gnv2SW30EYDLP+vOzd8e39jVuibjpgY5a4XtOalFPMGJK1JuzBBQPc7GOEe5VVFuxhbLpxUxWsvolNdQUzLXT/zfbVMMllZ2ZOrGg/nXE2PpuDjGln55XHjZS1zU8QMgd/5NXYtUCdxoZjMltQdmSHrZzOZXK/cfMzs93ZOb2QfhapAnmdkfsxGwc9mQ6V+Of4SPyGnzNlq94K0ULrsSE5JC1b4wW2ILXHCs9jgunSSvUHkZJZaZHpeJVK5rh4lyKwBV0pqqBokx/r4XKZ8Ki2V8/nIzNltr5u8oYcqC4QyeuICqDZtoN3oo9w86nq+dNiusp+7PgnMNwcxKgdLw+RZJC4ASoHqSWp86qiT1ldTKzHZm67zO1UdGSap/03ItRTaSklSTp0zrih/XmIlqFPWlIisxVTRAF7yOHbN/TtcihSs3jgLeqeHlIyR9QHD7/vtmNi/N0y8GpkmaTHDHFAAz+21m0TpXP5muOLWYmif8TTq63znnWpLEBSL6+eh+V7tukhLnKJ0Qzt28B0ntgKeB681sc7WXZwL7mtlWSacS3O0clGYcn4aPPIKVH52LVKa3+8ckPC8Cvgp0qX84zjnXPCQuEDGmUydfIMLVZq2ZjamtgKRCggT1MTObVP31xKTVzJ6TdL+kbmnOwvOzdIJ2rqFlert/XbVdd0maAfy0/iE551wzs2lT1BG4JkySgInAgmS33iXtA6wyM5M0lqA1tPrf6rrq6Q78ABhK0AAFgJkdl2nsztVHprf7D0nYzCNoWc3N4bvOOZcD/udvH0Raf7f2rfmfEw8gL0+RxuEychRwITBH0qxw34+AfhAsiAOcC1wtqRLYDpxnZum24D8GPAmcDowHLgbW1D985zKTaWL5G3b3Sa0ElhDc8nfOOVeDaQtTvuuaddsrqtiwrYJzDunDwB7tIovDZcbM3gRq/XZhZvcC99azqq5mNlHSdWb2OvC6pPfqeU7nMpZpknoKcA7QP+Ec5wGpThzsnHMtx+jRvHXz8ZFVP23hWi54+B3Wbt3hSaqrTUX4s1TSaQSzBPh4ExeZ+syTupFgNGF59sJxzrlmaE20d0y7t28dhLFlRx0lXQv3C0kdCZY+v4dgWVRfccpFJtMktY+ZnZzVSJxzrrlaujTS6nt4kupqIamIoA/qQIJFAiaa2bHRRuVcMOgpE29JGp7VSJxzzjWIjm0KKcwXqz1JdTX7I8EA6DkE3fl+E204zgUybUk9GrgknNR/B0GHbjOzEVmLzDnnXFZIonu71t6S6pIZYmbDASRNBN6NOB7ngPoNnHLOOZeKgQOjjoDu7VuzZqsnqa5G8QFTmFllMC2rc9HLdDL/z7IdiHPONVvFxVFHQPf2rVm+0ce5uhqNlBRfsUpAm3A7fpe0Q3ShuZbMJ+B3zrmGNnt21BHQvX1rZn3uK1+5vZlZftQxOFeTTAdOOeeca0K6t2vN+rIdVMXSXYTIOeei0WhJqqRHJK2WNDfJ6xdImi1pjqS3JI1srNicc665696+NTGDdWXeL9U51zQ0Zkvqo0Btc6suBr4UjjC8DZjQGEE551yD69Yt6gh8Qn/nXJPTaH1SzewNSf1ref2thM23gT4NHZNzzjWKffeNOgJPUp1zTU6uDpy6HHg+6iCccy4rFiyIOgJ6tC8CYPnG7WzfWRVxNLmhqDAPn24pd4QNWVPNbFjEoWRE0q3AVjO7M+pYmoucS1IlHUuQpB5dS5krgSsBioh+ahfX9CV+pvqU+EBXl2XbtkUdAd3bt0aCHz8zlx8/U+PQgBZnZJ+OfOsL+9GuKOf+FDrnyLEkVdII4GHgFDNbl6ycmU0g7LPaQV18qKqrt8TP1KiRrfwz5ZqdosJ87v/GIXy2PvqEORfsqIjxxHtLufav70cdSs4JWzSfB94EjgSWA2eG+75vZtMldQOmm1l/SZcAZwFtgUHAnUAr4EKCVSlPNbP1SeoaDTwSbr6UsD8fuB04BmgN3GdmD0k6BrgVWAsMA2YA3zQzk3Q7cAZQCbxkZt+X1B14EOgXnvp6M5uWJJZbw3L7hT/vMrO7w9duAC4Liz5sZneF+38MXAysBj4P40HS/sB9QHdgG3CFmX0o6avALUAVsMnMvlhTLC6QM0mqpH7AJOBCM/s46niccy5rCgujjgCAU4b3ijqEnHLVl/bjw5VbMIv2e+khv4q0+mQGAeeb2RWSngLOqaP8MGAUUAQsBG4ys1GSfgdcBNyV5Lg/ANeE41buSNh/OUESd6ik1sA0SfEkdhQwFFgBTAOOkrQAOBs4MExYO4Vlfw/8zszeDPOMF4GDarmOA4FjgfbAR5IeAEYAlwKHESxw8I6k1wkGn58HHEyQT80kTFIJGj3Gm9knkg4D7geOA34KnGRmyxNidEk0WpIq6a8E34i6SVpG8E2iEMDMHiT4h+sK3B/2Eao0szGNFZ9zzjWYESOijsDVoKgwn4P7ep6QxGIzmxU+nwH0r6P8a2a2BdgiaRMwJdw/hyDJ20uYpHUyszfCXX9m97LrJwIjJJ0bbnckSJx3Au+a2bLwHLPC2N4GyoGJkqYCU8PjvgwMSeh73EFSOzPbmuQ6/mlmO4AdklYDPQm6Hz5jZmVhnZOALxAkqc+Y2bZw/+TwZzuCFui/JdTbOvw5DXg0TPwnJYnBhRpzdP/5dbz+LeBbjRSOc841nhUroo7AuXQlTgNRBbQhuI0en7qyqJbysYTtGJnlGgKuNbMX99gZ3O6vHluBmVVKGgscD5wLXEPQcpkHHG5mqa4JvNe5M4g9D9hoZgdXf8HMxoctq6cBMySNrq17Y0vnK04551xDKy2NOgLXxEk6WdJHkhZK+mENr7eW9GT4+ju1TflYD0uA0eHzc2splxIz2whslBQfKH1BwssvAldLKgSQNFhS22TnClsvO5rZc8D3gPiCQC8B1yaU2ytxTMF/gLMkFYcxnB3ueyPc30ZSe2BceF2bgcVh/1MUGBk+39/M3jGznwJrgL4ZxNNieJLqnHPO5bBwENF9BLfChwDnSxpSrdjlwAYzGwj8DmiInq53EiSO7wPZWqHiUuC+8LZ94nxgDwPzgZnhSpUPUXurZntgqqTZBAO+bgj3fxcYE65oOR8Yn26AZjaTYEGid4F3CAZOvR/ufxL4gGBQ2XsJh10AXC7pA2AewcAzgDvClTXnAm+Fx7okcmbglHPOOedqNBZYaGaLACQ9QZD0zE8ocybBqHeAvwP3SpJlMCrMzJYQDISKbyfO+5nYv/Qn4euPEiRx8fL9E57v8VoNdc1gd6snwA/C/THgR+Ej0b/DR/z4axJeG1vD+dcCX09Wf7Wyt1bbTnwPfgv8toZjfgn8sob9i6lhlU0z+0oqsbiAt6S6Zq0gF76IrfOJ01u8g2obTOxcnUoIpjeKWxbuq7GMmVUCmwgGIzvXZHmS6pq1/jnQ36f1/9sQdQguApKulDRd0vSNG/wz4GrVLf5ZCR9XRh1Qtkm6T9Ksao9LI4rl0hpiuS+KWFztPEl1zVaJbaEjdIkyBn1aQcE/tpG3qCLKMFwEzGyCmY0xszGdVq6MOhyX29bGPyvhY0K115ez5xfuPuG+GstIKiCYsinlUeOSloR9JWdJmh7u6yLpZUmfhD87p3thCdoAvQlG4h8cjnx/tqbzhwON7g4Hgc2WdEg6FUl6RNLqsN9nfN+tkpaHfV+vA36UEMeTwEnhwLST0qinr6TXJM2XNE/SdeH+Gt+3+lxXLXXtuq7wcWrCMTeHdaV1XbmkySeplfgf/0QbYmuiDiEyMqPIKnY9jmFpRuepiu29b2tZjLnzy9laFtu1HVeW+HxLFSqLQVmMstWVFEwOVvcpfHb7rn2UxSBme50f4IV/bWXV6kpWra5MK+bEeBKfJytTX7XVV1Zte83qShYt2Vnje5fNWOp6D9Ktt7IqmIlmTfhvkXB8k/+96Zqc94BBkgZIakUwgfzkamUmE6x8BMHI+1cz6I96bJi4xeco/yHwipkNAl4JtzP1KHv30Ux2/lMI5kQdRLBc9QNZqAuCSf0PDh/PAYQD0M4jWBzgZIK52lNdG7sSuNHMhgCHA98Jz9cQ15Wsroa4rpwRfX+9etrOVv4Ve4ov530t6lAi96/YU8GTGC3y/RDGV+0jLmAB+QS/m+/N4Dxz5lZw2fh1PPJg0J3roqvWMmnK7qUkS3rns3xFFWePawPAM1O2c8a4IqoMnp9azl8G5/O1T6rIT/jz0ObOzbS5czPtgL8PLuDkf/Xc9dpl49fxzJTte8VR0jufue/VvUJQPL6vjCvGMJ6Zsp2zx7XZFX9iHdX3Z6K2+uL1nHJ6ET+7rycnHraSlaW7++Qmvnf1jQPgqqs3MHlK+a7zJnsP0r3+ObH/smrmU+jaQsZWVFBUBOXlQfwEq90412jCOUCvIZiWKR94xMzmSfo5wdKkk4GJwJ8lLQTWEyQo9XUmwSI8AH8kGLB0UyYnCleU6p/i+c8E/hQm2W9L6iSpl5mlNJdbkrqSORN4IpzAf3H4/o0F/ptCPaVAafh8i4JVr0oa4rpqqSvr15VLFPVycPUlKX4BHwHJVpDIlm4E6wU3hnTrKmbPpd4WEKwX3BB11VeD1tcB2u0Lg1uBlgBrzVTXMYkSPlPxRb3TTkraAQMIFq+O2wksZteH9H2CSa7z6jj/BwTfoJOp7fhkdcT3Z6K2+pYR3IaMmwMMr+Vc6cZR/XNT13sXrwNSu/74+es8r6X/mVoDfJbOMa5F2dfMukcZgKTFwAbAgIfMbIKkjWbWKXxdBFNcZbxEV5g4To2Pmk92fgUrRt1uZm+Gr71CsMzq9HrUdStwCbAZmE7QKrlB0r3A22b2l7DcROB5M/t7Btf2BsGsCEsb6rpqqOuGhryuqDX5ltR0/1jUh6TpjbVUa3OtK4r60lXTZ6qhY26M96SRrqHBBqo15X+DqBMQ51JwdLiefA/gZUkfJr5oZpbwBT7rGvr8BLfWbyNIwm8DfgNclo0TK1hI4GngejPbrN1LoWb9umqoq8GuKxd43yrnnHOuhTOz5eHP1cAzBLeGV0nqBRD+XJ3lapOdP5WBYmkxs1VmVhXOv/p/7J5TtV51KVgR62ngMTObFO5ukOuqqa6Guq5c4Umqc84514JJaqtgWU8ULPt5IjCXPQdjXQw8m+Wqk51/MnBROBr+cGBTqv1Rk4knjaGzCa4vXtd5CpaVHUAwqOndFM8pgr7AC8LJ/uOyfl3J6mqI68olTf52fyOrPi2I19U06suGho65Md6Tpn4NTf38zuWqnsAz4W3qAuBxM3tB0nvAU5IuJ+hTnfGIXEl/JRhM1E3SMuAW4PYk538OOBVYSDC2Iq35VJPUdYykgwluiy8BrgIIB6A9RbB6VyXwHTNLdQWWo4ALgTkKpraCYIWshriuZHWd3wDXlTOa/MAp55xzzjnX/Pjtfuecc845l3M8SXXOOeeccznHk1TnnHPOOZdzPEl1zjnnnHM5x5NU55xzzjmXczxJdc4559xeJF3pdXldUfIk1TnnnHM1acykx+tqWnU1igZNUiX1lfSapPmS5km6Ltx/q6TlkmaFj1MTjrlZ0kJJH0k6qSHjc84555xzualBJ/MPl+vqZWYzwyXXZgBnEay+sNXM7qxWfgjwV4K1Z3sD/wIG17ZKgiQD6DS4K8pTw1xICtrm76BQ0S3mUFVpLP1w+67tfge2Ib8guvejlSopjq56SldWsnJ1DAAzSyuSLl3yrKTP7sXYLGbMm1e5a3vo0AJiMViwYPe+gw4qoKBwz/PkIWIY/z97Zx7nVlU2/u+TmWmnkyl0X6GUpazKWhZBEUSRQktdEAEVF2xLAS0vIOL2A0R9UQq+VWlpeeEFVFBQsO1IWURARWQplM0i+9p9hcl0liTP749z02ams+RmkpybzPP9fO4nuTf3nPMkuTn3yXOepbPh01l9L+/Qd6yKTtt0JLj0t/WZYjs5Y1WgaXj++WTH5p2yz37VVGX9jkSUGBLI7N5LZtyuZEwDaYTWVIzX/t3cTp6ufqPpDvtVWe+tu8+i43fT2XvIfi/Adu8nDSSJ0aZVbE7W0by2lqr1CVI71tK0eVvFwg/sW0NVNTzzbNs6VR3epVCdkJmnBlBPNTU9nV4cRKjbRxle1ehnfKC5Oc0rL2+bJ/eYUEVtrb8FPRGlmhiCn8lq83spXn3DXb9h5ymAYcOG6fjx4wst1lbWrl3L8OGhLvW+MdaKFTBmTGnGCkH2WEuXLg09T0WRopZFDWrSrgyevy8iy4Gx3TSZCvxOVVuA10XkFZzC+mh34+x83G586IpPFEjq/Dhi0GvsWNXkVYaLPvoEG1e1MnhUPy5deLBXWcb3W8vh/Zt7PrFINCbSjJrwTl5tx+5UzZ13D9u6XyvK+TPf487FTUyeUsvceYMAOOKwNaxYkWb0mBj33j9su35qJUazpmnu5N7TlK4CoC6WYtbMjSxpaGbS5FrmX+f67qzN9v1v/wfznJmbaFjcvq9aifG1szfQsLiZUaNjrFrZXiU8YVJ/7lnSwqTJtcyZN3i7MWolFsjk3ktm3K5kbEpX0azVrE/Hmf2NN3nwz41b5emuTTZDqrYpnt19FtnfDcDkKbVcM3dwl+e6x/bvpyldxab0AN5NDmbJ+g/y4vx9GXTzYyQ+eRjPP/wD3l+9hZGjq/jXX0bTpElGjF3xZpcC9cBRsRN7PqlISFUV+9+aYsbQv3K5ZLIAACAASURBVHuTIZFIc9Deq7fu37F4GPG4PyW1VpRhsVpqpKrnk4tEzehX8247fvx4nnzyyQJKY+SEiFNUI4yI5D1PRYmiKqnZiMh44CDgMVwN2vNE5EzgSeBCVd2IU2D/ldXsHbpXatlxwjAOufwEWjuaYkpMs1azo18RmP3woWxa28qg4f08S+Jo9lgmuLoOpk4ZwMLFW3o+OQdumT+M2Ve3Ul237diyJ0axek2SHYf37iY7Z95gfjI7Hdyse7eyMXfeIK64SrfrK/t4qinF+edt5t77W7Yqj4lEmqq6wt+oL/3VaH7wsxZ22qGN3r63rrhl/jDmXpOmSZNU10FzAYf5xn0n8LHWRxk2ondT5QDqvSqoUSEejzFpcu3WP2U+FdSo0LZyd2pGv/qybzkMI4qUREkVkXrgj8D5qvqeiMwDrsDdta4Arga+FqK/6QQOwgNG1Bde4DxpVk/LeFnUDqsp6E06X6LwWdxw3RAWLn736VzOzb6mxoztXFmrjzvLaDYjR1RvdywfCnmz7qqvzPF4PMavbxrK+sZkoJg65bVY101dCRSR+niMmG7//WSTSKTz+pzzVVCzr6la6no4u+/Q/k+ZEeDPB8MITxHdJI32FH2WEJEanIL6W1W9E0BVV6tqSlXTwPW4JX2Ad4Gds5rvFBxrh6ouUNWJqjpxy5pGHvvBfbSmqrxtAC1p/0qZ0Sk5aZDZ19SQIX3j5tmXlIRZMzeyz15rOGfmppKNmX1N1dC/ZOOWA33p2uuJM2asArfKaJQLCxb4lqDPUFRLqogIcAOwXFWvyTo+OvBXBfg08HzwfBFwq4hcgwucmgA83tM4Kx98hQ9e3Ep1nZ9lblNUO6elABZGw+gtiUSaJQ3OP7phcTONV6epNyXJiACNiTR3LEr4FsMIy4wZML3isj1FkmIv9x8FfAl4TkSWBce+C5wuIgfilvvfAGYAqOoLInI78G8gCZzbXWR/hpHHTCBdO8CbX2q/WIrmdA07VhXG/7ESaDaF3YgI2X6Qk6fUtlNQE4k0MsBf0IzRt6mPx/jcyXFTVA2jC4od3f8P6DS3x93dtPkx8OOiCVUEWtN2k+uMZvPbMSLCnHmDuebqVLtl5kwmhOMnD+C7v9zJo3SGYRhGZ5Qsur+YrH7oZSZcmKJqgJ/l/pqqFC3papq1Ij7OgtCs0cgwYBgZshXUxkSahsXOBeC+hi2cf2Uacxs1So0t95cpixb5lqDPUBGOWcM/uqc3BRWgLRUdS2pLIrek7aWgWWPeN6NzGhN921+4Ph5j8pRaAI6fPKAkGQgMoyOZ5X6jzDjkEN8S9BnK3vRXv8cI9v3BFN9iAP4Dp/5w0ZO8cO8K9vvkGE6ZPdGrLOaTGl0yy9zdJb7vC2Ryx8qAGjb1bZ3dMIwwjB1raahKRNkrqSpCWwR8QlvSfj/K1qYkL9zrKmC8cO8KTv5hkn51fmVqVv/fCxWyWlAoElnL3A2Lm7niqjS19R7r13qiMZGmus65ADSZglpy8s1ZW2nYcr9hdI/NEgWkOV3jbUvXDmDv413wx97H70S6doBXeaKQzP+Sc9eB5R9sRzxrmXvylL5Z8Wfa2RvYbc9VJc2Zamxj1syNHLT3ambN3OhbFO/Ycr9hdE/5W1KRSPiEtnq2pAJMvepwJl1+iHcLagafgWRNiTT3N1hKsM6YO28Qv7iagpcQLQcSiTQLO1iS+1IKqra03z8l2TlrlzQ0W+Up4Nb5o7hj0as5VcYzIsK0ab4l6DNEQ5upEHwv+QNQW01LBJYvffvn1sVjfOykOH/9sy2ldUZnJV77AvF4jKlTalkY+OTacn9pyc5ZO2ly37Tkd4FdheWEVZwqGRHQqnqHKiQ9WweqY2laUtUMLPtPs7K44toR/PXPr5uFwmjH9dcN4edXp/ukJTkKzJk32CyoRnlzyCGwdKlvKfoEplYVgIySHAlLaoSIgl9quWEBJaWh1JZkxbThbOwaN8qap57yLUGfoQJmCiGZinndIBo+qVEiCoUNrjhvBZRR4NSsmRvZZ681FtBTgTSyiefSj/oWwzAMo6zwr0lUABlF1WiPT0vqlkSKh/7c6G38sGQHlGQCeszaVFms5m320YlUi60wGEZZM3q0bwn6DGWvpKZT6l1JrK5K05quioQ1ta2pjZo6/zdB34FTA+JVfOSkHfj7n98rSv+JRJra+q6vu55e70h2QElfTQ1V6YxkZ1NQja6wH3w5sWKFbwn6DGX/w2h5fRXvzP6DVxl8K8kZ/vqdh7nlo7fy1+887FsUwG/e2OZ0DefP2Q3gmUK/r3NmbmKfvdYw7ewNeb3eFXPmDWb5f0Zw1ewdCiGmETVEkKoqb5sRTc6YsQrKyC3JAC67zLcEvUZEbhSRNSLyfNaxISJyv4i8HDx2WpJQRL4cnPOyiHy5mHJGQ7vqJe8/8gJtiTZS6ZiXDaA1VUVLqtrb1vi+8vpf3gDg9b+8QeP76lWeKJRFvfqbbwAcUMg+sys2LVzcTGMi3e3riUS44JxvXfQe++y1xhKdVyCr9S2S2uZbDCNCWMWpMuXyy31LUAhuAk7ocOwS4AFVnQA8EOy3Q0SGAJcChwOHAZd2pcwG548UkRtEZEmwv6+InJWrkBWhpNYfuR+xAf29jZ/ynAILoKauhl2O2xWAXY7bNRJL/s3az9u2qbGKf95d+ACk7IpNU6fUUt9hWb7j62GW7bMV3CUN4RVcwzDKC6s4ZfhCVf8GdFzumwrcHDy/GfhUJ00/CdyvqhtUdSNwP9sru9ncBNwLjAn2XwLOz1XOvJwoA615Z1V9Np/2haTf+NGMOv800il/MsSqnDLh2yf18CuO5+DvOJ/UVs/6je90XLXxKg4/cQiP3R1uyT0X5s4bxFWz0wyt7/w9Zr8eJs1RRsFtWGyJziuRkTLOfFKN7bCKU0aEGKmqK4Pnq4CRnZwzFng7a/+d4FhXDFPV20XkOwCqmhSRnDW2nDUJEXkIODlosxRYIyKPqOoFufZRDCQmPocHIJ2K0ZqOhs9XFCyoGXwv+U//+T48dvcjRZn8e1Ig81UwMwpuVV00riejMAxkMPtXH+VbDCO65PxvVkSmA9MBxo0bVzSBjG548knfEuTCMBHJFnSBquZcKktVVUQKkeA5ISJDwSWLFpEjgM25Ng5j7tpRVd8Tka8Dt6jqpSLi3ZIaJaKiqBrtKLs183g8ZpWQKg3x/2faqAwCRWMBwMSJE22mMLpinapODNlmtYiMVtWVIjIaWNPJOe8Cx2Tt7wQ81E2fFwCLgN1F5BFgOHBKrgKFUVKrA6FPBb6XSwMR2Rm4BWcyVpwmPydwvP09MB54AzhVVTeKiABzgBOBJuArqtptaQdVZ8n0TVuqCqJjxPROS7raKk4ZhmEYlcfEiU75qDwWAV8GrgweF3Zyzr3AT7KCpY4HvtNVh6r6lIh8FNgLEOA/qrlHkIZRUn8YCPeIqj4hIrsBL/fQJglcGAg5EFgqIvcDX8FFkF0pIpfgIsi+DUwCJgTb4cC84LEsaE2ZJdUwDMMwjGgjIrfhLKLDROQdXMT+lcDtQfT9mzijJCIyEThbVb+uqhtE5ArgiaCrH6pql8EfInIu8FtVfSHYHywip6vq3FzkzFlJVdU7gDuy9l8DPttDm5XAyuD5+yKyHOdgO5Vt5uKbcabibwfHb1FVBf4lIoMypueuRxHSKb9LabEqpc2W+tvhO4jMMAzDMIzOUdXTu3jpuE7OfRL4etb+jcCNOQ41TVWvzWq7UUSmAYVVUkVkT5xlc6SqfkBE9gdOVtUf5dh+PC5h8WN0HUHWVdRY10pq2r/LYUZJNp/U9viuOmUUjsZEmuo631IYlUIiYWV/jTLm0kt9S1BOVImIBMZHRKQK6Jdr4zCzxPU4v4M2gCD91Gm5NBSReuCPwPmq2q5OZSB4KOcOEZkuIk+KyJOtb65kzS9uQ1PibYPAJ9XYSkuqvCyp2dfUhg3+//hEiWlnb2C3PVdxzszC552tZLKvqTZt9i1OZJg1cyMH7b3aClYY5UsFVJwqIfcAvxeR40TkOOC24FhOhFFS61T18Q7Hkj01EpEanIL6W1W9Mzi8OgjCokME2bvAzlnNdwqOtUNVF6jqxEzkWtPjz5FubgnxVgpLRlE12tOs1d63XMm+poYMMQtPhkQizcKgwEBDHhW0+jLZ11SN1PoWJxIkEmmWNFjBCqPMGTOm53OMDN8GHgRmBtsDwMW5Ng5j7lonIruzLdfVKXS3DO/OEeAGYLmqXpP1UlcRZIuA80Tkd7iAqc3d+6M66g7dn1i/Wn/JhmJKMh0za2oW5pNaGcTjMaZOqWXh4mYmb62gVZFRrUYJiMdjTJpcy5IGK1hhlDEre1RLjABVTeNcRefl0z6MJnEuLjfb3iLyLvA68MUe2hwFfAl4TkSWBce+SxcRZMDduPRTr+BSUH21J6Fqxo1h+DlnhHgbRSBtltTOMJ/UyuD664bw86udT6rlbzV6y5x5g/nJbPNJNYy+gIgcBVwG7ILTOQXn6blbLu3DRPe/BnxcROJATFXfz6HNPwKBOqOzCDLFKcM5E4WKUxkswn8bEQoiszthAaiPx0KVeK1EGhNpYhY8VhBMQd0O+0DKiYMP9i1BOXED8F+4SqWhC9j3qKSKSKdlTyWooNJhGb/0aDR8QpOpmCXz74DvsqgLv/UYuIwShtErLj53Pfc1bGHqFPMtNQrLGTNWgc1T5cXSpb4lKCc2q+qSfBvnYkkdGDzuBRyK8xsFmAJ0DKQqOZqOzvqj+aRGh9amJC/e9w5R+EZiG9Kk+3gwViKRprbe/5/JfGhKpLmvYQtAJogs9JcZ09AGBKMP0JhIc8eihG8xjLBMnw4LFviWolx4UESuAu4Etka491RNNEOPk62qXq6ql+Mi7Q9W1QtV9ULgEGBcfjIXjra3V7Bu3m+RlHjdkhEozRolWlNVtKSrvW1aW8uE43f2eoE2BpHLI/7fZo9S+Od7567hoL1XRyqFVWOIqPK6eIzjJw8AyFhSQ/s9DGEzzyYfCdvMqHDq4zE+d3LctxhGWK6/3rcE5cThwETgJ8DVwTY718ZhNKuRQGvWfivbkvB7pWnps15TUGVIpmPet5ZE0rsMyXQsEj6p8aVrGNKL9mvXtM+wlotik0mpc87MTey25yp+9KUNDFzYTM3rPWZrKytyTR3UlEjzwJ+bgOKlsAqjcELueV9TbS20Nrnv7WfXDuXVZWO4/rohkIcldQgwUN8imXvJ6orGUk8ZRt9AVY/tZPtYru3DTLa3AI+LyGUicjmuctRNIeUtClJTQ6ymFlLibUul/VtSX7/yTp495Spev/LOnk8uAaW2nrYmq6BRoVFperOJyevz/+Ny9GGrOeqQtRx92GoAzvj62h4Vm3NmbmLfvdbwza9u4MHFzcSBEQ+6/3UDF25BEumtGyV0UwmrxPXEOTM3sc9ea3JKxl4Xj3HcSS7aaFsKq230Vlk5Z+YmRk14hzNnrMvp/Fzzvj6XfpTH7/oesz+0mIvPXc/F565n9wNXcOCh+fsPngbUapJabaNW2xCNjqtSKbFk/tuw5X6j0hGRkSJyg4gsCfb3DTI75UTOmpWq/hiXEmojsB74qqr+d1iBi4G2tZHc/F7PJxaZZCrmbWtpTLLp78sB2PT35bQ0Jr3K4wVVPv7r5cw+5g7mf2oxV+TZTVtSWbXSKS+rVqY580vrWXS380nsSrFJJNI0LG5GgAPub2WzQCNslWHo1Y3ssfdqdt9vNYMXJEqWajRj0S3UUnvmfULuydh/fO0Inn5xJHPnDWp3vLfKSrYsdy5uykkZz+R9hc6VZnAW1NVZ1Znva9iy1Sd1xYr8leorgLtTf+Ku5B85Jf0i0gfzzVoy//a89mZrzycZ0ePd7WoMGV1zE3AvkKmA8BJwfq6Nw2oTKZw/VmaLDFU1td78UQHS5pPajrZUFa3p6pJuLdKPu6YdzOy5x7Nu+IC8Za+pFkaNdt/nyFExHnpw2xLtCZP6d6rYxOMxJk+pJQ0snVLL+juG0Dqq/Xmto2O88/uhbPivgVBV/CCibCWuUEvtmfcJhErG3pkFNVtZacpDtmxZPjOljvocZbn+uiG89tKo7ZTmDFU1/RmZVfju+MkDtvqkjhnTu9/5agbwraqP8ZuqD5IWmzP6OiOGWdGTssSi+8MwTFVvJ9AZVTVJiFRUOc+SIjIL+C0wDBgB/EZEvhFO1uJQd9ABxPr39zZ+RlH1abnUfrUMPGo/AAYetR/ar7bvWVIDXj54FA+evm+v+vjb4yN5ZOlw/v7EyK2K0EmT+rPg+sFdtpk7bxDL/zOCufMG0XJEP1q/Xt/u9c1fi9N8eL9eyZULGWU0W4nrymqYD5n3OWde159FT2QqD4FTduvylG3uvEGsenknbpk/LFS7nhTaD8Y+xGGf/jEXPTqFn107lJ9dO5RVL+/EsidGATydl7DAwthePB8bkW/zsqfj997X86WOGlHNTmP8++8bITn5ZN8SlBMJERnKtmqlRwA5RxOH+Rt3FnC4qiaCgX4KPAr8MkQfBaff2LGM/PKZeaSILRxapc6S6jlP6k4XnULyrBOoHlzf88lFxndhg4MefIstMWFtL3w/h49wP4+58wbxv9dUE6tL95jQPvum23/JFtK1sOFTAxhy1xbq721m09nF/W7OmbmJhsWu5OT86wYxd94gfnE1Ba8WFY/Het1fduWhpl4YeTMKZ2PCVcUqFFU1/elXt22KrI/HaHLff2hp1wJbEI7Sd/gD+xROyDJkzrzBfP/y5NbfV1/n9aXjqRn96jO+5TCMInEBLnXp7iLyCDAcOCXXxmFmCaG9Kpii62pSpSMCFacyllTfwVMrrr6dxn++QP2R+zHmwlN7blBkWlJ+bkKD1ySoSbRxaFp5qwD91UqMum0KSk5UrUwhTcqahqG8v1c/EmfFGfWNTVStSpEaVRwFvjN/0Xg8FulqUYWypJ05Yx13Lm5i8pRarpmbv4W3WLwFHIqyUNsYok1skL5bumrWzI0saXB/pHpjja8wKiv9h2EEqOpTIvJRXK59Af6jmnuakzB3iP8DHgui+y8D/oUrd+UXBUn53aJAeksLjf98AYDGf75AeovflFw+CxvEUmkuv3Eqjcft6k0GUrBx8XDa9nbm9da9a3h70bCiXi/5+ouWO42JNHcuLm6aq0KwTsYxq/p4qvpgwFQGC5wyKoL5831LEHlE5DOZDTgZp6TuCUwJjuVEzqYuVb1GRB4GjgoOfVVV8/bNqiQkBWnPpVljA/pTf+R+Wy2psQH+fHQztKb9WFJXjnTWmcOvOJ43H5jv5RpN7VRFjQhkpRnSAUJybHGV97nzBnHFVRooqH1DGaqPx/jMlLqtltQoKucDGcz+1UfRCqwNtYBVWWR8UjOW1Ch+V4bRI9On+5agHJgSPI4AjgT+GuwfC/wTV4GqR8LOlsuAlZl2IjJOVQuxopo3ov6tmRroHd4j/FW2PvqWJRmBvLEBZWmqqRWlWfP742M3/ggi/t2SDMMoEB0MEOWIiOwF/D7r0G7A/1PV/8k65xhgIfB6cOhOVf1hLv2r6leDPu4D9lXVlcH+aELk2M9ZSQ0i+S8FVrPNH1WB/XPto1KRFKhnS2q6uYXGR58HoPHR50nP/BSxWr/W1ChUnTIqn47L/VdclTZFPaJ0XO7PBM4ZhlFaVPU/wIEAIlIFvAvc1cmpf1fVyb0YaueMghqwGnKvWB7GkjoL2EtV14do06fwar2sGUD8iA+S+NdzxI/4INQMIO3Rwuw7DZXRdyiH5X7DYcv9hhFJjgNeVdU3i9D3AyJyL3BbsP954C+5Ng6jpL5NiNxWpULTujW63itp/zKMnHUa6Rmf9m5BzdDqMXgqi6LfBTNR9OVEVKP98+WW+cOYfXXr1lRbUftOtMI+796QnXrMMMqSyb0xLEaS09imRHbkQyLyDLACuEhVXwjTsaqeFwRKfSQ4tEBVO7PYdkoYJfU14CER+TOwNXRcVa8J0UfBaV35LqtuvYVRZ5zpUwzAf/AUADW1Xi2oUeLpy5ZAnnXWc2Xa2RtYuLiZyVNqu6xgZJSGTKqtKKY4amQTf2tbyNE1U32LEglMQc0fEZkOTAcYNy7nVVOjkCxe7FuCXBgmIk9m7S9Q1QUdTxKRfrjo++900sdTwC6q2igiJwJ/AiaEFURV7yTHQKmOhJkp3gLuB/oBA7M27zQ+twxtavGagsq3T2rUSKZitKarvG1NiRQrH3ylqO+xMZFmYYHLjhq9I8opjlpoojm9xbcYRpmjqgtUdaKqThw+fLhvcfomU6b0fI5/1mWuk2DbTkENmAQ8paqrO76gqu+pamPw/G6gRkRClfcL0lC9LCKbReQ9EXlfRN7LtX2YFFSX9yDIL1X1Gx2O3QhMBtao6geCY5cB03BFWAC+G7x5ROQ7uMpWKeCbqnpvLrLVf+BAz2VR3aMpqtGhekA/Rh4zgdUPvVy0MerjMaZOqd1qSTXrkH+i7PPYnzpqYwN8i2EYRm9paPAtQSE5nS6W+kVkFLBaVVVEDsMZNsPGJf0MmKKqy/MRrpAJ+47q5NhNwK+AWzoc/7mqzs4+ICL74vwi9gPGAH8RkT1VtdvF6/6jxjLm1DO9JhrSWFB1ynNZVHBR/lHwSfVdfQvgg5eexOqH/qeoeVKvv24IV85OelOGouZ7GQWi6PNYzyA+VDPJtxiRwa5bw/CPiMSBTwAzso6dDaCq1+HKl84UkSSwBThNNXTurdX5KqhQWCV1O1T1byIyPsfTpwK/U9UW4HUReQU4DHi021ZRKIsakRXFtdfeStPjz1F32AcZfu4ZvsXxWnUqi6J/O75utufM3ERDBPxho6hwRE0eiUAF6agQRZ9hw+iLqGoCGNrh2HVZz3+FMzT2hidF5Pc4f9bseKacfFR9zeTnicizInKjiGRmqbG4DAIZ3gmOdU+UyqKmxduWbmql6fHnAGh6/DnSTa1+5bEUVEUlkUjTEAF/2FkzN3LQ3quZNXOjl/HLhffZyLPJR3yL4Z0o+wwbRs6UeSL/ErMD0AQcj6tCNQXnBpoThdQkcjUVzAN2xyWRXQlcHXogkeki8qSIPJlqSoRtXnAkBZg/6na0pau8b7mSfU1t2BD9G2c8HmPylFoAb/6wpnB0T/Y1BbBa3yKpbb7F8krGZxiInM+wYeTMgq5ikIyOqOpXO9m+lmv70DOEiNR18dKcXNqr6mpVTalLHHg9bkkfXLWDnbNO3Sk41lkfW6Mbq+vikbGkakq8bVJTS91EV/yrbuL+SE2tV3nKzZKafU0NGVIess+dN4jl/xnhbanfFI7uyb6mAEbKOKolAo7rnpkzbzBPvzjSlvqN8mXGjJ7PMQAQkT1F5AEReT7Y319Evp9r+zBlUY8E/heoB8aJyAHADFU9B0BVb8qxn9FZJbI+DTwfPF8E3Coi1+ACpyYAj+cqn0+csir4XgAYdvYXSDefEonAKYiMT2pF41sxjGKQUhQZyGD2r+4strRvYteLYfQZrge+BcwHUNVnReRW4Ee5NA4TOPVz4JM4ZRJVfUZEju6ugYjcBhyDSyr7DnApcIyIHAgo8AZBVJmqviAitwP/BpLAuT1F9keNKFS+qqqpdQm8PBOJwgZGSTCFIwfEfg+GYfRJ6lT1cWk/ByZzbRwqul9V3+4wULfqkKqe3snhG7o5/8fAj8PIlAmc8k0ULKlRIxmBNFSGYRiGUVAWLfItQTmxTkR2xxkmEZFTcPFIORFGSX07WPJXEakBZgF5576qSMx6uBUrbGAYhmFUJIcc4luCcuJcYAGwt4i8C7wOfCHXxmGU1LNxwVFjcQFN9wWD+yUyllTfEkSPZJkFTxmGYRhGj4wda2mockRVXwM+HhQOiKnq+2HahymLuo4Q2m9fJAo+qVHBfr6GYRiG0bcRkaG4eKQP41bi/wH8UFVzKq8aJrr/Z7horC3APcD+wH+p6m9CS11gYhGwYpoltQNpMUuqYRiGYfRtfgf8DfhssP8F4PfAx3NpHGa5/3hVvVhEPo2Lyv9MMLB3JTUqmCV1G2ZJNQzDMCqSadN8S1BOjFbVK7L2fyQin8+1cRglNXPuScAdqrpZIpBWRRTEc6EbjWUUVFPNMljgFDSrVWAyDMOoOKziVBjuE5HTgNuD/VOAe3NtHEZJbRCRF3HL/TNFZDjQHKJ9xeJbSY4qKUtBZRiGYVQahxwCS5f6lqJcmAacj1t1V6AKSIjIDEBVdYfuGocJnLok8EvdrKopEUkAU/OXu3D49gfVqmjIESXM9cEwDMOoSJ56yrcEZYOqDuxN+1DJ/IG9gfEikt3ult4IUAm4sqi+pYgeaQucMgzDMIw+izi/0C8Au6rqFSKyM85PNaey92Gi+38N7A4sY1ulKcWU1K2YopqFWVINwzCMSmT0aN8SlBNzgTTwMeAKoBG4Fjg0l8ZhLKkTgX1VI5bBNirJ/M0vdTvMkmoYhmFUHCtW+JagnDhcVQ8WkacBVHWjiPTLtXEYJfV5YBQhaq72NcwPcxv2WRiGYRgVyWWXua3MEZE3gPdxq+NJVZ3Y4XXBVRo9EWgCvqKqYR1y20SkiiD9URB0n7NZL4ySOgz4t4g8DrRkDqrqySH6KAqRSeZf41uKaJE2RdUwDMOoNC6/vCKU1IBjg4qinTEJmBBshwPzgscw/AK4CxghIj/GpaD6fq6Nwyipl4WTq+8RBbeDqCApsayxhmEYhlG+TAVuCdw8/yUig0RktKrmvKKuqr8VkaXAcYAAn1LV5bm2D5OC6mER2QWYoKp/EZE6XL4rv6Q1EsqhWVK3xxL6G+VGW1ObbxGMvok58Bs+UFyyfQXmq2rHKgVjgbez9t8JjvWopIrIkKzdNcBt2a+p6oZcBAwT3T8NmA4MwUX5jwWuw2nH3tiy/l1ee+AWdjvuTJ9iABY8lU0U/jgYRhh+MesVHrv7cQaPP4BD2Mu3+Zn0LgAAIABJREFUOEYf4YwZqwAOyvV8EZmOuxczbty4IklldMuTT/qWIBeGiUi2oAs6UUI/rKrvisgI4H4ReVFV/1ag8ZfilGABxgEbg+eDgLeAXXPpJMy/t3OBo4D3AFT1ZWBEiPZFY9Nry0g3NyMp9bKBKWVG+dKsZvFuSqR57G73x37jG8+QVLOoGsWnMZHmjkWJUG1UdYGqTlTVicOHDy+SZEYFsC5znQTbdrVcVfXd4HENzm/0sA6nvAvsnLW/U3CsR1R1V1XdDfgLMEVVh6nqUGAycF+ubyKMktqiqq2ZnSChfyTcDgfvegBVNf29jZ+tqNqWpbCnxf9mGDlQF49x+IludWrw+AOoFvPdMYpPfTzG506O+xbDCMvEiT2fE3FEJC4iAzPPgeNxWZyyWQScKY4jcBVHw2Z4OkJV787sqOoS4MhcG4dRUh8Wke8CA0TkE8AdwOLuGojIjSKyRkSezzo2RETuF5GXg8fBwXERkV+IyCsi8qyIHJyLUHVDxrLH0WcSS+FtA7OkdsQ+D6Pc+OacPfj8X89k92P9uw4ZfYdb548CeNq3HEafYyTwDxF5Bngc+LOq3iMiZ4vI2cE5dwOvAa8A1wPn5DHOChH5voiMD7bvATknmg0T3X8JcBbwHDADJ/z/9tDmJuBXtK9KdQnwgKpeKSKXBPvfJt9UB+LfWhaFFFhRxAKnjHKjps4sqIYXLKLBKCmq+hpwQCfHr8t6rjhXz95wOnApzp1Agb8Fx3IiTHR/GqdJXx+izd9EZHyHw1OBY4LnNwMP4ZTU/FIdRKDilFY5RTVtyupWIpTM36JmuyCRSFNV5z9Bh2EYNk+VFZde6luCsiGI4p+Vb/selVQReY5ufE9Vdf+QY47MUjxX4UzO0ItUB77xrSRHFd+K6toFv4EQUbN9iXNmbqJhcTOTJtcyZ95g3+IYRp8lbHS/EQEqJ5F/5MnFkjo5eMyYfH8dPH6RXgZOqaoG+blCkZ2Co188OjdYU1a34fuzSDe30LT02ZzPz76mxoytbOtiIpGmYXEzAEsamvnJ7DS19ZGxfFcM2ddULXWepTGiSD7R/UYEGDMGVuTsVmn0gh6XGFT1TVV9E/iEql6sqs8F27dx0WBhWS0iowGCxzXB8ZxTHWSn4KjpHyeWUu+b5UjthJR422I1tdQdvJ27TZdkX1NDhlT2yls8HmPylFoAJk2uJR6v7Pfri3bzlNT6FseIIBbdX6asjPwCb8UQJnBKROQoVX0k2DmS/PxoFgFfBq4MHhdmHT9PRH6HC5jKJ9WBV3xbD6NEFJT24V//Im+e84xFzXbC3HmDuGq2+aQahm9unT+KOxa9avOUUVGIyC/p3lX0m7n0E0ZJPQu4UUR2DPY3AV/rroGI3IYLkhomIu/gIryuBG4XkbOAN4FTg9PvBk7EpTpoAr6ak1QKkgzxLopEVMqipltbiPXzlzM2G98+qQERUJdzJ5FIl8yyGY/HaI5EpuM+gNoHbXRLWc1TfZ6Dc8qQ2dcpSFmuMNH9S4EDMkqqqm7Ofl1EvqyqN3do01Wage1KqRYo1YFXfKeiemvxLbz30jJ22PNAxk3xm+vRrMrhmTVzI0sampk8pZa58wb5FscoIO+zkWeTj7B/9VG+RTEMo7csXepbgsjTUR/Ml9AmG1Xd3FFBDcg7xUCvULz7o4J/pSzV2sJ7Ly0D4L2XlpFqbfErkBGKRCLNkgYXzNSwuJlEwgwrlcZqfcvKrRpGJTB9um8JygYRGS4is0XkbhH5a2bLtX0h1xUjsbbrg62KatrfVl3dnx33PBCAHfc8kOrq/l7lAbfc73srF+LxGJMmu+CayVMsmKkSGSnjrNyqYVQC1+ecLt6A3wLLgV2By4E3gCdybRzGJ7UnvDhdiSqS8uvvpVXikvl7lQLGnXQmqU98nqoI+KT6tiyXI3PmDeaaq1OmoFYgAxlsS/2GYfRFhqrqDSIyS1UfBh4WES9KavmYrQqMbyU5qpiiGp6oKqiNiTTVluozfyJQvtmINNH84RtG78n4OK0UkZOAFcCQXBsXUkl9pIB9hcJ3wFK6yilkvpWyN5fcwuaXl7HjhAPZZZIFThmFYdrZG1i42AV0XTM3OsUzDKMSsIpTZci7naZwNzrnR0HA/YXAL4EdgP/KtXHOSqqIDALOBMZnt8vkulLV83Ltq9LwrSSDC5za/LILnNr88jJSx/lf9jdFtfxJJNIsXLwtoOuKq6w6lWEUCqs4VaYsXeqqThk9oqoNwdPNwLFh24expN4N/At4Dv/ul5HEp1JWXdWfHfc4kM2vLGPHPQ6kuqo/eJTHFNTKIB6PMXVK7VZLqnNHMPcWozJoVr9/uDIVp0xRLTNOPtlyH/eAiFysqj/rKql/MZL516rqBSHOLw0KkvR/sTi/VL8T3vhPnknqWP8W1AzlFF1vdM311w3h51c7n1RL/m8YhcUqThkVyvLgsVdJ/cMoqb8WkWlAA7A1CaeqbuiNAL1G/QcuaZVTxqKw7B/zbEHdKkcEZDAKR308RrPaAophFAn7cRkVhaouDp42qeod2a+JyOdy7SeMktoKXAV8j22mWwV2C9FHRSIpjUQKqqhhS/6GYRhGxTF/vm8JyonvAHfkcKxTwiipFwJ7qOq6EG36FKaUbUNMYzcMwzAqEas41SMiMgk4ERgrIr/IemkHIJlrP2GU1FeAphDnlwQBYkn/GpEpqNtjiqphGIZRcYiUfeCUiOwM3AKMxK2KL1DVOR3OOQZYCLweHLpTVX+Y4xArcP6oJwNLs46/TzFSUAEJYJmIPEh7n9ScIrT6Ar59Y6OEBU0ZhmEYRmRJAheq6lMiMhBYKiL3q+q/O5z3d1WdHLZzVX1GRJ4HPqmqN+crZBgl9U/BFi0iUxZVSfXdoludYtZlwzAMw4geqroSWBk8f19ElgNjgY5Kam/GSInIziLST1Vb8+kjZyW1N5pwpZNRki2i3TAMwzAqnMmhDYuRRkTG46qePdbJyx8SkWdwy/cXqeoLIbt/HXhERBbhVuQBUNVrcmkcpuLU63SekNVvdL+CePZJ1eoYkgT6eRUjUpgV1TAMo/CIyHRgOsC4ceM8S9NHWby453P8M0xEsnOULlDVBR1PEpF64I/A+ar6XoeXnwJ2UdVGETkRt5o+IaQcrwZbDBgYsm2o5f6JWc9rgc8BQ8IOWIlklGRTzNpjn4dhGEZhCRSNBQATJ060QAgfTJlSDorqOlWd2N0JIlKDU1B/q6p3dnw9W2lV1btFZK6IDAuT5UlVLw8jdEfCLPev73Dof0RkKfD/eiNAIZCUZ0tqVcx8Ujtgrg+GYRhGRdLQ0PM5EUdEBLgBWN7V0ruIjAJWq6qKyGE4a2hHXbCncYYDFwP74QycAKjqx3JpH2a5/+Cs3RjOshrGEtuxvzdwqQhSQFJVJ4rIEOD3wHjgDeBUVd2Y7xilwreSHFXMkhptfNctNwzDMLxxFPAl4DkRWRYc+y4wDkBVrwNOAWaKSBLYApymGjr31m9xet1k4Gzgy8DaXBuHUTKvZptPahKnROZc2qoLju1gNr4EeEBVrxSRS4L9b3fXgahGwicVIGYpqLZiKagMwzAMI5qo6j+g++VfVf0V8KteDjVUVW8QkVmq+jDwsIg8kWvjMErqJOCzOCtnpt1pQK6JXXNhKnBM8Pxm4CF6UFKjgCTTQYS/KWbZmCXVMAzDqDjKPJF/iWkLHleKyEm4LAE5xzOFzZO6CRft1RyiXVcocJ+IKDA/cAYfGeTuAliFq4RQNkjOhb4qH1PaDcMwjIpkwQIrjZo7PxKRHYELgV/iyqIWpeLUTqp6QkjhuuPDqvquiIwA7heRF7NfDBx1O/27kp2Co7ZmR+/L/eAChcxw2J6IBE/Fcjkp+5oaM7aq03MaE2mq6wonmFHZtJunsAsnm0QiTTye00/TMKLHjBmmpPaAiNTifFD3wBUJuEFVjw3bT5hZ4p8i8sGwA3SFqr4bPK4B7gIOA1aLyGiA4HFNF20XqOpEVZ3YL0JaQyyltm3dfH8b8NbiW8AlKO6R7GtqyJDtfxZnzljHbnuu4pyZmwospVGpZF9TNVLbc4M+wqyZGzlo79XMmhn5mFjDMPLnZlyA/XM4d9Gr8+kkjJL6YVxt1/+IyLMi8pyIPJvPoCISD2rFIiJx4HjgeWARLvKL4HFhj52pQirldwMkaT4qHZG0vy3d3MJ7Ly3rWcgcaEykuXNxEwANi5tJJPxb7g2jHEkk0ixpcN5iSxrst2QYFcy+qvpFVZ2PyxLwkXw6CRs4VShGAne5NF1UA7eq6j1BxNftInIW8CZwagHHLB4ZRdWi+7fiO2iqql9/dtzzQDYXQFGtj8f4zJQ67lzcxOQptbZMaRh5Eo/HmDS5liUNzUyabL8lo0xZtMi3BOVAJmAKVU0G+l5owiTzfzOvETrv6zXggE6OrweOC9tfFHxSLVBoe3wrqruccCbPvrTs6UL0dcv8Ycy+utV8Ug2jl8yZN5ifzDafVKOMOeQQ3xKUAweISKZilQADgn3BhR3tkEsneSfjN7YnCn6YRvGoj8dY35i0m6sRHktZ0w77DRllzdix9pvuAVXtPAI5JGU/U2g6DcmU3w2IRcCaC5BKtvgWAYhGYYM3l+QeOJUL087ewD57rSlq8NSaNUkSiTRvvNHW88lZVJpvX8f30937ayyD9/4+G3k2+YhvMSJDpV2vBcAMRobRCWWvpL7fuoZlqzz7hyS3BU/53JY/+RseuecHLH/yN95lAbfc72tLb2lh88uFCZwCpwgtXOwCPnoTPNVdu6MPW83Eg9exz15rOPrD69l/wsrtzums/TkzNxVdeS4lF5+7nn32WsP0aRtJJNLdRoOfM3MToya8w5kz1nXSU7RYrW+R1HB/PioRi+5vz66HvAGduL8ZhlEBSirAqsb/kGxtgmTSz4b/oKlUsoW1q1yyhbWrnvVuUfVd2GDzG/8uaH/18RhTp7g0QvkGT3V3c167Jsmqle0V0OZmeDPLotpZ+0QiTUMBlOeo0JRIc1/DFgDuWdLCPnut6TIaPPu937m4qSwsquuTq3yL4BWL7m/PqjVJ3llhfmJlx7RpviXoM1TEEsOouj2pjvXzJ0DSf6mpqur+DB+1P2tXPcvwUftTVd3ft0hefXQH7Dii4H1ef90Qrpydn09qx5vzD36Wpi6rn+EjqqmtdYpphtpa2GV8TaftfzI7TW29EI/HmDyllobFzVuV52b/nhZ5UxePcdwJtTxwz/ZF7TqLBs+8989MqaM+HqNZo630DK0e5VsEr1h0f3sSTb27Xl9c9T5HXfnXvNrmGWxd9L4ApMBByAV9rwATToerHixMfwX+8CotfLvsldSBNcM5cMgJW5fcvVBdhSTT3q2p6LZH37L49kmtqRtYlH7zval2vDnXdegnkUi3U1DvuW8Iu+3Tv8v2Tg73Gc+dN4irQkRLN6tQ23kxt0hw9fxhXPT1Nfzl/tatxx5ZOpzhI7ZNV7NmbmRJg1PMV728E/XxGE3a/Z9Fq3JkRI2Rw3t3C67vX80Ruw0N3U4p4O+/wFNJoWcmLWCAU6ani757BrN/cmvv+yviZ1cYFdo/Za+kFvofV174VJAzIiRbWLs6WO5f/SzJ5Gep9mxN9ZmCqn//Hdzf5whFYGan3uloQInHY4waHWPVyjSjx8TYd99+21lEu0vdE3XlK5Fwlt9cufH/hjB92kbuWdLCpMm17RTUbKtyw+JmuKbn/rKV2huvGxJa/kIQo4pqqfEydlTobEUg6tduMamPx/jcyXHuWJTIq/1Ogwdw9anmzlpyTn+ROacVLCa3KPzqDN8SFIa+OzsUgVgy7W3rRw0jRriqtSNGfJB+1HiVx7clt7XpvUgpqBm6uiEnEumtPqkrV6S79NUrxxv6985dw0F7rw4d2LXg+sEs/88I5swb3O54xqoMbrm/vofPpKNS68t3NU2K5vQWL2NHhezvzpb7HbfOHwVQkHzOhlFplL0lFdVI+IRKKg0UJC1Y3nxgv9NJ7vUZ7xbUDD4V1UL7KK9ek6RJk+w4vDg31ezl/FL4liYSaarqin+9NiXSPPDnbSVlr7gqnOWsq88hY1UemoN1tuNn25NSW0yqpfyn3N4yZ95gZn6jlb339RhHED2i7UxttGf0aN8S9Blsxiwgvq2HADXSDyIgh+/CBlU1hVPUDzx0FStWuHvI6DExHnu88EFZ4G7e11ydKrp1KbP0PWly7XZWykJTF4/x4eNq+ccDzZwwqT/xeKxgvqHZfrk9UarPtjtsud+x/4SVNDe7wMBnX7abvVGGrFjhW4I+QwWstaj/ZP7JVCRKs0aNWMrf1rjyjYK8h7VrklsVVHBL8WvWdG25721KnWIrUaVOAXTqka/zjwfcePcsaeHow1Z7y5Hpe2nZlvtdSrVMgGDHFGuGUTZcdplvCfoMFaCkRgdJpm0LNt80b15bkH46RuEDNDV1br0rh6T6pfQJ3LC2jTUr2yv0Gb/bvpojszEV3WvDMIwcufxy3xIUBBE5QUT+IyKviMglnbzeX0R+H7z+mIiML7WM5a+kKmgy6XUDIqGYRYlM1SlfDB2/f0H6iWcl8Qe3RDl+/PZLtuWUVH/OvM4DkgrNkOE1jBjd3qNo1Gg35fTVoJlB1cN8i+CVXcbXUBv8nLLzABuGUVpEpAq4FpgE7AucLiL7djjtLGCjqu4B/Bz4aWmlNJ/UgrBVUU1FVzHxgc8UVNWx/gzZ5QA2vPlMr/u6/rohzL8mxptrmhm9S+c/mc6S6keZUiX9v/2fu5JYu4UxO7RRK0pVXVWfzVc6UsaZTyrOD/XNN9pMQTUMvxwGvKKqrwGIyO+AqUB2ucapwGXB8z8AvxIR0UImn+2B8ldSVdE2v35NUlMDqRSV8HEWiigEke3xkTN5/M0LC5LapT4eY9fx/bqtaBQ2qX5fYeiIauJVKWpFaVb/vqE+GMhg9q8+yrcYkcEUVKOsefJJ3xIUgrHA21n77wCHd3WOqiZFZDMwFFhXEgmpAK0qHYEyiNrWFoWSApHDd9WpfEml85e7LypgfYGmRBoGbf1u7UvuJX3Vmt4NOX8YIjIdmA4wbty4oglklD3DRCRbm16gqgu8SZMnZa+kNupGHk78gY/2/5Q/IaqrIxMwFBViEfgsXnrk1wChy4IsfyHJrJkbi+6zaUSXpG5bnbn43PXc17CFz0ypI+XSWYa+pt5nI88mHzFrKqVNgVYOnDFjFYS4pgJFYwHAxIkTy9MSUO5MnBjJYjEdWKeqE7t5/V1g56z9nYJjnZ3zjohUAzsC6wsqZQ9UxF/ZFpr8pnaJQDGBKCJJf1u6uYUNb+Xvj+or+rxQY/oM3GqKcNBYLrz64C08pHfx0qO/prUpyX0Nbm65c3ETC4PguHxYrW+1U377IqVOgRZ1GhPpvEuiGkYveQKYICK7ikg/4DRgUYdzFgFfDp6fAvy1lP6oUCFKKkSjkovvtE+R2jwv9VdV9ydWlb/fm4/o81kzNxYkhdWsmRvzKkNaCGbN3MiR+67ge+euKfnYhaCtqY2Nb7g/N+vfdo/HTx4AwGem1LXL9BAWS+ZvZVENIyqoahI4D7gXWA7crqoviMgPReTk4LQbgKEi8gpwAbBdmqpiIyVWiguOiGTewNMUv7TcMErnMFypY5VqvGrgAABVDeUyLCLrgDc6HC62zCNov/SS7/Uco/3SYXY/5fIeuqLY8mf63w0YDGwEXgtei7HtvYxX1VC5pLLmqWeAMEsvlToPjABK9U8myvPb1t9r2HkKQETWAm+GbWf0CXZR1eG+hegtZa+klhIRebIHHw8bK4LjFYJiy1yKz6Tc30O5958PlToPVOpYPsYzjErG1loMwzAMwzCMyGFKqmEYhmEYhhE5TEkNRylzjFXqWD7GKwTFlrkUn0m5v4dy7z8fKnUeqNSxfIxnGBWL+aQahmEYhmEYkcMsqYZhGIZhGEbkMCXVKDkiYlVkDcMwDMPoFlNSQ2DKVcHY0bcAuZL5zov53ZdiDMPIFRGp8i1DobHflmGUJ/7LNJUJInICMEpEFqnqhhKM9xFgf2CFqt5V5LE+ALQBMVVdXuSxPglME5FzVLUcyhKNAFbjfittIhJT1UIXjSjFGASl71DV1kL3XaL+j8F9VtWqemu59d9binVdZPV/MPCCqrYUa4yssQ4DaoCkqj5W5LFKNneXct42jL6AWVJz5zzgLOATIhKq2kxYROR44EagHvijiJxYxLFOBG4DLgRuDCb0Yo31UWA+cH05KKgiMhn4k4gsAC4XkfGqmhaRgv1uSjFGMM5ngVuBBhE5SUQGl1n/x+Ku03HABSIyV0TGlEv/ecr0cRH5roj8SETiwXVRFItgMA/8DjiiGP13GOuTuJrgJwG3ich5IlJfxCFLMneXct42jL6CKam58wywBfg4MElEqkSkoJZoceyIq497sar+FDgH2CGwchQUEZkIXAN8HZgBzMO9Nym0khSwF/BTVb1XREaJyJEicnQRxuk1IrI78AvgO8CvgQTwexGZUCglshRjBOPsCfwIuBr4P9x3fWYwfjn0L8Ak4GeqOhv4MM5l5BIRGZl1TiT7z1Omk3C/zY24UrP3iUh/LUI6FhEZB8wGpqnqw0X67Wfmt/7A6cA3VfW7wGeAqcDZIlJXjHEp8txd6nnbMPoSpqTmzl04ReJPwNHA5cCPRaS2UAOoYzPwGLCLiBwK/BT4JHCXiFxcqLEChgE/VtXHgpvfa7ilqmItK7YCh4jIrsDdwGnAr0XkoiKM1VvWAQ+q6kPAP4CfAHfi5N2lQJ/PemBJkccAV4d+tao+qqq3Af8NfAA4SUQGFqD/ocXsP7g2nwL2EpGRqtoMTMMtzV+adU5v+l8K7FmM/sMiIqOBc3GK3DxV/TLwCrBHkYasAv4dKKhjgP8WkfkicoyIDC3UIMH81gIsB/YXkXpVXQacD5wIfLVQY3WgqHO3h3nbMPoMpqTmTgz4iqr+GVgJfAvoB6SKMNarwG7AHOCXqvpV3CQ+o5BLSKp6D/CXrEPLgC2qmgIQkVGFGivgCZxF4wvAr1X1m2x7X5MKPFZeiMiHReSLwHs4peWS4CakwFXAn4EvBdaYvKxrInKEiHwJ+BBwpIhcUOgxsgl8/t4SkVNFpFpVH8VZPCcFMuRFoEwB/At4vQj97ywi/UVkAPAoMBCn3AxQ1SacUnO4iJycZ/+HyLYgodeAeCH77wUJ4FpVfSi4BmK4PwKHZJ9UQIvnG0A/cS4bNwFvASuArwCHBWMV0pL8LO797B5cLy/g5tMLROSAAo6ToVRzd0nmbcPoS5iSmiOq+jjwoIhMwVkAf4GztHxaChQNm7kRqOr/BgrcfJxykZnI/4jzdyoYqroya7ca2Cm4MX4FuEFE6gp1gwrewybgZGB0YEl5AfgDMKAQY+SLiMTE+cXNB36AW4I8FfiCiMwCCCybjwNjVDWVj3UtUHgW4KwsJwGXAV8TkXMKNUYwzsGBwn14cOjvOH/DD4tIjar+E+eD+PV8lj6DPxVzRGQPQHCWyMML2P9JwBLglzg/v1acz+gs4CMiMlpVtwAPkIeyEfwB+ydws7iApCdwivC5wXvoVf+9QVXfC8YFSAfXxDJgcyD7CcHSf68t7VmK7oM4N4d3VPVaVb0ceBo4M5CpYJZkVV0CNALfBD4QzANLgXtw11JBKfbc7WPeNoy+gkX3dyC46Q4Cng+W/bKZAHwfOEVVG0TkFODRjOWxCONlFLoVgd/YVNwEmO9Y++GW+Jd3Ebi0BWdR+nYw7lmBRalgY6nqD0SkFReg8k0RacTdOP43n3EKRXDDbxSRm3FKyam4ZfKPAf8UkaSqXguMxi07DwQaw9y8g6XTc4EzVPV5EbkF95l/A6csgfMLznuMYJzJwBXAc0CdiNwF3Iy7dk/C+Tj+GlCgOXgM0/9hwHXAV1X1leDY/+GWbaf2pv/ghr8TcCUu4GU58GWc4n4E7vP5YnDuu8AZ5HfttOAUs4OBO0Tkc6o6T0RSOEt/rJf9h0ZEqjJzSWYuyPruk8E5pwA/A44DXu/tWBlFV0QW4r6/D4jIVFVdiLOmNgUKcV4R/13Nb6r6LRH5KTAdaBGRt4FP4VYS8n1PJZu7SzlvG0ZfxsqiZhHc3H+C8xVchfPXfD775iEiB6jqM0UeL5Z18/hv3L/wPYH/UtV/5znWJJyf1Gu41C/TVfXd4DXJ3AxF5FFc0MhnNc90VF2NFVgWMjfbjwG7A/sCC/Idq9CIyAU4BXoxzi/xBeAgnML9Om7581RVfS6PvncM+v0FcD/OOvbvYNsF93k8AXy0F2MchFNIv6Sqz4jI54CPqOo3A6X3dOBIYDxuyfVMVX065BhfBPZR1e+J82GciFuVWQR8CfgETtHMt/8qnDJ6OS6VjwbfyyycoirAocABwO9V9T9h+s8a5xyca8VsnDI9FxeoVIX7Lg7sTf8h5NhTVV8Knld1pjiJyPdxy+9rcX8e850HuhwrUKhOx/mlg1PgP6+qz+Y5VlfzW42qtgXnHBuMtyfOxSHf91WyubuU87Zh9HlU1Tannx2Js9ocFOzPBW7Mer26w/lS5PGqOpxf14uxjgFeAg4L9u8CPt7FON8F9i7FWJ19rr43nHJySfD8Qlz+2EuD/X7AsF72fwpuafxfwP8Ljh2Ps459GKgFhvfyujo7a38PnBVyfLCf+WN6IDCiF9/xtTiL6VM4q+fTwA1Z5+wftv9A1kNxyu3vcZHS2a9/B7f03z9PuffAKdQDgv0fAN8Knj8GpIGTSny9TQaagFuzjnX2OzktmC/2KsZYuGBJgutvR+BYYGwvr8Pu5rdYh/PzngdyGKtgc3cOYxVs3rbNNtvUfFI78FPdZvW5FBgiLmUKqpoUF2hxYrBfCBN0d+OlROSwwDcP3LJwvqwGZqjq44Ev3uHAeSIyn8DnTEQODawsP1HVF4s81iGyLTVLSf1exJVPAAAMo0lEQVT9cmALbql9GnA2LrXSYSJytqq2quq63nSuqn/ApcL5O07BQ1Xvwy1HDlXVZlVdG7ZfcWmgUOcL+sfgWBVuyXY1kEliPiE4b5mGyFWb6T8gkxbpTOA3qnqJqh4E7Jflv/tsyP4n4zIbzMZZUH8LnCMi38k67TbcsnfoYgFZ/V8F3CQiE3C+0K0isjMwHKeoflVEasL2nw8iEse5NJwfyPEb2Prbr846rx74K3C85m817nYs3ebfGlfVzar6oAYrLb2gu/ktHcw5k4PXezsPlHLuLtW8bRh9HlNSt/EY7iaWubn3xy3B7hAc2wm3NP1UCcfbC2d169XEqqrLVfXBYPcsYK6qfgoXKDJJRMbjrHjv5ztGyLGOxilPhVL2C4aqrgDexlnZLlAXQDIbtyxcqDE24pSOz4rI8eKCqXbB5XMMTXCjXyYivwv6XxssPaZwy9jVwXlfAmZLyET7Wf3fFvT/DC6F2FnAbiIyKDj1TlxWhLDyH4lTHr+sqh/FWawPw1mtZorI9wMfwGNwS9CDuuorh/6PxS3Tfgt4E+cP/CLO+vwhnDV1ZNj3kA+qmgC+hiuCcBFQm6U8ZtxiDgzO2aCqbxd5rAOA06UwqZlymd/2Bp4MZOjNPFDKubtk87ZhGOaT2imBFaMWWKiqxwU+eAcBl6lqrxU53+N1GHsJMEsDP7VKGas3BJa1EeoijpEilKMMFLszgc/iFMmLNQ9/ucBC9kfcjfNI3NJmJrCoCvdH9FZcZPiBOB/RnP3jOum/n6qeEbw2Dee+sAQXZHYq8OmwlvhAidxTVW8K9ocDN6nqSSKyGy7gpRlnlf+KhvTV7aL/G1V1SmBda1TVv4XpsxiIC6xbgEsD90UR2R9n+f57GKt0BMcq2fxWqWMZRl/FlNRuEJGbcHn1jsdFMucVQBCV8US2BUgF+5/F+aBO1vapqMpqrGLR8T0UaYyBuN9haAtkVh9jcBbMWlzUfXNGUQ1e/xMugOPT+SwXd9J/m6qeHrz2YZyCejgu920+/VfhlpnfC56PxgWYnaiqK0VkF+Dd4JzNBez/E6q6TkR2wClrbWH7LjTiynZehftDEAOOLtbvpZRjBePdRInm00odyzD6GqakdoKICC4qfXnweJyqvlxB4/XHpfG5ABe9+3wljGV0aiGbgEtK/5swFtQc+m9V1dMDC9z6AvgvZvrvzDr1EeB8dXlLC9X/IlX9mIh8AefqclGwJO4dEfkvXBq4T4S1GkdxrFLOb5U6lmH0VUxJ7QZxCe2fUJeQuWLGCwJDPgG8mm8gRhTHMhxZFrKjgkMfUdXVRej/SFy6pmNU9Z1C9R+McRPbrFOhl/h9958vgb/w7cCFJVi5KdlYwXhfoUTzaaWOZRh9DVNSu6EUy70+xzMql2JbyIrVf7GtU+Vg/RKRWt0+QXwljFWy+a1SxzKMvoZVnOqGUk88NtEZhSCwkJ2IS1lUDAW1aP0Hv4FWEbkCZ50qqAJZ7P4LQamURg9jlWx+q9SxDKOvYZZUw6hAim0hK0H/RbVOmfXLMAwj+piSahiGYRiGYUQOS+ZvGIZhGIZhRA5TUg3DMAzDMIzIYUqqYRiGYRiGETlMSTUAEJE3gvyXhmEYPSIi40XEW3EOEfmKiPzK1/iGYRQfU1KNkhCUozQMwygLgupkhmF4xJTUHAmsBstF5HoReUFE7hORASLykIhMDM4ZJiJvBM+/IiJ/EpH7AyvleSJygYg8LSL/EpEh3Yy1h4j8RUSeEZGnRGR3cVwlIs+LyHMi8vng3GMCGf4gIi+KyG+Dc08QkTuy+jxGRBpyfK9/EpGlwfucHhz7moj8T9Y50/5/e+cf62VVx/HXmzRUIAglcpiC64cFCIaphDKnxayw1BhuKQTNUkeoNe3XXNLSRUPHFIi5YVSYNpRZSKYolhbxS5Rfw2VarOzXconzBpTAuz/O5+4+Xe793u8d13sv935e2xmf55zznPM593n4PJ/zec7zPZLmh3ylpI2Stki6u9EhldQg6Q5JW4HxkuZK2ilpm6Tb23sNkt6FpIau1uFIoJNt07iwS1uBWZX8t4R92hT/v6+O/BbtU5QdYg8kDZG0ItrZJGlCi4ocqtfFkjbEGJ6QNFRSH0m/lzQk6vSR9GL00WI/kuZIWiZpLbBM0siKbdumss1wkiSdhe1MdSRgOLAfGBvHyyl70v8KODPyTgB2hTwDeBEYAAwBXgOuibL5lL3IW+trA3BpyMcAxwGfBh6nbEM5FPgTcCJwfrR9EmXSsY6yF/lRUadftLMYuLJGn7uAE0IeHP8eC+wAjgf6Ay8BR0fZb4HRwPuBhyv53wOmh2xgasjHA7+j6WfPBnX1Nc3UvRPQ0NU6HAmpk23TNmBiyPOAHSF/Abg55L7AM8CIGvapRXsA3AecG/LJwPM1dJkBLAz57ZW2rgLuCPmWxvFQtsBdUasfYA6wGTg2jhcAV4T81sb8TJkydU7KSGr7+KPtLSFvpjwcavFL26/b/ifFUD8c+dtbO1fSAGCY7Yeg7Ahjew/FsN9v+4DLPuxPAR+K0zbaftn2QWALMNz2fuBR4GKV11afAH5W5zivi0jJeuBdwHtsNwBPApMlnUZxSrcDFwLjgE2StsTxqdHOAWBFyK8B+4B7JF0G7KlTlx5LJ0fAOj06H5H026LP9ZKGVsb9ZESm1kg6OfJHSFoXutzarK2bKlG6b0VeP0k/j/Z3NOrfS+kM2zSI4kw+HVnLKsWTgOlhAzZQnNDGqOMh9onW7cFHgIXRzkrgbZL6tzEWKE7wY5K2AzcBIyP/+8D0kD8HLK2jn5W294a8DviGpK8Cp1TykyTpBNJJbR//qcgHKNHK/TT9HY+pUf9g5fggHbslbUt6AfwEmApcADxj+/W2GpJ0PsWAj7c9BniOpnEtoUQvZtJk7AX80PbYSO+zPSfK9tk+ABBO81nAg8BkigOdlAf5Itsjgd2UiHktRgGXUSYotwF7bJ9BeZhOr3Hej6OfMcCHgb9FO2OBMZRrPk/SiVH/DOAG4AOUSccE4AngbEn9os7llHusNfoB66PPp4HPR/4Cyj1zeuh1V+TfCSy2PTr0A0DSJMrf6azQd5ykicBFwF9tj7E9it59T3W1bRIwu2IHRthe3ZpuNexBH+CcSjvDYoLcFgsoUdXRwNXEeG3/GfiHpAuiv1/U0c+/Gxu1fR/wSWAv8Ei0kyRJJ5FO6uGzixJJBJhyuI2FI/mypEsAJPWVdBzwa+BylbVfQ4CJwMY2mnsK+CDFOajlTFQZCLxqe09ETM+p6LaBEln9DHB/ZK8Bpkh6R+g7WNIpzRuNKMVA248AX6I4RknPjs7/F2iMtFbHNp7yuhVKNO7ckCfQdF81j9JNokyYngVOozit24GPSvqupPNsv1ZDl97ILjrWNu0GdktqvF5XVIofA66VdDSApPdWJjOHUMMerAZmV+qNrVO9gcBfQv5ss7IlwL3AA42T5nr7kXQq8Afbd1Hu9dPr1CdJkg4gndTD53aKcX6Osu6rI5hGeeW+jbL2853AQ5T1YFspr92/YvvvtRoJg7wK+BhNzkJbPAocJel5YC7llX+V5cBa269GHzuBm4HVoe/jlLWyzRkArIo6vwG+XKc+PZ2ujoC1Ry9oX3T+DduN+y5X26hFS/s0C/hOJer1btv32H6BMgnbDtwq6Zt1tN+beDNs00xgUbwmVyV/CbATeFblZ6nupvb1bs0eXAecGcs6dgLX1KnXHOABSZuBV5qVraSsqV9ayau3n6nAjhjvKOBHdeqTJEkHoKZnSJK0TaxBnG97TVfrcqQjaTiwKl5VI+lGysP0JGCz7cWSbqB8+DFc0gzKhzBfjPq74viV5mUt9LUemGv7p5L6Uj7Au4jyavTjwGDKxy5nUyKVN9qeHOcupDikP1D55YaXgE2UyNTyGuNrsN0/5CnAZNszJK2Mc5eF3p+yfWnkL7d9r6RrgXm2+8fr/m8DF9pukDQMeIPiBP3L9j5Jk4GrbF/SjkuQ9AJU1nfPt31eV+uSJEn7yEhqUheSBkl6AdibDuqbTk+LzjdnNjAzdJkGXB/51wOz4uOXYZU+V1OWB6yLsgcpkbjRwMaIct0C/N/HVkki6WuUjze/3tW6JEnSfjKS2oVIWkRZh1flTttLW6rfQX1uoPxETJVp8aV+kiRJl9imGrrMpGki08ha27Naqp8kSc8hndQkSZIkSZKk25HbviVJDyKj80mSJElPISOpSZIkSZIkSbcjP5xKkiRJkiRJuh3ppCZJkiRJkiTdjnRSkyRJkiRJkm5HOqlJkiRJkiRJtyOd1CRJkiRJkqTb8T9sTrmv46bI3AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 648x648 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7AWGjNCydMG"
      },
      "source": [
        "# Caso necessário uma superfície de contorno ampliada, comparando a exploração\r\n",
        "# de dois hiperparâmetros específicos com a função de custo: \r\n",
        "\r\n",
        "skopt.plots.plot_objective_2D(result,\r\n",
        "                              dimension_identifier1='num_conv_nodes',\r\n",
        "                              dimension_identifier2='num_dense_nodes',\r\n",
        "                              levels=100)\r\n",
        "#plt.savefig('/content/drive/My Drive/MESTRADO - UFES/objective_2D B.png', transparent=True, bbox_inches='tight', dpi=600)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDTkH37zTVC9"
      },
      "source": [
        "The history of the optimization process is shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzz-lNXuSoD0",
        "outputId": "92c96e4c-838b-4667-9bee-1668b2a231d1"
      },
      "source": [
        "print('Hyperparameters: num_conv_layers / num_conv_nodes / num_dense_layers / num_dense_nodes')\r\n",
        "\r\n",
        "for fitness, x in sorted(zip(result.func_vals, result.x_iters)):\r\n",
        "    print('The fitness was:', fitness, 'with hyperparameters:', x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hyperparameters: num_conv_layers / num_conv_nodes / num_dense_layers / num_dense_nodes\n",
            "The fitness was: 0.024427924305200577 with hyperparameters: [4, 64, 1, 180]\n",
            "The fitness was: 0.028512557968497276 with hyperparameters: [4, 211, 3, 256]\n",
            "The fitness was: 0.03179307281970978 with hyperparameters: [4, 250, 1, 256]\n",
            "The fitness was: 0.03341296315193176 with hyperparameters: [4, 214, 1, 256]\n",
            "The fitness was: 0.034644030034542084 with hyperparameters: [4, 126, 2, 256]\n",
            "The fitness was: 0.03741927444934845 with hyperparameters: [4, 83, 1, 36]\n",
            "The fitness was: 0.0381108894944191 with hyperparameters: [4, 124, 2, 256]\n",
            "The fitness was: 0.03869060054421425 with hyperparameters: [4, 83, 1, 32]\n",
            "The fitness was: 0.03909004107117653 with hyperparameters: [4, 49, 2, 256]\n",
            "The fitness was: 0.039211075752973557 with hyperparameters: [4, 210, 3, 181]\n",
            "The fitness was: 0.039361219853162766 with hyperparameters: [4, 173, 1, 21]\n",
            "The fitness was: 0.04057883098721504 with hyperparameters: [4, 83, 1, 28]\n",
            "The fitness was: 0.040726955980062485 with hyperparameters: [4, 34, 3, 182]\n",
            "The fitness was: 0.04131153225898743 with hyperparameters: [4, 82, 1, 35]\n",
            "The fitness was: 0.043962206691503525 with hyperparameters: [4, 195, 3, 32]\n",
            "The fitness was: 0.04641636461019516 with hyperparameters: [4, 128, 3, 256]\n",
            "The fitness was: 0.04657062515616417 with hyperparameters: [4, 127, 3, 255]\n",
            "The fitness was: 0.046656668186187744 with hyperparameters: [4, 256, 1, 182]\n",
            "The fitness was: 0.04700182005763054 with hyperparameters: [4, 172, 1, 20]\n",
            "The fitness was: 0.04745558276772499 with hyperparameters: [4, 171, 1, 21]\n",
            "The fitness was: 0.04757019132375717 with hyperparameters: [4, 231, 3, 183]\n",
            "The fitness was: 0.05433652177453041 with hyperparameters: [4, 248, 3, 182]\n",
            "The fitness was: 0.055783968418836594 with hyperparameters: [4, 42, 2, 23]\n",
            "The fitness was: 0.05702213570475578 with hyperparameters: [4, 50, 2, 255]\n",
            "The fitness was: 0.05727885663509369 with hyperparameters: [4, 41, 2, 21]\n",
            "The fitness was: 0.060367174446582794 with hyperparameters: [4, 106, 1, 35]\n",
            "The fitness was: 0.06050872802734375 with hyperparameters: [4, 253, 3, 189]\n",
            "The fitness was: 0.062398843467235565 with hyperparameters: [4, 40, 2, 23]\n",
            "The fitness was: 0.06292971223592758 with hyperparameters: [4, 40, 1, 22]\n",
            "The fitness was: 0.06338714808225632 with hyperparameters: [4, 84, 1, 35]\n",
            "The fitness was: 0.07029452174901962 with hyperparameters: [4, 42, 1, 22]\n",
            "The fitness was: 0.07095915079116821 with hyperparameters: [4, 195, 3, 34]\n",
            "The fitness was: 0.07547266781330109 with hyperparameters: [4, 42, 2, 47]\n",
            "The fitness was: 0.07616687566041946 with hyperparameters: [4, 34, 3, 182]\n",
            "The fitness was: 0.07876452803611755 with hyperparameters: [4, 126, 3, 256]\n",
            "The fitness was: 0.10470236837863922 with hyperparameters: [4, 193, 3, 33]\n",
            "The fitness was: 0.10948821902275085 with hyperparameters: [4, 41, 1, 20]\n",
            "The fitness was: 0.12212402373552322 with hyperparameters: [3, 56, 2, 182]\n",
            "The fitness was: 0.12387489527463913 with hyperparameters: [3, 118, 2, 180]\n",
            "The fitness was: 0.13439054787158966 with hyperparameters: [4, 61, 1, 183]\n",
            "The fitness was: 0.1351754516363144 with hyperparameters: [4, 253, 1, 181]\n",
            "The fitness was: 0.14685386419296265 with hyperparameters: [3, 133, 3, 256]\n",
            "The fitness was: 0.14994660019874573 with hyperparameters: [2, 152, 1, 181]\n",
            "The fitness was: 0.15582267940044403 with hyperparameters: [2, 59, 3, 217]\n",
            "The fitness was: 0.16645298898220062 with hyperparameters: [2, 40, 3, 179]\n",
            "The fitness was: 0.19519522786140442 with hyperparameters: [4, 249, 1, 256]\n",
            "The fitness was: 0.24142195284366608 with hyperparameters: [2, 145, 2, 198]\n",
            "The fitness was: 0.330492228269577 with hyperparameters: [3, 213, 2, 167]\n",
            "The fitness was: 0.4520321488380432 with hyperparameters: [2, 42, 1, 181]\n",
            "The fitness was: 0.5241356492042542 with hyperparameters: [4, 182, 3, 179]\n",
            "The fitness was: 0.5305382013320923 with hyperparameters: [4, 110, 1, 174]\n",
            "The fitness was: 0.535614013671875 with hyperparameters: [4, 127, 2, 254]\n",
            "The fitness was: 0.5371974110603333 with hyperparameters: [4, 198, 3, 35]\n",
            "The fitness was: 0.5383220911026001 with hyperparameters: [4, 110, 3, 255]\n",
            "The fitness was: 0.5390374660491943 with hyperparameters: [4, 80, 1, 256]\n",
            "The fitness was: 0.5432280898094177 with hyperparameters: [4, 83, 1, 222]\n",
            "The fitness was: 0.5434507131576538 with hyperparameters: [4, 96, 3, 33]\n",
            "The fitness was: 0.5446630716323853 with hyperparameters: [4, 86, 1, 35]\n",
            "The fitness was: 0.5483015775680542 with hyperparameters: [4, 126, 2, 23]\n",
            "The fitness was: 0.5515177249908447 with hyperparameters: [4, 41, 1, 256]\n",
            "The fitness was: 0.5595468878746033 with hyperparameters: [4, 86, 2, 36]\n",
            "The fitness was: 0.561312735080719 with hyperparameters: [4, 85, 1, 215]\n",
            "The fitness was: 0.5614110827445984 with hyperparameters: [4, 231, 2, 231]\n",
            "The fitness was: 0.5682880282402039 with hyperparameters: [3, 102, 2, 152]\n",
            "The fitness was: 0.5789933204650879 with hyperparameters: [4, 43, 1, 37]\n",
            "The fitness was: 0.5809475779533386 with hyperparameters: [3, 157, 1, 150]\n",
            "The fitness was: 0.5837708711624146 with hyperparameters: [4, 128, 3, 100]\n",
            "The fitness was: 0.5937812328338623 with hyperparameters: [3, 47, 1, 216]\n",
            "The fitness was: 0.5953404903411865 with hyperparameters: [3, 214, 2, 173]\n",
            "The fitness was: 0.6048155426979065 with hyperparameters: [4, 163, 1, 5]\n",
            "The fitness was: 0.6195657253265381 with hyperparameters: [3, 146, 3, 82]\n",
            "The fitness was: 0.6234989762306213 with hyperparameters: [4, 177, 1, 21]\n",
            "The fitness was: 0.636355459690094 with hyperparameters: [3, 83, 1, 90]\n",
            "The fitness was: 0.6365277171134949 with hyperparameters: [4, 218, 1, 160]\n",
            "The fitness was: 0.638695240020752 with hyperparameters: [3, 121, 1, 256]\n",
            "The fitness was: 0.6408593654632568 with hyperparameters: [4, 226, 1, 28]\n",
            "The fitness was: 0.6481099724769592 with hyperparameters: [4, 112, 3, 5]\n",
            "The fitness was: 0.6581435203552246 with hyperparameters: [2, 235, 3, 241]\n",
            "The fitness was: 0.6720766425132751 with hyperparameters: [2, 172, 1, 185]\n",
            "The fitness was: 0.6730738282203674 with hyperparameters: [4, 172, 1, 26]\n",
            "The fitness was: 0.6754029393196106 with hyperparameters: [2, 227, 3, 247]\n",
            "The fitness was: 0.6871072053909302 with hyperparameters: [2, 86, 1, 35]\n",
            "The fitness was: 0.7080899477005005 with hyperparameters: [3, 206, 2, 30]\n",
            "The fitness was: 0.71125328540802 with hyperparameters: [2, 171, 3, 256]\n",
            "The fitness was: 0.7113502621650696 with hyperparameters: [2, 242, 3, 111]\n",
            "The fitness was: 0.7200783491134644 with hyperparameters: [2, 244, 3, 7]\n",
            "The fitness was: 0.7345913052558899 with hyperparameters: [2, 251, 3, 9]\n",
            "The fitness was: 0.7453668117523193 with hyperparameters: [2, 82, 1, 6]\n",
            "The fitness was: 0.7595155835151672 with hyperparameters: [2, 256, 2, 10]\n",
            "The fitness was: 0.7753592729568481 with hyperparameters: [4, 32, 1, 9]\n",
            "The fitness was: 0.7996311187744141 with hyperparameters: [4, 70, 1, 34]\n",
            "The fitness was: 0.8137226104736328 with hyperparameters: [4, 39, 2, 21]\n",
            "The fitness was: 0.8148638606071472 with hyperparameters: [2, 51, 1, 159]\n",
            "The fitness was: 0.9825088977813721 with hyperparameters: [3, 210, 1, 36]\n",
            "The fitness was: 1.0134482383728027 with hyperparameters: [4, 42, 2, 21]\n",
            "The fitness was: 1.0296266078948975 with hyperparameters: [4, 192, 3, 35]\n",
            "The fitness was: 1.5566989183425903 with hyperparameters: [3, 125, 3, 235]\n",
            "The fitness was: 1.8310242891311646 with hyperparameters: [4, 57, 1, 18]\n",
            "The fitness was: 2.079444646835327 with hyperparameters: [4, 170, 1, 20]\n",
            "The fitness was: 50.07863235473633 with hyperparameters: [4, 246, 1, 182]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9DIzm_-Oboq"
      },
      "source": [
        "The best hyperparameter set (according to the optimizer) it's shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpETQIxZVVZb",
        "outputId": "821d61ee-7f8b-444b-bd15-fab8c6cc8a88"
      },
      "source": [
        "# Resumo da busca, segundo o otimizador:\r\n",
        "print('Best fitness: %.4f' % (result.fun))\r\n",
        "print('Best hyperparameters: %s' % (result.x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best fitness: 0.0244\n",
            "Best hyperparameters: [4, 64, 1, 180]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCUX7BCnO4Wo"
      },
      "source": [
        "The amount of time spent in the 2nd stage of optimization it's shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oa3cCRDXGVGA",
        "outputId": "fee83bf1-837c-4614-82be-75884d5b6986"
      },
      "source": [
        "# Atributo \"iter_time\" evidencia o tempo (s) de cada iteração:\r\n",
        "times = TimerCallback.iter_time\r\n",
        "print('The time of each iteration is:', times)\r\n",
        "print('\\nThe total time spent in this optimization was:', sum(times), 'seconds.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The time of each iteration is: [676.5342819690704, 494.0890290737152, 878.5202553272247, 526.1753678321838, 783.6225616931915, 742.007040977478, 273.4700028896332, 558.6790137290955, 231.6851327419281, 867.1138958930969, 686.0034372806549, 292.0412037372589, 162.1615982055664, 655.230880022049, 827.4893305301666, 305.8786633014679, 155.88616752624512, 543.6607713699341, 537.8061833381653, 472.6346354484558, 165.449702501297, 271.90510153770447, 265.6947636604309, 411.49338722229004, 436.23884987831116, 587.7107408046722, 552.587185382843, 628.5607993602753, 285.0674810409546, 451.81813883781433, 513.0009491443634, 219.0943796634674, 422.60375213623047, 418.37366557121277, 670.8251116275787, 135.1550829410553, 460.11988973617554, 530.3298738002777, 781.576060295105, 456.8771936893463, 628.748612165451, 537.5552096366882, 650.9463455677032, 841.7163505554199, 590.8821816444397, 654.3348021507263, 650.5910487174988, 518.7363619804382, 418.5367262363434, 461.9452564716339, 531.3175382614136, 629.296101808548, 573.2263886928558, 397.0013310909271, 954.3805487155914, 629.4787282943726, 412.6168351173401, 469.2933728694916, 949.5299127101898, 380.9030327796936, 884.9458863735199, 910.7809596061707, 525.6619787216187, 443.77962827682495, 321.42621779441833, 472.1598563194275, 645.6971125602722, 438.19660902023315, 504.9886441230774, 182.4055576324463, 619.2531261444092, 360.8950080871582, 604.4009630680084, 811.3469738960266, 521.4159533977509, 938.0571367740631, 686.0513923168182, 834.751208782196, 856.9306478500366, 607.1557297706604, 955.6189739704132, 525.5023488998413, 908.221652507782, 184.41246438026428, 651.495477437973, 552.2768092155457, 537.2672822475433, 698.8687379360199, 363.3939731121063, 762.9798078536987, 248.72113132476807, 306.4561128616333, 214.23974871635437, 654.578432559967, 1060.4156413078308, 668.6416749954224, 439.74866771698, 902.4489810466766, 562.533367395401, 209.69921875]\n",
            "\n",
            "The total time spent in this optimization was: 54794.02934193611 seconds.\n",
            "The time of each iteration is: [676.5342819690704, 494.0890290737152, 878.5202553272247, 526.1753678321838, 783.6225616931915, 742.007040977478, 273.4700028896332, 558.6790137290955, 231.6851327419281, 867.1138958930969, 686.0034372806549, 292.0412037372589, 162.1615982055664, 655.230880022049, 827.4893305301666, 305.8786633014679, 155.88616752624512, 543.6607713699341, 537.8061833381653, 472.6346354484558, 165.449702501297, 271.90510153770447, 265.6947636604309, 411.49338722229004, 436.23884987831116, 587.7107408046722, 552.587185382843, 628.5607993602753, 285.0674810409546, 451.81813883781433, 513.0009491443634, 219.0943796634674, 422.60375213623047, 418.37366557121277, 670.8251116275787, 135.1550829410553, 460.11988973617554, 530.3298738002777, 781.576060295105, 456.8771936893463, 628.748612165451, 537.5552096366882, 650.9463455677032, 841.7163505554199, 590.8821816444397, 654.3348021507263, 650.5910487174988, 518.7363619804382, 418.5367262363434, 461.9452564716339, 531.3175382614136, 629.296101808548, 573.2263886928558, 397.0013310909271, 954.3805487155914, 629.4787282943726, 412.6168351173401, 469.2933728694916, 949.5299127101898, 380.9030327796936, 884.9458863735199, 910.7809596061707, 525.6619787216187, 443.77962827682495, 321.42621779441833, 472.1598563194275, 645.6971125602722, 438.19660902023315, 504.9886441230774, 182.4055576324463, 619.2531261444092, 360.8950080871582, 604.4009630680084, 811.3469738960266, 521.4159533977509, 938.0571367740631, 686.0513923168182, 834.751208782196, 856.9306478500366, 607.1557297706604, 955.6189739704132, 525.5023488998413, 908.221652507782, 184.41246438026428, 651.495477437973, 552.2768092155457, 537.2672822475433, 698.8687379360199, 363.3939731121063, 762.9798078536987, 248.72113132476807, 306.4561128616333, 214.23974871635437, 654.578432559967, 1060.4156413078308, 668.6416749954224, 439.74866771698, 902.4489810466766, 562.533367395401, 209.69921875]\n",
            "\n",
            "The total time spent in this optimization was: 54794.02934193611 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oveqB1fDhyX3"
      },
      "source": [
        "# 7.Independent test with the hyperparameters found set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOAKiXp-m0QR"
      },
      "source": [
        "**This section is dedicated to the execution and analysis (duplicate test) of the solution (set of hyperparameters) found by the 2nd optimizer.**\r\n",
        "\r\n",
        "* Observe that the experiment is based on a stochastic process, which hinders \"perfect\" reproducibility.\r\n",
        "* Accuracy/Loss plots and confusion matrix were implemented to verify the performance of the model found."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT4CoU1ujbsl"
      },
      "source": [
        "# EarlyStopping (O modelo pára o treinamento caso não perceba melhoria):\r\n",
        "earlystopping = EarlyStopping(monitor=\"val_accuracy\", min_delta=0, patience=100,\r\n",
        "                              verbose=1, mode=\"auto\", baseline=None,\r\n",
        "                              restore_best_weights=True)\r\n",
        "\r\n",
        "# TensorDash (acompanhamento das métricas do modelo pelo app Android):\r\n",
        "histories = Tensordash(ModelName = 'AutoML', email = 'viniciuswv@gmail.com',\r\n",
        "                       password = 'admin1')\r\n",
        "\r\n",
        "# Batch size (número de exemplos de treinamento usados em uma iteração/época):\r\n",
        "batch_size = 32\r\n",
        "\r\n",
        "# Épocas (quantidade de ciclos de treinamento da rede neural):\r\n",
        "epochs = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmM_7fFOnQ23"
      },
      "source": [
        "**Function that creates the final architecture (like optimized on 2nd stage):**\r\n",
        "* (Before running the function, adjust the number of convolutional and dense layers, as well as their respective neurons)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJCDxyZ2jn4o"
      },
      "source": [
        "def final_model():\r\n",
        "  # Criação das camadas com apoio do recurso 'keras.sequential':\r\n",
        "  model = Sequential() # Empilha linearmente as camadas da rede, conforme abaixo:\r\n",
        "  # Camada de alimentação do dataset:\r\n",
        "  model.add(InputLayer(input_shape=(1666,1))) # O tensor de entrada tem o shape (1666, 1).\r\n",
        " \r\n",
        "  # 1ª camada convolucional:\r\n",
        "  model.add(Conv1D(64, kernel_size=4, strides=1, activation='relu', padding='same'))\r\n",
        "  # 1ª camada MaxPooling:\r\n",
        "  model.add(MaxPooling1D(pool_size=3, strides=None, padding='valid'))\r\n",
        "  # 1ª camada de normalização do batch:\r\n",
        "  model.add(BatchNormalization())\r\n",
        "  \r\n",
        "  # 2ª camada convolucional:\r\n",
        "  model.add(Conv1D(64, kernel_size=4, strides=1, activation='relu', padding='same'))\r\n",
        "  # 2ª camada MaxPooling:\r\n",
        "  model.add(MaxPooling1D(pool_size=3, strides=None, padding='valid'))\r\n",
        "  # 2ª camada de normalização do batch:\r\n",
        "  model.add(BatchNormalization())\r\n",
        " \r\n",
        "  # 3ª camada convolucional:\r\n",
        "  model.add(Conv1D(64, kernel_size=4, strides=1, activation='relu', padding='same'))\r\n",
        "  # 3ª camada de MaxPooling:\r\n",
        "  model.add(MaxPooling1D(pool_size=3, strides=None, padding='valid'))\r\n",
        "  # 3ª camada de normalização do batch:\r\n",
        "  model.add(BatchNormalization())\r\n",
        "  \r\n",
        "  # 4ª camada convolucional:\r\n",
        "  model.add(Conv1D(64, kernel_size=4, strides=1, activation='relu', padding='same'))\r\n",
        "  # 4ª camada de MaxPooling:\r\n",
        "  model.add(MaxPooling1D(pool_size=3, strides=None, padding='valid'))\r\n",
        "  # 4ª camada de normalização do batch:\r\n",
        "  model.add(BatchNormalization())\r\n",
        "  \r\n",
        "  # Camada de achatamento (flatten):\r\n",
        "  model.add(Flatten())\r\n",
        "  \r\n",
        "  # 1ª camada densa (fully connected):\r\n",
        "  #model.add(Dense(73, activation='relu'))\r\n",
        "  # 2ª camada densa (fully connected):\r\n",
        "  #model.add(Dense(115, activation='relu'))\r\n",
        "  # 3ª camada densa (fully connected):\r\n",
        "  model.add(Dense(180, activation='relu'))\r\n",
        "  # 4ª camada densa (fully connected):\r\n",
        "  model.add(Dense(16, activation='softmax'))\r\n",
        "\r\n",
        "  model.summary()\r\n",
        "\r\n",
        " \r\n",
        "  # Otimizador da rede:\r\n",
        "  optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, name=\"SGD\")\r\n",
        "\r\n",
        "\r\n",
        "  # Compilação do modelo neural:\r\n",
        "  model.compile(loss='categorical_crossentropy',\r\n",
        "                optimizer= optimizer,\r\n",
        "                metrics=['accuracy'])\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wyBkmXqubUl",
        "outputId": "43cb0a2a-01fd-40cd-dde5-3534094d4f0d"
      },
      "source": [
        "final_model()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d (Conv1D)              (None, 1666, 64)          320       \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 555, 64)           0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 555, 64)           256       \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 555, 64)           16448     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 185, 64)           0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 185, 64)           256       \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 185, 64)           16448     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 61, 64)            0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 61, 64)            256       \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 61, 64)            16448     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 20, 64)            0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 20, 64)            256       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1280)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 180)               230580    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                2896      \n",
            "=================================================================\n",
            "Total params: 284,164\n",
            "Trainable params: 283,652\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.engine.sequential.Sequential at 0x7fc9f00292e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIc9fePVnghI"
      },
      "source": [
        "Passing the necessary functions to subset data, train and evaluate the final model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaXZwntJjA2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87132d77-c006-4c66-ed14-2e9a11cca968"
      },
      "source": [
        "# Subset - lembrar de usar o mesmo subset usado no 1º estágio de otimização:\r\n",
        "X_train3, Y_train3, X_val3, Y_val3, X_test3, Y_test3, n_cases3 = subsetB(400)\r\n",
        "\r\n",
        "# Quando for rodar uma segunda vez, habilite o código abaixo:\r\n",
        "#del model\r\n",
        "#del final_model\r\n",
        "K.clear_session()\r\n",
        "tf.keras.backend.clear_session()\r\n",
        "tf.compat.v1.reset_default_graph()\r\n",
        "\r\n",
        "# Instanciando a nova CNN:\r\n",
        "cnn = final_model()\r\n",
        "\r\n",
        "# Treino e validação:\r\n",
        "final_model, history3, acc_max3, train_time3 = fit_model(cnn,\r\n",
        "                                                         X_train3,\r\n",
        "                                                         Y_train3,\r\n",
        "                                                         X_val3,\r\n",
        "                                                         Y_val3,\r\n",
        "                                                         batch_size,\r\n",
        "                                                         epochs)\r\n",
        "\r\n",
        "# Teste:\r\n",
        "test_accuracy = test_model(X_test3, Y_test3, final_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d (Conv1D)              (None, 1666, 64)          320       \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 555, 64)           0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 555, 64)           256       \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 555, 64)           16448     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 185, 64)           0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 185, 64)           256       \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 185, 64)           16448     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 61, 64)            0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 61, 64)            256       \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 61, 64)            16448     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 20, 64)            0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 20, 64)            256       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1280)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 180)               230580    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                2896      \n",
            "=================================================================\n",
            "Total params: 284,164\n",
            "Trainable params: 283,652\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n",
            "Epoch 1/500\n",
            "128/128 [==============================] - 5s 8ms/step - loss: 1.5511 - accuracy: 0.4621 - val_loss: 2.8039 - val_accuracy: 0.0762\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.07617, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 2/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.8483 - accuracy: 0.6440 - val_loss: 2.7312 - val_accuracy: 0.2109\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.07617 to 0.21094, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 3/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.7103 - accuracy: 0.6989 - val_loss: 2.6337 - val_accuracy: 0.2412\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.21094 to 0.24121, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 4/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.7090 - accuracy: 0.7095 - val_loss: 1.5648 - val_accuracy: 0.3652\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.24121 to 0.36523, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 5/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5581 - accuracy: 0.7775 - val_loss: 1.5942 - val_accuracy: 0.3818\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.36523 to 0.38184, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 6/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5462 - accuracy: 0.7976 - val_loss: 1.3804 - val_accuracy: 0.4092\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.38184 to 0.40918, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 7/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.7102 - accuracy: 0.7023 - val_loss: 2.2473 - val_accuracy: 0.3369\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.40918\n",
            "Epoch 8/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6964 - accuracy: 0.7103 - val_loss: 1.0137 - val_accuracy: 0.5527\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.40918 to 0.55273, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 9/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6681 - accuracy: 0.7351 - val_loss: 1.2547 - val_accuracy: 0.4297\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.55273\n",
            "Epoch 10/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6933 - accuracy: 0.6955 - val_loss: 1.5388 - val_accuracy: 0.5117\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.55273\n",
            "Epoch 11/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6339 - accuracy: 0.7354 - val_loss: 3.1296 - val_accuracy: 0.3750\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.55273\n",
            "Epoch 12/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.7230 - accuracy: 0.6838 - val_loss: 1.1246 - val_accuracy: 0.5557\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.55273 to 0.55566, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 13/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6805 - accuracy: 0.7152 - val_loss: 0.8809 - val_accuracy: 0.6758\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.55566 to 0.67578, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 14/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5717 - accuracy: 0.7711 - val_loss: 1.6943 - val_accuracy: 0.4258\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.67578\n",
            "Epoch 15/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6828 - accuracy: 0.6995 - val_loss: 1.3519 - val_accuracy: 0.5283\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.67578\n",
            "Epoch 16/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6641 - accuracy: 0.7143 - val_loss: 2.3137 - val_accuracy: 0.4316\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.67578\n",
            "Epoch 17/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.5417 - accuracy: 0.7810 - val_loss: 1.6888 - val_accuracy: 0.4053\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.67578\n",
            "Epoch 18/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.3149 - accuracy: 0.8808 - val_loss: 0.6368 - val_accuracy: 0.7617\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.67578 to 0.76172, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 19/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.2992 - accuracy: 0.8861 - val_loss: 0.9204 - val_accuracy: 0.6299\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.76172\n",
            "Epoch 20/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.2575 - accuracy: 0.9106 - val_loss: 0.2711 - val_accuracy: 0.9199\n",
            "\n",
            "Epoch 00020: val_accuracy improved from 0.76172 to 0.91992, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 21/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.2691 - accuracy: 0.8925 - val_loss: 0.6380 - val_accuracy: 0.7051\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.91992\n",
            "Epoch 22/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.2213 - accuracy: 0.9127 - val_loss: 0.7124 - val_accuracy: 0.7480\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.91992\n",
            "Epoch 23/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.2225 - accuracy: 0.9118 - val_loss: 0.3186 - val_accuracy: 0.8799\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.91992\n",
            "Epoch 24/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4593 - accuracy: 0.8078 - val_loss: 2.4658 - val_accuracy: 0.4854\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.91992\n",
            "Epoch 25/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6638 - accuracy: 0.7235 - val_loss: 1.4377 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.91992\n",
            "Epoch 26/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.6190 - accuracy: 0.7470 - val_loss: 1.6884 - val_accuracy: 0.5127\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.91992\n",
            "Epoch 27/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4921 - accuracy: 0.8092 - val_loss: 0.8449 - val_accuracy: 0.6465\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.91992\n",
            "Epoch 28/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4157 - accuracy: 0.8399 - val_loss: 2.1445 - val_accuracy: 0.3809\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.91992\n",
            "Epoch 29/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.3436 - accuracy: 0.8666 - val_loss: 2.7586 - val_accuracy: 0.2002\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.91992\n",
            "Epoch 30/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.3513 - accuracy: 0.8665 - val_loss: 0.5530 - val_accuracy: 0.7588\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.91992\n",
            "Epoch 31/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.2526 - accuracy: 0.9018 - val_loss: 2.5803 - val_accuracy: 0.5469\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.91992\n",
            "Epoch 32/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.2373 - accuracy: 0.8985 - val_loss: 0.4504 - val_accuracy: 0.8340\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.91992\n",
            "Epoch 33/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.2273 - accuracy: 0.9065 - val_loss: 1.3775 - val_accuracy: 0.6328\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.91992\n",
            "Epoch 34/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.2108 - accuracy: 0.9005 - val_loss: 0.6209 - val_accuracy: 0.7432\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.91992\n",
            "Epoch 35/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.3201 - accuracy: 0.8812 - val_loss: 0.9060 - val_accuracy: 0.6123\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.91992\n",
            "Epoch 36/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.2643 - accuracy: 0.8934 - val_loss: 3.2470 - val_accuracy: 0.3770\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.91992\n",
            "Epoch 37/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.3267 - accuracy: 0.8617 - val_loss: 0.6709 - val_accuracy: 0.7324\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.91992\n",
            "Epoch 38/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.2284 - accuracy: 0.9005 - val_loss: 0.2780 - val_accuracy: 0.8965\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.91992\n",
            "Epoch 39/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1955 - accuracy: 0.9182 - val_loss: 0.9094 - val_accuracy: 0.6572\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.91992\n",
            "Epoch 40/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1862 - accuracy: 0.9233 - val_loss: 0.2189 - val_accuracy: 0.9229\n",
            "\n",
            "Epoch 00040: val_accuracy improved from 0.91992 to 0.92285, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 41/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.2353 - accuracy: 0.9080 - val_loss: 1.4754 - val_accuracy: 0.5205\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.92285\n",
            "Epoch 42/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.2267 - accuracy: 0.9093 - val_loss: 0.7412 - val_accuracy: 0.6865\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.92285\n",
            "Epoch 43/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.2062 - accuracy: 0.9119 - val_loss: 0.2757 - val_accuracy: 0.8887\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.92285\n",
            "Epoch 44/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1924 - accuracy: 0.9243 - val_loss: 0.6399 - val_accuracy: 0.7627\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.92285\n",
            "Epoch 45/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.2163 - accuracy: 0.9011 - val_loss: 0.2589 - val_accuracy: 0.9023\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.92285\n",
            "Epoch 46/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1921 - accuracy: 0.9148 - val_loss: 0.3311 - val_accuracy: 0.8438\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.92285\n",
            "Epoch 47/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1888 - accuracy: 0.9276 - val_loss: 0.2726 - val_accuracy: 0.9023\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.92285\n",
            "Epoch 48/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1932 - accuracy: 0.9246 - val_loss: 0.3702 - val_accuracy: 0.8369\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.92285\n",
            "Epoch 49/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1819 - accuracy: 0.9217 - val_loss: 0.3072 - val_accuracy: 0.8770\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.92285\n",
            "Epoch 50/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1796 - accuracy: 0.9216 - val_loss: 0.3699 - val_accuracy: 0.8262\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.92285\n",
            "Epoch 51/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1817 - accuracy: 0.9179 - val_loss: 0.1872 - val_accuracy: 0.9355\n",
            "\n",
            "Epoch 00051: val_accuracy improved from 0.92285 to 0.93555, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 52/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1894 - accuracy: 0.9171 - val_loss: 0.4922 - val_accuracy: 0.7988\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.93555\n",
            "Epoch 53/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1683 - accuracy: 0.9277 - val_loss: 0.1756 - val_accuracy: 0.9424\n",
            "\n",
            "Epoch 00053: val_accuracy improved from 0.93555 to 0.94238, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 54/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1819 - accuracy: 0.9221 - val_loss: 0.2236 - val_accuracy: 0.9033\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.94238\n",
            "Epoch 55/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1778 - accuracy: 0.9196 - val_loss: 0.1764 - val_accuracy: 0.9355\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.94238\n",
            "Epoch 56/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1666 - accuracy: 0.9301 - val_loss: 0.2071 - val_accuracy: 0.9268\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.94238\n",
            "Epoch 57/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1759 - accuracy: 0.9278 - val_loss: 0.2100 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.94238\n",
            "Epoch 58/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1798 - accuracy: 0.9218 - val_loss: 0.4607 - val_accuracy: 0.8164\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.94238\n",
            "Epoch 59/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1701 - accuracy: 0.9251 - val_loss: 0.2134 - val_accuracy: 0.9189\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.94238\n",
            "Epoch 60/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1657 - accuracy: 0.9308 - val_loss: 0.1781 - val_accuracy: 0.9316\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.94238\n",
            "Epoch 61/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1740 - accuracy: 0.9249 - val_loss: 0.2065 - val_accuracy: 0.9277\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.94238\n",
            "Epoch 62/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1922 - accuracy: 0.9164 - val_loss: 0.2400 - val_accuracy: 0.9131\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.94238\n",
            "Epoch 63/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1707 - accuracy: 0.9294 - val_loss: 0.1672 - val_accuracy: 0.9385\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.94238\n",
            "Epoch 64/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1502 - accuracy: 0.9398 - val_loss: 0.3143 - val_accuracy: 0.8701\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.94238\n",
            "Epoch 65/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1571 - accuracy: 0.9332 - val_loss: 0.2358 - val_accuracy: 0.9072\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.94238\n",
            "Epoch 66/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1497 - accuracy: 0.9438 - val_loss: 0.2301 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.94238\n",
            "Epoch 67/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1578 - accuracy: 0.9300 - val_loss: 0.2850 - val_accuracy: 0.8730\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.94238\n",
            "Epoch 68/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1436 - accuracy: 0.9434 - val_loss: 0.1860 - val_accuracy: 0.9209\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.94238\n",
            "Epoch 69/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.1472 - accuracy: 0.9430 - val_loss: 0.7378 - val_accuracy: 0.8047\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.94238\n",
            "Epoch 70/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1497 - accuracy: 0.9408 - val_loss: 0.2008 - val_accuracy: 0.9316\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.94238\n",
            "Epoch 71/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1300 - accuracy: 0.9534 - val_loss: 1.5522 - val_accuracy: 0.6211\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.94238\n",
            "Epoch 72/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1467 - accuracy: 0.9419 - val_loss: 0.3811 - val_accuracy: 0.8135\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.94238\n",
            "Epoch 73/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1358 - accuracy: 0.9521 - val_loss: 0.3935 - val_accuracy: 0.8633\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.94238\n",
            "Epoch 74/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1280 - accuracy: 0.9491 - val_loss: 0.2035 - val_accuracy: 0.9141\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.94238\n",
            "Epoch 75/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1344 - accuracy: 0.9504 - val_loss: 0.7504 - val_accuracy: 0.6846\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.94238\n",
            "Epoch 76/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1720 - accuracy: 0.9353 - val_loss: 1.6973 - val_accuracy: 0.5928\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.94238\n",
            "Epoch 77/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1626 - accuracy: 0.9412 - val_loss: 1.0704 - val_accuracy: 0.7705\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.94238\n",
            "Epoch 78/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1296 - accuracy: 0.9507 - val_loss: 1.1080 - val_accuracy: 0.7490\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.94238\n",
            "Epoch 79/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1227 - accuracy: 0.9600 - val_loss: 0.4385 - val_accuracy: 0.8721\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.94238\n",
            "Epoch 80/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1253 - accuracy: 0.9520 - val_loss: 0.2778 - val_accuracy: 0.8984\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.94238\n",
            "Epoch 81/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1226 - accuracy: 0.9535 - val_loss: 0.1709 - val_accuracy: 0.9365\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.94238\n",
            "Epoch 82/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1109 - accuracy: 0.9595 - val_loss: 0.1468 - val_accuracy: 0.9512\n",
            "\n",
            "Epoch 00082: val_accuracy improved from 0.94238 to 0.95117, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 83/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1177 - accuracy: 0.9604 - val_loss: 0.1770 - val_accuracy: 0.9365\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.95117\n",
            "Epoch 84/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1098 - accuracy: 0.9569 - val_loss: 0.1430 - val_accuracy: 0.9434\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.95117\n",
            "Epoch 85/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1176 - accuracy: 0.9539 - val_loss: 0.1603 - val_accuracy: 0.9375\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.95117\n",
            "Epoch 86/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1016 - accuracy: 0.9643 - val_loss: 0.1930 - val_accuracy: 0.9307\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.95117\n",
            "Epoch 87/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0989 - accuracy: 0.9645 - val_loss: 0.1307 - val_accuracy: 0.9473\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.95117\n",
            "Epoch 88/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0962 - accuracy: 0.9641 - val_loss: 0.1583 - val_accuracy: 0.9326\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.95117\n",
            "Epoch 89/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1085 - accuracy: 0.9593 - val_loss: 0.1237 - val_accuracy: 0.9590\n",
            "\n",
            "Epoch 00089: val_accuracy improved from 0.95117 to 0.95898, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 90/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0858 - accuracy: 0.9716 - val_loss: 0.4966 - val_accuracy: 0.8760\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.95898\n",
            "Epoch 91/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0982 - accuracy: 0.9605 - val_loss: 0.1512 - val_accuracy: 0.9473\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.95898\n",
            "Epoch 92/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0940 - accuracy: 0.9627 - val_loss: 0.1211 - val_accuracy: 0.9570\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.95898\n",
            "Epoch 93/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0829 - accuracy: 0.9716 - val_loss: 0.1913 - val_accuracy: 0.9346\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.95898\n",
            "Epoch 94/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1000 - accuracy: 0.9613 - val_loss: 0.3399 - val_accuracy: 0.8994\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.95898\n",
            "Epoch 95/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0839 - accuracy: 0.9664 - val_loss: 0.1438 - val_accuracy: 0.9561\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.95898\n",
            "Epoch 96/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0909 - accuracy: 0.9686 - val_loss: 0.1583 - val_accuracy: 0.9492\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.95898\n",
            "Epoch 97/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0829 - accuracy: 0.9712 - val_loss: 0.1252 - val_accuracy: 0.9492\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.95898\n",
            "Epoch 98/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0771 - accuracy: 0.9727 - val_loss: 0.1187 - val_accuracy: 0.9580\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.95898\n",
            "Epoch 99/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0896 - accuracy: 0.9679 - val_loss: 0.2159 - val_accuracy: 0.9131\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.95898\n",
            "Epoch 100/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0868 - accuracy: 0.9691 - val_loss: 0.1336 - val_accuracy: 0.9590\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.95898\n",
            "Epoch 101/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0738 - accuracy: 0.9747 - val_loss: 0.1090 - val_accuracy: 0.9639\n",
            "\n",
            "Epoch 00101: val_accuracy improved from 0.95898 to 0.96387, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 102/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0667 - accuracy: 0.9766 - val_loss: 0.1130 - val_accuracy: 0.9580\n",
            "\n",
            "Epoch 00102: val_accuracy did not improve from 0.96387\n",
            "Epoch 103/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0668 - accuracy: 0.9756 - val_loss: 0.1099 - val_accuracy: 0.9580\n",
            "\n",
            "Epoch 00103: val_accuracy did not improve from 0.96387\n",
            "Epoch 104/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0678 - accuracy: 0.9769 - val_loss: 0.1116 - val_accuracy: 0.9541\n",
            "\n",
            "Epoch 00104: val_accuracy did not improve from 0.96387\n",
            "Epoch 105/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0591 - accuracy: 0.9794 - val_loss: 0.1255 - val_accuracy: 0.9521\n",
            "\n",
            "Epoch 00105: val_accuracy did not improve from 0.96387\n",
            "Epoch 106/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0660 - accuracy: 0.9783 - val_loss: 0.2444 - val_accuracy: 0.8916\n",
            "\n",
            "Epoch 00106: val_accuracy did not improve from 0.96387\n",
            "Epoch 107/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0598 - accuracy: 0.9785 - val_loss: 0.1023 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00107: val_accuracy did not improve from 0.96387\n",
            "Epoch 108/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0567 - accuracy: 0.9790 - val_loss: 0.1225 - val_accuracy: 0.9531\n",
            "\n",
            "Epoch 00108: val_accuracy did not improve from 0.96387\n",
            "Epoch 109/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0699 - accuracy: 0.9770 - val_loss: 0.1246 - val_accuracy: 0.9541\n",
            "\n",
            "Epoch 00109: val_accuracy did not improve from 0.96387\n",
            "Epoch 110/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0546 - accuracy: 0.9819 - val_loss: 0.1150 - val_accuracy: 0.9580\n",
            "\n",
            "Epoch 00110: val_accuracy did not improve from 0.96387\n",
            "Epoch 111/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0616 - accuracy: 0.9766 - val_loss: 0.4161 - val_accuracy: 0.9023\n",
            "\n",
            "Epoch 00111: val_accuracy did not improve from 0.96387\n",
            "Epoch 112/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0600 - accuracy: 0.9789 - val_loss: 0.1653 - val_accuracy: 0.9297\n",
            "\n",
            "Epoch 00112: val_accuracy did not improve from 0.96387\n",
            "Epoch 113/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0640 - accuracy: 0.9755 - val_loss: 0.1135 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00113: val_accuracy did not improve from 0.96387\n",
            "Epoch 114/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0509 - accuracy: 0.9829 - val_loss: 0.1350 - val_accuracy: 0.9541\n",
            "\n",
            "Epoch 00114: val_accuracy did not improve from 0.96387\n",
            "Epoch 115/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0486 - accuracy: 0.9849 - val_loss: 0.5054 - val_accuracy: 0.7969\n",
            "\n",
            "Epoch 00115: val_accuracy did not improve from 0.96387\n",
            "Epoch 116/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0576 - accuracy: 0.9810 - val_loss: 0.2245 - val_accuracy: 0.9258\n",
            "\n",
            "Epoch 00116: val_accuracy did not improve from 0.96387\n",
            "Epoch 117/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0573 - accuracy: 0.9782 - val_loss: 0.1833 - val_accuracy: 0.9297\n",
            "\n",
            "Epoch 00117: val_accuracy did not improve from 0.96387\n",
            "Epoch 118/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0510 - accuracy: 0.9808 - val_loss: 0.1289 - val_accuracy: 0.9482\n",
            "\n",
            "Epoch 00118: val_accuracy did not improve from 0.96387\n",
            "Epoch 119/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0545 - accuracy: 0.9822 - val_loss: 0.0976 - val_accuracy: 0.9668\n",
            "\n",
            "Epoch 00119: val_accuracy improved from 0.96387 to 0.96680, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 120/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0444 - accuracy: 0.9877 - val_loss: 0.1666 - val_accuracy: 0.9365\n",
            "\n",
            "Epoch 00120: val_accuracy did not improve from 0.96680\n",
            "Epoch 121/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0470 - accuracy: 0.9836 - val_loss: 0.1096 - val_accuracy: 0.9639\n",
            "\n",
            "Epoch 00121: val_accuracy did not improve from 0.96680\n",
            "Epoch 122/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0449 - accuracy: 0.9855 - val_loss: 0.1018 - val_accuracy: 0.9648\n",
            "\n",
            "Epoch 00122: val_accuracy did not improve from 0.96680\n",
            "Epoch 123/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0441 - accuracy: 0.9854 - val_loss: 0.1156 - val_accuracy: 0.9570\n",
            "\n",
            "Epoch 00123: val_accuracy did not improve from 0.96680\n",
            "Epoch 124/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0419 - accuracy: 0.9854 - val_loss: 0.0940 - val_accuracy: 0.9639\n",
            "\n",
            "Epoch 00124: val_accuracy did not improve from 0.96680\n",
            "Epoch 125/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0407 - accuracy: 0.9867 - val_loss: 0.2093 - val_accuracy: 0.9287\n",
            "\n",
            "Epoch 00125: val_accuracy did not improve from 0.96680\n",
            "Epoch 126/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0439 - accuracy: 0.9857 - val_loss: 0.3078 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00126: val_accuracy did not improve from 0.96680\n",
            "Epoch 127/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0470 - accuracy: 0.9832 - val_loss: 0.1715 - val_accuracy: 0.9336\n",
            "\n",
            "Epoch 00127: val_accuracy did not improve from 0.96680\n",
            "Epoch 128/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0405 - accuracy: 0.9870 - val_loss: 0.3921 - val_accuracy: 0.9160\n",
            "\n",
            "Epoch 00128: val_accuracy did not improve from 0.96680\n",
            "Epoch 129/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0435 - accuracy: 0.9858 - val_loss: 0.1022 - val_accuracy: 0.9619\n",
            "\n",
            "Epoch 00129: val_accuracy did not improve from 0.96680\n",
            "Epoch 130/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0392 - accuracy: 0.9879 - val_loss: 0.0892 - val_accuracy: 0.9648\n",
            "\n",
            "Epoch 00130: val_accuracy did not improve from 0.96680\n",
            "Epoch 131/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0384 - accuracy: 0.9875 - val_loss: 0.0880 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00131: val_accuracy improved from 0.96680 to 0.97168, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 132/500\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.0376 - accuracy: 0.9866 - val_loss: 0.0819 - val_accuracy: 0.9678\n",
            "\n",
            "Epoch 00132: val_accuracy did not improve from 0.97168\n",
            "Epoch 133/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0435 - accuracy: 0.9845 - val_loss: 0.1447 - val_accuracy: 0.9502\n",
            "\n",
            "Epoch 00133: val_accuracy did not improve from 0.97168\n",
            "Epoch 134/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0385 - accuracy: 0.9898 - val_loss: 0.1288 - val_accuracy: 0.9590\n",
            "\n",
            "Epoch 00134: val_accuracy did not improve from 0.97168\n",
            "Epoch 135/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0309 - accuracy: 0.9909 - val_loss: 0.1525 - val_accuracy: 0.9395\n",
            "\n",
            "Epoch 00135: val_accuracy did not improve from 0.97168\n",
            "Epoch 136/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0309 - accuracy: 0.9912 - val_loss: 0.1644 - val_accuracy: 0.9482\n",
            "\n",
            "Epoch 00136: val_accuracy did not improve from 0.97168\n",
            "Epoch 137/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0321 - accuracy: 0.9893 - val_loss: 0.1028 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00137: val_accuracy did not improve from 0.97168\n",
            "Epoch 138/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0358 - accuracy: 0.9875 - val_loss: 0.2576 - val_accuracy: 0.8809\n",
            "\n",
            "Epoch 00138: val_accuracy did not improve from 0.97168\n",
            "Epoch 139/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0303 - accuracy: 0.9905 - val_loss: 0.0792 - val_accuracy: 0.9678\n",
            "\n",
            "Epoch 00139: val_accuracy did not improve from 0.97168\n",
            "Epoch 140/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0299 - accuracy: 0.9897 - val_loss: 0.1905 - val_accuracy: 0.9424\n",
            "\n",
            "Epoch 00140: val_accuracy did not improve from 0.97168\n",
            "Epoch 141/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0325 - accuracy: 0.9881 - val_loss: 0.0814 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00141: val_accuracy did not improve from 0.97168\n",
            "Epoch 142/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0323 - accuracy: 0.9889 - val_loss: 0.1124 - val_accuracy: 0.9551\n",
            "\n",
            "Epoch 00142: val_accuracy did not improve from 0.97168\n",
            "Epoch 143/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0257 - accuracy: 0.9935 - val_loss: 0.0732 - val_accuracy: 0.9707\n",
            "\n",
            "Epoch 00143: val_accuracy did not improve from 0.97168\n",
            "Epoch 144/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0257 - accuracy: 0.9906 - val_loss: 0.1014 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00144: val_accuracy did not improve from 0.97168\n",
            "Epoch 145/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0282 - accuracy: 0.9922 - val_loss: 0.0934 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00145: val_accuracy did not improve from 0.97168\n",
            "Epoch 146/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0273 - accuracy: 0.9918 - val_loss: 0.0807 - val_accuracy: 0.9697\n",
            "\n",
            "Epoch 00146: val_accuracy did not improve from 0.97168\n",
            "Epoch 147/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0302 - accuracy: 0.9907 - val_loss: 0.1232 - val_accuracy: 0.9648\n",
            "\n",
            "Epoch 00147: val_accuracy did not improve from 0.97168\n",
            "Epoch 148/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0262 - accuracy: 0.9909 - val_loss: 0.1037 - val_accuracy: 0.9697\n",
            "\n",
            "Epoch 00148: val_accuracy did not improve from 0.97168\n",
            "Epoch 149/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0226 - accuracy: 0.9920 - val_loss: 0.1240 - val_accuracy: 0.9678\n",
            "\n",
            "Epoch 00149: val_accuracy did not improve from 0.97168\n",
            "Epoch 150/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0336 - accuracy: 0.9899 - val_loss: 0.0734 - val_accuracy: 0.9707\n",
            "\n",
            "Epoch 00150: val_accuracy did not improve from 0.97168\n",
            "Epoch 151/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0261 - accuracy: 0.9928 - val_loss: 0.0808 - val_accuracy: 0.9678\n",
            "\n",
            "Epoch 00151: val_accuracy did not improve from 0.97168\n",
            "Epoch 152/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0418 - accuracy: 0.9831 - val_loss: 0.4604 - val_accuracy: 0.8818\n",
            "\n",
            "Epoch 00152: val_accuracy did not improve from 0.97168\n",
            "Epoch 153/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0263 - accuracy: 0.9920 - val_loss: 0.4226 - val_accuracy: 0.8857\n",
            "\n",
            "Epoch 00153: val_accuracy did not improve from 0.97168\n",
            "Epoch 154/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0265 - accuracy: 0.9905 - val_loss: 0.8648 - val_accuracy: 0.8203\n",
            "\n",
            "Epoch 00154: val_accuracy did not improve from 0.97168\n",
            "Epoch 155/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0338 - accuracy: 0.9898 - val_loss: 0.1571 - val_accuracy: 0.9258\n",
            "\n",
            "Epoch 00155: val_accuracy did not improve from 0.97168\n",
            "Epoch 156/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0243 - accuracy: 0.9931 - val_loss: 0.2729 - val_accuracy: 0.8945\n",
            "\n",
            "Epoch 00156: val_accuracy did not improve from 0.97168\n",
            "Epoch 157/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0267 - accuracy: 0.9916 - val_loss: 0.1247 - val_accuracy: 0.9619\n",
            "\n",
            "Epoch 00157: val_accuracy did not improve from 0.97168\n",
            "Epoch 158/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0264 - accuracy: 0.9913 - val_loss: 0.0670 - val_accuracy: 0.9746\n",
            "\n",
            "Epoch 00158: val_accuracy improved from 0.97168 to 0.97461, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 159/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0192 - accuracy: 0.9958 - val_loss: 0.0849 - val_accuracy: 0.9678\n",
            "\n",
            "Epoch 00159: val_accuracy did not improve from 0.97461\n",
            "Epoch 160/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0287 - accuracy: 0.9910 - val_loss: 0.1554 - val_accuracy: 0.9307\n",
            "\n",
            "Epoch 00160: val_accuracy did not improve from 0.97461\n",
            "Epoch 161/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0239 - accuracy: 0.9940 - val_loss: 0.0691 - val_accuracy: 0.9746\n",
            "\n",
            "Epoch 00161: val_accuracy did not improve from 0.97461\n",
            "Epoch 162/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0257 - accuracy: 0.9918 - val_loss: 0.0738 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00162: val_accuracy did not improve from 0.97461\n",
            "Epoch 163/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0232 - accuracy: 0.9932 - val_loss: 0.2769 - val_accuracy: 0.9180\n",
            "\n",
            "Epoch 00163: val_accuracy did not improve from 0.97461\n",
            "Epoch 164/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0249 - accuracy: 0.9917 - val_loss: 0.2470 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00164: val_accuracy did not improve from 0.97461\n",
            "Epoch 165/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0236 - accuracy: 0.9913 - val_loss: 0.0828 - val_accuracy: 0.9619\n",
            "\n",
            "Epoch 00165: val_accuracy did not improve from 0.97461\n",
            "Epoch 166/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0246 - accuracy: 0.9907 - val_loss: 0.0828 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00166: val_accuracy did not improve from 0.97461\n",
            "Epoch 167/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0215 - accuracy: 0.9931 - val_loss: 0.1653 - val_accuracy: 0.9277\n",
            "\n",
            "Epoch 00167: val_accuracy did not improve from 0.97461\n",
            "Epoch 168/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0173 - accuracy: 0.9949 - val_loss: 0.0896 - val_accuracy: 0.9639\n",
            "\n",
            "Epoch 00168: val_accuracy did not improve from 0.97461\n",
            "Epoch 169/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0168 - accuracy: 0.9943 - val_loss: 0.1052 - val_accuracy: 0.9678\n",
            "\n",
            "Epoch 00169: val_accuracy did not improve from 0.97461\n",
            "Epoch 170/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0205 - accuracy: 0.9941 - val_loss: 0.3292 - val_accuracy: 0.8760\n",
            "\n",
            "Epoch 00170: val_accuracy did not improve from 0.97461\n",
            "Epoch 171/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0219 - accuracy: 0.9934 - val_loss: 0.0916 - val_accuracy: 0.9609\n",
            "\n",
            "Epoch 00171: val_accuracy did not improve from 0.97461\n",
            "Epoch 172/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0162 - accuracy: 0.9966 - val_loss: 0.1264 - val_accuracy: 0.9668\n",
            "\n",
            "Epoch 00172: val_accuracy did not improve from 0.97461\n",
            "Epoch 173/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0171 - accuracy: 0.9961 - val_loss: 0.0667 - val_accuracy: 0.9746\n",
            "\n",
            "Epoch 00173: val_accuracy did not improve from 0.97461\n",
            "Epoch 174/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0171 - accuracy: 0.9958 - val_loss: 0.1182 - val_accuracy: 0.9668\n",
            "\n",
            "Epoch 00174: val_accuracy did not improve from 0.97461\n",
            "Epoch 175/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0195 - accuracy: 0.9928 - val_loss: 0.1327 - val_accuracy: 0.9404\n",
            "\n",
            "Epoch 00175: val_accuracy did not improve from 0.97461\n",
            "Epoch 176/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0149 - accuracy: 0.9970 - val_loss: 0.1009 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00176: val_accuracy did not improve from 0.97461\n",
            "Epoch 177/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0163 - accuracy: 0.9947 - val_loss: 0.1099 - val_accuracy: 0.9541\n",
            "\n",
            "Epoch 00177: val_accuracy did not improve from 0.97461\n",
            "Epoch 178/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0152 - accuracy: 0.9969 - val_loss: 0.0835 - val_accuracy: 0.9727\n",
            "\n",
            "Epoch 00178: val_accuracy did not improve from 0.97461\n",
            "Epoch 179/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0121 - accuracy: 0.9970 - val_loss: 0.0683 - val_accuracy: 0.9766\n",
            "\n",
            "Epoch 00179: val_accuracy improved from 0.97461 to 0.97656, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 180/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0122 - accuracy: 0.9976 - val_loss: 0.0663 - val_accuracy: 0.9746\n",
            "\n",
            "Epoch 00180: val_accuracy did not improve from 0.97656\n",
            "Epoch 181/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0150 - accuracy: 0.9963 - val_loss: 0.1036 - val_accuracy: 0.9609\n",
            "\n",
            "Epoch 00181: val_accuracy did not improve from 0.97656\n",
            "Epoch 182/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0134 - accuracy: 0.9963 - val_loss: 0.0919 - val_accuracy: 0.9697\n",
            "\n",
            "Epoch 00182: val_accuracy did not improve from 0.97656\n",
            "Epoch 183/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0127 - accuracy: 0.9961 - val_loss: 0.0738 - val_accuracy: 0.9766\n",
            "\n",
            "Epoch 00183: val_accuracy did not improve from 0.97656\n",
            "Epoch 184/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0145 - accuracy: 0.9962 - val_loss: 0.0721 - val_accuracy: 0.9785\n",
            "\n",
            "Epoch 00184: val_accuracy improved from 0.97656 to 0.97852, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 185/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0133 - accuracy: 0.9961 - val_loss: 0.1075 - val_accuracy: 0.9521\n",
            "\n",
            "Epoch 00185: val_accuracy did not improve from 0.97852\n",
            "Epoch 186/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0125 - accuracy: 0.9971 - val_loss: 0.0756 - val_accuracy: 0.9727\n",
            "\n",
            "Epoch 00186: val_accuracy did not improve from 0.97852\n",
            "Epoch 187/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0116 - accuracy: 0.9967 - val_loss: 0.0855 - val_accuracy: 0.9727\n",
            "\n",
            "Epoch 00187: val_accuracy did not improve from 0.97852\n",
            "Epoch 188/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0181 - accuracy: 0.9941 - val_loss: 0.1461 - val_accuracy: 0.9414\n",
            "\n",
            "Epoch 00188: val_accuracy did not improve from 0.97852\n",
            "Epoch 189/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0114 - accuracy: 0.9971 - val_loss: 0.0641 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00189: val_accuracy improved from 0.97852 to 0.98047, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 190/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0103 - accuracy: 0.9982 - val_loss: 0.1120 - val_accuracy: 0.9678\n",
            "\n",
            "Epoch 00190: val_accuracy did not improve from 0.98047\n",
            "Epoch 191/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0125 - accuracy: 0.9969 - val_loss: 0.0864 - val_accuracy: 0.9727\n",
            "\n",
            "Epoch 00191: val_accuracy did not improve from 0.98047\n",
            "Epoch 192/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0108 - accuracy: 0.9978 - val_loss: 0.1693 - val_accuracy: 0.9141\n",
            "\n",
            "Epoch 00192: val_accuracy did not improve from 0.98047\n",
            "Epoch 193/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0233 - accuracy: 0.9916 - val_loss: 0.1508 - val_accuracy: 0.9453\n",
            "\n",
            "Epoch 00193: val_accuracy did not improve from 0.98047\n",
            "Epoch 194/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0167 - accuracy: 0.9947 - val_loss: 0.1209 - val_accuracy: 0.9531\n",
            "\n",
            "Epoch 00194: val_accuracy did not improve from 0.98047\n",
            "Epoch 195/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0124 - accuracy: 0.9969 - val_loss: 0.0601 - val_accuracy: 0.9785\n",
            "\n",
            "Epoch 00195: val_accuracy did not improve from 0.98047\n",
            "Epoch 196/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0107 - accuracy: 0.9977 - val_loss: 0.0779 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00196: val_accuracy did not improve from 0.98047\n",
            "Epoch 197/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0169 - accuracy: 0.9953 - val_loss: 0.0686 - val_accuracy: 0.9707\n",
            "\n",
            "Epoch 00197: val_accuracy did not improve from 0.98047\n",
            "Epoch 198/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0103 - accuracy: 0.9977 - val_loss: 0.4152 - val_accuracy: 0.8770\n",
            "\n",
            "Epoch 00198: val_accuracy did not improve from 0.98047\n",
            "Epoch 199/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0089 - accuracy: 0.9981 - val_loss: 0.1340 - val_accuracy: 0.9502\n",
            "\n",
            "Epoch 00199: val_accuracy did not improve from 0.98047\n",
            "Epoch 200/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0091 - accuracy: 0.9979 - val_loss: 0.0754 - val_accuracy: 0.9746\n",
            "\n",
            "Epoch 00200: val_accuracy did not improve from 0.98047\n",
            "Epoch 201/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0096 - accuracy: 0.9984 - val_loss: 0.0704 - val_accuracy: 0.9746\n",
            "\n",
            "Epoch 00201: val_accuracy did not improve from 0.98047\n",
            "Epoch 202/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0109 - accuracy: 0.9973 - val_loss: 0.0692 - val_accuracy: 0.9736\n",
            "\n",
            "Epoch 00202: val_accuracy did not improve from 0.98047\n",
            "Epoch 203/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0072 - accuracy: 0.9993 - val_loss: 0.0787 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00203: val_accuracy did not improve from 0.98047\n",
            "Epoch 204/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0083 - accuracy: 0.9982 - val_loss: 0.0681 - val_accuracy: 0.9746\n",
            "\n",
            "Epoch 00204: val_accuracy did not improve from 0.98047\n",
            "Epoch 205/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0118 - accuracy: 0.9980 - val_loss: 0.1012 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00205: val_accuracy did not improve from 0.98047\n",
            "Epoch 206/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0087 - accuracy: 0.9966 - val_loss: 0.0982 - val_accuracy: 0.9678\n",
            "\n",
            "Epoch 00206: val_accuracy did not improve from 0.98047\n",
            "Epoch 207/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0071 - accuracy: 0.9993 - val_loss: 0.1015 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00207: val_accuracy did not improve from 0.98047\n",
            "Epoch 208/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0084 - accuracy: 0.9979 - val_loss: 0.0852 - val_accuracy: 0.9746\n",
            "\n",
            "Epoch 00208: val_accuracy did not improve from 0.98047\n",
            "Epoch 209/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0070 - accuracy: 0.9987 - val_loss: 0.1121 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00209: val_accuracy did not improve from 0.98047\n",
            "Epoch 210/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0109 - accuracy: 0.9980 - val_loss: 0.1561 - val_accuracy: 0.9551\n",
            "\n",
            "Epoch 00210: val_accuracy did not improve from 0.98047\n",
            "Epoch 211/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0088 - accuracy: 0.9970 - val_loss: 0.0607 - val_accuracy: 0.9785\n",
            "\n",
            "Epoch 00211: val_accuracy did not improve from 0.98047\n",
            "Epoch 212/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0107 - accuracy: 0.9972 - val_loss: 0.0862 - val_accuracy: 0.9678\n",
            "\n",
            "Epoch 00212: val_accuracy did not improve from 0.98047\n",
            "Epoch 213/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0089 - accuracy: 0.9978 - val_loss: 0.6360 - val_accuracy: 0.8984\n",
            "\n",
            "Epoch 00213: val_accuracy did not improve from 0.98047\n",
            "Epoch 214/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0115 - accuracy: 0.9973 - val_loss: 0.0765 - val_accuracy: 0.9746\n",
            "\n",
            "Epoch 00214: val_accuracy did not improve from 0.98047\n",
            "Epoch 215/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0071 - accuracy: 0.9994 - val_loss: 0.0657 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00215: val_accuracy did not improve from 0.98047\n",
            "Epoch 216/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0121 - accuracy: 0.9968 - val_loss: 1.0133 - val_accuracy: 0.8096\n",
            "\n",
            "Epoch 00216: val_accuracy did not improve from 0.98047\n",
            "Epoch 217/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0167 - accuracy: 0.9955 - val_loss: 0.1041 - val_accuracy: 0.9531\n",
            "\n",
            "Epoch 00217: val_accuracy did not improve from 0.98047\n",
            "Epoch 218/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0442 - accuracy: 0.9848 - val_loss: 0.1977 - val_accuracy: 0.9346\n",
            "\n",
            "Epoch 00218: val_accuracy did not improve from 0.98047\n",
            "Epoch 219/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0735 - accuracy: 0.9740 - val_loss: 1.1458 - val_accuracy: 0.7861\n",
            "\n",
            "Epoch 00219: val_accuracy did not improve from 0.98047\n",
            "Epoch 220/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0228 - accuracy: 0.9918 - val_loss: 0.1426 - val_accuracy: 0.9561\n",
            "\n",
            "Epoch 00220: val_accuracy did not improve from 0.98047\n",
            "Epoch 221/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0231 - accuracy: 0.9919 - val_loss: 0.0973 - val_accuracy: 0.9609\n",
            "\n",
            "Epoch 00221: val_accuracy did not improve from 0.98047\n",
            "Epoch 222/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0114 - accuracy: 0.9959 - val_loss: 0.1037 - val_accuracy: 0.9609\n",
            "\n",
            "Epoch 00222: val_accuracy did not improve from 0.98047\n",
            "Epoch 223/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0093 - accuracy: 0.9992 - val_loss: 0.0972 - val_accuracy: 0.9727\n",
            "\n",
            "Epoch 00223: val_accuracy did not improve from 0.98047\n",
            "Epoch 224/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0139 - accuracy: 0.9960 - val_loss: 0.1664 - val_accuracy: 0.9229\n",
            "\n",
            "Epoch 00224: val_accuracy did not improve from 0.98047\n",
            "Epoch 225/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0106 - accuracy: 0.9972 - val_loss: 0.1169 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00225: val_accuracy did not improve from 0.98047\n",
            "Epoch 226/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0083 - accuracy: 0.9983 - val_loss: 0.1437 - val_accuracy: 0.9453\n",
            "\n",
            "Epoch 00226: val_accuracy did not improve from 0.98047\n",
            "Epoch 227/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0083 - accuracy: 0.9982 - val_loss: 0.0820 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00227: val_accuracy did not improve from 0.98047\n",
            "Epoch 228/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0089 - accuracy: 0.9968 - val_loss: 0.1203 - val_accuracy: 0.9668\n",
            "\n",
            "Epoch 00228: val_accuracy did not improve from 0.98047\n",
            "Epoch 229/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0082 - accuracy: 0.9992 - val_loss: 0.0705 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00229: val_accuracy did not improve from 0.98047\n",
            "Epoch 230/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0069 - accuracy: 0.9996 - val_loss: 0.0701 - val_accuracy: 0.9727\n",
            "\n",
            "Epoch 00230: val_accuracy did not improve from 0.98047\n",
            "Epoch 231/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0073 - accuracy: 0.9992 - val_loss: 0.0845 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00231: val_accuracy did not improve from 0.98047\n",
            "Epoch 232/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0656 - val_accuracy: 0.9746\n",
            "\n",
            "Epoch 00232: val_accuracy did not improve from 0.98047\n",
            "Epoch 233/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0056 - accuracy: 0.9997 - val_loss: 0.0802 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00233: val_accuracy did not improve from 0.98047\n",
            "Epoch 234/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0059 - accuracy: 0.9988 - val_loss: 0.0797 - val_accuracy: 0.9775\n",
            "\n",
            "Epoch 00234: val_accuracy did not improve from 0.98047\n",
            "Epoch 235/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0046 - accuracy: 0.9999 - val_loss: 0.0825 - val_accuracy: 0.9766\n",
            "\n",
            "Epoch 00235: val_accuracy did not improve from 0.98047\n",
            "Epoch 236/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0084 - accuracy: 0.9980 - val_loss: 0.0751 - val_accuracy: 0.9697\n",
            "\n",
            "Epoch 00236: val_accuracy did not improve from 0.98047\n",
            "Epoch 237/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0063 - accuracy: 0.9990 - val_loss: 0.0808 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00237: val_accuracy did not improve from 0.98047\n",
            "Epoch 238/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0042 - accuracy: 0.9999 - val_loss: 0.0651 - val_accuracy: 0.9814\n",
            "\n",
            "Epoch 00238: val_accuracy improved from 0.98047 to 0.98145, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 239/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.1471 - val_accuracy: 0.9326\n",
            "\n",
            "Epoch 00239: val_accuracy did not improve from 0.98145\n",
            "Epoch 240/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0047 - accuracy: 0.9994 - val_loss: 0.0808 - val_accuracy: 0.9736\n",
            "\n",
            "Epoch 00240: val_accuracy did not improve from 0.98145\n",
            "Epoch 241/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0043 - accuracy: 0.9995 - val_loss: 0.0663 - val_accuracy: 0.9775\n",
            "\n",
            "Epoch 00241: val_accuracy did not improve from 0.98145\n",
            "Epoch 242/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0683 - val_accuracy: 0.9746\n",
            "\n",
            "Epoch 00242: val_accuracy did not improve from 0.98145\n",
            "Epoch 243/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0036 - accuracy: 0.9999 - val_loss: 0.0828 - val_accuracy: 0.9775\n",
            "\n",
            "Epoch 00243: val_accuracy did not improve from 0.98145\n",
            "Epoch 244/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.1472 - accuracy: 0.9557 - val_loss: 0.4160 - val_accuracy: 0.8057\n",
            "\n",
            "Epoch 00244: val_accuracy did not improve from 0.98145\n",
            "Epoch 245/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0688 - accuracy: 0.9754 - val_loss: 0.2968 - val_accuracy: 0.9043\n",
            "\n",
            "Epoch 00245: val_accuracy did not improve from 0.98145\n",
            "Epoch 246/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0624 - accuracy: 0.9775 - val_loss: 1.5368 - val_accuracy: 0.6289\n",
            "\n",
            "Epoch 00246: val_accuracy did not improve from 0.98145\n",
            "Epoch 247/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0638 - accuracy: 0.9778 - val_loss: 0.6923 - val_accuracy: 0.8438\n",
            "\n",
            "Epoch 00247: val_accuracy did not improve from 0.98145\n",
            "Epoch 248/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0386 - accuracy: 0.9867 - val_loss: 0.6059 - val_accuracy: 0.7627\n",
            "\n",
            "Epoch 00248: val_accuracy did not improve from 0.98145\n",
            "Epoch 249/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0365 - accuracy: 0.9857 - val_loss: 0.5307 - val_accuracy: 0.8535\n",
            "\n",
            "Epoch 00249: val_accuracy did not improve from 0.98145\n",
            "Epoch 250/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0287 - accuracy: 0.9891 - val_loss: 0.1513 - val_accuracy: 0.9316\n",
            "\n",
            "Epoch 00250: val_accuracy did not improve from 0.98145\n",
            "Epoch 251/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0244 - accuracy: 0.9918 - val_loss: 0.3122 - val_accuracy: 0.8740\n",
            "\n",
            "Epoch 00251: val_accuracy did not improve from 0.98145\n",
            "Epoch 252/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0230 - accuracy: 0.9908 - val_loss: 0.0600 - val_accuracy: 0.9775\n",
            "\n",
            "Epoch 00252: val_accuracy did not improve from 0.98145\n",
            "Epoch 253/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0173 - accuracy: 0.9956 - val_loss: 0.6029 - val_accuracy: 0.8857\n",
            "\n",
            "Epoch 00253: val_accuracy did not improve from 0.98145\n",
            "Epoch 254/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0149 - accuracy: 0.9971 - val_loss: 0.1216 - val_accuracy: 0.9639\n",
            "\n",
            "Epoch 00254: val_accuracy did not improve from 0.98145\n",
            "Epoch 255/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0195 - accuracy: 0.9946 - val_loss: 0.0772 - val_accuracy: 0.9727\n",
            "\n",
            "Epoch 00255: val_accuracy did not improve from 0.98145\n",
            "Epoch 256/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0120 - accuracy: 0.9976 - val_loss: 0.0772 - val_accuracy: 0.9785\n",
            "\n",
            "Epoch 00256: val_accuracy did not improve from 0.98145\n",
            "Epoch 257/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0115 - accuracy: 0.9975 - val_loss: 0.1269 - val_accuracy: 0.9609\n",
            "\n",
            "Epoch 00257: val_accuracy did not improve from 0.98145\n",
            "Epoch 258/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0140 - accuracy: 0.9952 - val_loss: 0.3057 - val_accuracy: 0.8916\n",
            "\n",
            "Epoch 00258: val_accuracy did not improve from 0.98145\n",
            "Epoch 259/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0155 - accuracy: 0.9957 - val_loss: 0.1334 - val_accuracy: 0.9648\n",
            "\n",
            "Epoch 00259: val_accuracy did not improve from 0.98145\n",
            "Epoch 260/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0110 - accuracy: 0.9983 - val_loss: 0.0682 - val_accuracy: 0.9785\n",
            "\n",
            "Epoch 00260: val_accuracy did not improve from 0.98145\n",
            "Epoch 261/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0086 - accuracy: 0.9979 - val_loss: 0.0848 - val_accuracy: 0.9736\n",
            "\n",
            "Epoch 00261: val_accuracy did not improve from 0.98145\n",
            "Epoch 262/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0088 - accuracy: 0.9973 - val_loss: 0.0912 - val_accuracy: 0.9707\n",
            "\n",
            "Epoch 00262: val_accuracy did not improve from 0.98145\n",
            "Epoch 263/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0145 - accuracy: 0.9962 - val_loss: 0.0973 - val_accuracy: 0.9736\n",
            "\n",
            "Epoch 00263: val_accuracy did not improve from 0.98145\n",
            "Epoch 264/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0252 - accuracy: 0.9912 - val_loss: 0.5057 - val_accuracy: 0.8994\n",
            "\n",
            "Epoch 00264: val_accuracy did not improve from 0.98145\n",
            "Epoch 265/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0141 - accuracy: 0.9961 - val_loss: 0.0754 - val_accuracy: 0.9727\n",
            "\n",
            "Epoch 00265: val_accuracy did not improve from 0.98145\n",
            "Epoch 266/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0079 - accuracy: 0.9990 - val_loss: 0.0731 - val_accuracy: 0.9736\n",
            "\n",
            "Epoch 00266: val_accuracy did not improve from 0.98145\n",
            "Epoch 267/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0220 - accuracy: 0.9929 - val_loss: 0.0658 - val_accuracy: 0.9785\n",
            "\n",
            "Epoch 00267: val_accuracy did not improve from 0.98145\n",
            "Epoch 268/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0098 - accuracy: 0.9984 - val_loss: 0.1267 - val_accuracy: 0.9590\n",
            "\n",
            "Epoch 00268: val_accuracy did not improve from 0.98145\n",
            "Epoch 269/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0071 - accuracy: 0.9989 - val_loss: 0.0637 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00269: val_accuracy did not improve from 0.98145\n",
            "Epoch 270/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0058 - accuracy: 0.9996 - val_loss: 0.0609 - val_accuracy: 0.9785\n",
            "\n",
            "Epoch 00270: val_accuracy did not improve from 0.98145\n",
            "Epoch 271/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0086 - accuracy: 0.9977 - val_loss: 0.1284 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00271: val_accuracy did not improve from 0.98145\n",
            "Epoch 272/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0092 - accuracy: 0.9977 - val_loss: 0.1344 - val_accuracy: 0.9414\n",
            "\n",
            "Epoch 00272: val_accuracy did not improve from 0.98145\n",
            "Epoch 273/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0058 - accuracy: 0.9981 - val_loss: 0.0761 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00273: val_accuracy did not improve from 0.98145\n",
            "Epoch 274/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0084 - accuracy: 0.9978 - val_loss: 0.2853 - val_accuracy: 0.8877\n",
            "\n",
            "Epoch 00274: val_accuracy did not improve from 0.98145\n",
            "Epoch 275/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0130 - accuracy: 0.9957 - val_loss: 0.1295 - val_accuracy: 0.9531\n",
            "\n",
            "Epoch 00275: val_accuracy did not improve from 0.98145\n",
            "Epoch 276/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0069 - accuracy: 0.9990 - val_loss: 0.0929 - val_accuracy: 0.9707\n",
            "\n",
            "Epoch 00276: val_accuracy did not improve from 0.98145\n",
            "Epoch 277/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0076 - accuracy: 0.9982 - val_loss: 0.0659 - val_accuracy: 0.9746\n",
            "\n",
            "Epoch 00277: val_accuracy did not improve from 0.98145\n",
            "Epoch 278/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0759 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00278: val_accuracy did not improve from 0.98145\n",
            "Epoch 279/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0042 - accuracy: 0.9999 - val_loss: 0.0595 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00279: val_accuracy improved from 0.98145 to 0.98438, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 280/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.1600 - val_accuracy: 0.9541\n",
            "\n",
            "Epoch 00280: val_accuracy did not improve from 0.98438\n",
            "Epoch 281/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0060 - accuracy: 0.9991 - val_loss: 0.1200 - val_accuracy: 0.9648\n",
            "\n",
            "Epoch 00281: val_accuracy did not improve from 0.98438\n",
            "Epoch 282/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0969 - val_accuracy: 0.9697\n",
            "\n",
            "Epoch 00282: val_accuracy did not improve from 0.98438\n",
            "Epoch 283/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0052 - accuracy: 0.9996 - val_loss: 0.0543 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00283: val_accuracy did not improve from 0.98438\n",
            "Epoch 284/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0581 - val_accuracy: 0.9795\n",
            "\n",
            "Epoch 00284: val_accuracy did not improve from 0.98438\n",
            "Epoch 285/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0866 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00285: val_accuracy did not improve from 0.98438\n",
            "Epoch 286/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0061 - accuracy: 0.9983 - val_loss: 0.1001 - val_accuracy: 0.9727\n",
            "\n",
            "Epoch 00286: val_accuracy did not improve from 0.98438\n",
            "Epoch 287/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0028 - accuracy: 0.9999 - val_loss: 0.0525 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00287: val_accuracy did not improve from 0.98438\n",
            "Epoch 288/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0037 - accuracy: 0.9999 - val_loss: 0.0593 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00288: val_accuracy did not improve from 0.98438\n",
            "Epoch 289/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0042 - accuracy: 0.9995 - val_loss: 0.0627 - val_accuracy: 0.9785\n",
            "\n",
            "Epoch 00289: val_accuracy did not improve from 0.98438\n",
            "Epoch 290/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0774 - val_accuracy: 0.9785\n",
            "\n",
            "Epoch 00290: val_accuracy did not improve from 0.98438\n",
            "Epoch 291/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0743 - val_accuracy: 0.9746\n",
            "\n",
            "Epoch 00291: val_accuracy did not improve from 0.98438\n",
            "Epoch 292/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0050 - accuracy: 0.9993 - val_loss: 0.0506 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00292: val_accuracy improved from 0.98438 to 0.98535, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 293/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0534 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00293: val_accuracy did not improve from 0.98535\n",
            "Epoch 294/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0573 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00294: val_accuracy did not improve from 0.98535\n",
            "Epoch 295/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0704 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00295: val_accuracy did not improve from 0.98535\n",
            "Epoch 296/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0532 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00296: val_accuracy did not improve from 0.98535\n",
            "Epoch 297/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0613 - val_accuracy: 0.9785\n",
            "\n",
            "Epoch 00297: val_accuracy did not improve from 0.98535\n",
            "Epoch 298/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0635 - val_accuracy: 0.9814\n",
            "\n",
            "Epoch 00298: val_accuracy did not improve from 0.98535\n",
            "Epoch 299/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0944 - val_accuracy: 0.9678\n",
            "\n",
            "Epoch 00299: val_accuracy did not improve from 0.98535\n",
            "Epoch 300/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0029 - accuracy: 0.9998 - val_loss: 0.0888 - val_accuracy: 0.9707\n",
            "\n",
            "Epoch 00300: val_accuracy did not improve from 0.98535\n",
            "Epoch 301/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0029 - accuracy: 0.9998 - val_loss: 0.0539 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00301: val_accuracy did not improve from 0.98535\n",
            "Epoch 302/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0536 - accuracy: 0.9846 - val_loss: 0.7139 - val_accuracy: 0.8223\n",
            "\n",
            "Epoch 00302: val_accuracy did not improve from 0.98535\n",
            "Epoch 303/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0091 - accuracy: 0.9981 - val_loss: 0.1816 - val_accuracy: 0.9209\n",
            "\n",
            "Epoch 00303: val_accuracy did not improve from 0.98535\n",
            "Epoch 304/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0093 - accuracy: 0.9969 - val_loss: 0.1257 - val_accuracy: 0.9658\n",
            "\n",
            "Epoch 00304: val_accuracy did not improve from 0.98535\n",
            "Epoch 305/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0059 - accuracy: 0.9984 - val_loss: 0.3201 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00305: val_accuracy did not improve from 0.98535\n",
            "Epoch 306/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0685 - val_accuracy: 0.9795\n",
            "\n",
            "Epoch 00306: val_accuracy did not improve from 0.98535\n",
            "Epoch 307/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0034 - accuracy: 0.9999 - val_loss: 0.0586 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00307: val_accuracy did not improve from 0.98535\n",
            "Epoch 308/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0094 - accuracy: 0.9978 - val_loss: 0.0789 - val_accuracy: 0.9785\n",
            "\n",
            "Epoch 00308: val_accuracy did not improve from 0.98535\n",
            "Epoch 309/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0049 - accuracy: 0.9990 - val_loss: 0.0638 - val_accuracy: 0.9746\n",
            "\n",
            "Epoch 00309: val_accuracy did not improve from 0.98535\n",
            "Epoch 310/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0064 - accuracy: 0.9989 - val_loss: 0.0715 - val_accuracy: 0.9717\n",
            "\n",
            "Epoch 00310: val_accuracy did not improve from 0.98535\n",
            "Epoch 311/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0027 - accuracy: 0.9998 - val_loss: 0.0988 - val_accuracy: 0.9707\n",
            "\n",
            "Epoch 00311: val_accuracy did not improve from 0.98535\n",
            "Epoch 312/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0052 - accuracy: 0.9992 - val_loss: 0.0718 - val_accuracy: 0.9766\n",
            "\n",
            "Epoch 00312: val_accuracy did not improve from 0.98535\n",
            "Epoch 313/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0042 - accuracy: 0.9994 - val_loss: 0.5868 - val_accuracy: 0.8799\n",
            "\n",
            "Epoch 00313: val_accuracy did not improve from 0.98535\n",
            "Epoch 314/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0173 - accuracy: 0.9929 - val_loss: 0.0704 - val_accuracy: 0.9746\n",
            "\n",
            "Epoch 00314: val_accuracy did not improve from 0.98535\n",
            "Epoch 315/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0048 - accuracy: 0.9994 - val_loss: 0.0978 - val_accuracy: 0.9668\n",
            "\n",
            "Epoch 00315: val_accuracy did not improve from 0.98535\n",
            "Epoch 316/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0610 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00316: val_accuracy did not improve from 0.98535\n",
            "Epoch 317/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0642 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00317: val_accuracy did not improve from 0.98535\n",
            "Epoch 318/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0662 - val_accuracy: 0.9814\n",
            "\n",
            "Epoch 00318: val_accuracy did not improve from 0.98535\n",
            "Epoch 319/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0033 - accuracy: 0.9997 - val_loss: 0.0652 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00319: val_accuracy did not improve from 0.98535\n",
            "Epoch 320/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0912 - val_accuracy: 0.9756\n",
            "\n",
            "Epoch 00320: val_accuracy did not improve from 0.98535\n",
            "Epoch 321/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0614 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00321: val_accuracy did not improve from 0.98535\n",
            "Epoch 322/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0655 - val_accuracy: 0.9814\n",
            "\n",
            "Epoch 00322: val_accuracy did not improve from 0.98535\n",
            "Epoch 323/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0032 - accuracy: 0.9998 - val_loss: 0.0691 - val_accuracy: 0.9795\n",
            "\n",
            "Epoch 00323: val_accuracy did not improve from 0.98535\n",
            "Epoch 324/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0685 - val_accuracy: 0.9795\n",
            "\n",
            "Epoch 00324: val_accuracy did not improve from 0.98535\n",
            "Epoch 325/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0574 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00325: val_accuracy did not improve from 0.98535\n",
            "Epoch 326/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0668 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00326: val_accuracy did not improve from 0.98535\n",
            "Epoch 327/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0635 - val_accuracy: 0.9795\n",
            "\n",
            "Epoch 00327: val_accuracy did not improve from 0.98535\n",
            "Epoch 328/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0686 - val_accuracy: 0.9814\n",
            "\n",
            "Epoch 00328: val_accuracy did not improve from 0.98535\n",
            "Epoch 329/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0632 - val_accuracy: 0.9795\n",
            "\n",
            "Epoch 00329: val_accuracy did not improve from 0.98535\n",
            "Epoch 330/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0605 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00330: val_accuracy did not improve from 0.98535\n",
            "Epoch 331/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0576 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00331: val_accuracy did not improve from 0.98535\n",
            "Epoch 332/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0562 - val_accuracy: 0.9814\n",
            "\n",
            "Epoch 00332: val_accuracy did not improve from 0.98535\n",
            "Epoch 333/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0621 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00333: val_accuracy did not improve from 0.98535\n",
            "Epoch 334/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0548 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00334: val_accuracy did not improve from 0.98535\n",
            "Epoch 335/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0597 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00335: val_accuracy did not improve from 0.98535\n",
            "Epoch 336/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0027 - accuracy: 0.9999 - val_loss: 0.0638 - val_accuracy: 0.9814\n",
            "\n",
            "Epoch 00336: val_accuracy did not improve from 0.98535\n",
            "Epoch 337/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0613 - val_accuracy: 0.9795\n",
            "\n",
            "Epoch 00337: val_accuracy did not improve from 0.98535\n",
            "Epoch 338/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0565 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00338: val_accuracy did not improve from 0.98535\n",
            "Epoch 339/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0795 - val_accuracy: 0.9775\n",
            "\n",
            "Epoch 00339: val_accuracy did not improve from 0.98535\n",
            "Epoch 340/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0606 - val_accuracy: 0.9814\n",
            "\n",
            "Epoch 00340: val_accuracy did not improve from 0.98535\n",
            "Epoch 341/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0655 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00341: val_accuracy did not improve from 0.98535\n",
            "Epoch 342/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0624 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00342: val_accuracy did not improve from 0.98535\n",
            "Epoch 343/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0207 - accuracy: 0.9938 - val_loss: 1.2784 - val_accuracy: 0.7822\n",
            "\n",
            "Epoch 00343: val_accuracy did not improve from 0.98535\n",
            "Epoch 344/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0137 - accuracy: 0.9944 - val_loss: 0.0777 - val_accuracy: 0.9766\n",
            "\n",
            "Epoch 00344: val_accuracy did not improve from 0.98535\n",
            "Epoch 345/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0635 - val_accuracy: 0.9775\n",
            "\n",
            "Epoch 00345: val_accuracy did not improve from 0.98535\n",
            "Epoch 346/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.0628 - val_accuracy: 0.9795\n",
            "\n",
            "Epoch 00346: val_accuracy did not improve from 0.98535\n",
            "Epoch 347/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0026 - accuracy: 0.9997 - val_loss: 0.1015 - val_accuracy: 0.9707\n",
            "\n",
            "Epoch 00347: val_accuracy did not improve from 0.98535\n",
            "Epoch 348/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0094 - accuracy: 0.9982 - val_loss: 0.1085 - val_accuracy: 0.9736\n",
            "\n",
            "Epoch 00348: val_accuracy did not improve from 0.98535\n",
            "Epoch 349/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0045 - accuracy: 0.9987 - val_loss: 0.0809 - val_accuracy: 0.9736\n",
            "\n",
            "Epoch 00349: val_accuracy did not improve from 0.98535\n",
            "Epoch 350/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.2577 - val_accuracy: 0.9141\n",
            "\n",
            "Epoch 00350: val_accuracy did not improve from 0.98535\n",
            "Epoch 351/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0594 - val_accuracy: 0.9863\n",
            "\n",
            "Epoch 00351: val_accuracy improved from 0.98535 to 0.98633, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 352/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0569 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00352: val_accuracy did not improve from 0.98633\n",
            "Epoch 353/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0711 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00353: val_accuracy did not improve from 0.98633\n",
            "Epoch 354/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0636 - val_accuracy: 0.9814\n",
            "\n",
            "Epoch 00354: val_accuracy did not improve from 0.98633\n",
            "Epoch 355/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.0507 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00355: val_accuracy did not improve from 0.98633\n",
            "Epoch 356/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0633 - val_accuracy: 0.9795\n",
            "\n",
            "Epoch 00356: val_accuracy did not improve from 0.98633\n",
            "Epoch 357/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0030 - accuracy: 0.9996 - val_loss: 0.0632 - val_accuracy: 0.9814\n",
            "\n",
            "Epoch 00357: val_accuracy did not improve from 0.98633\n",
            "Epoch 358/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0535 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00358: val_accuracy did not improve from 0.98633\n",
            "Epoch 359/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0623 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00359: val_accuracy did not improve from 0.98633\n",
            "Epoch 360/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0625 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00360: val_accuracy did not improve from 0.98633\n",
            "Epoch 361/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 0.0679 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00361: val_accuracy did not improve from 0.98633\n",
            "Epoch 362/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0599 - val_accuracy: 0.9814\n",
            "\n",
            "Epoch 00362: val_accuracy did not improve from 0.98633\n",
            "Epoch 363/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0567 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00363: val_accuracy did not improve from 0.98633\n",
            "Epoch 364/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0584 - val_accuracy: 0.9863\n",
            "\n",
            "Epoch 00364: val_accuracy did not improve from 0.98633\n",
            "Epoch 365/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0661 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00365: val_accuracy did not improve from 0.98633\n",
            "Epoch 366/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0573 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00366: val_accuracy did not improve from 0.98633\n",
            "Epoch 367/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0571 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00367: val_accuracy did not improve from 0.98633\n",
            "Epoch 368/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0529 - val_accuracy: 0.9863\n",
            "\n",
            "Epoch 00368: val_accuracy did not improve from 0.98633\n",
            "Epoch 369/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.0783 - val_accuracy: 0.9795\n",
            "\n",
            "Epoch 00369: val_accuracy did not improve from 0.98633\n",
            "Epoch 370/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0752 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00370: val_accuracy did not improve from 0.98633\n",
            "Epoch 371/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0666 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00371: val_accuracy did not improve from 0.98633\n",
            "Epoch 372/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0632 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00372: val_accuracy did not improve from 0.98633\n",
            "Epoch 373/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0656 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00373: val_accuracy did not improve from 0.98633\n",
            "Epoch 374/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0659 - val_accuracy: 0.9785\n",
            "\n",
            "Epoch 00374: val_accuracy did not improve from 0.98633\n",
            "Epoch 375/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 8.5004e-04 - accuracy: 1.0000 - val_loss: 0.0631 - val_accuracy: 0.9814\n",
            "\n",
            "Epoch 00375: val_accuracy did not improve from 0.98633\n",
            "Epoch 376/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 8.1559e-04 - accuracy: 1.0000 - val_loss: 0.0719 - val_accuracy: 0.9775\n",
            "\n",
            "Epoch 00376: val_accuracy did not improve from 0.98633\n",
            "Epoch 377/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0645 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00377: val_accuracy did not improve from 0.98633\n",
            "Epoch 378/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 9.1728e-04 - accuracy: 1.0000 - val_loss: 0.0678 - val_accuracy: 0.9814\n",
            "\n",
            "Epoch 00378: val_accuracy did not improve from 0.98633\n",
            "Epoch 379/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0639 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00379: val_accuracy did not improve from 0.98633\n",
            "Epoch 380/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 9.3273e-04 - accuracy: 1.0000 - val_loss: 0.0631 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00380: val_accuracy did not improve from 0.98633\n",
            "Epoch 381/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0677 - val_accuracy: 0.9785\n",
            "\n",
            "Epoch 00381: val_accuracy did not improve from 0.98633\n",
            "Epoch 382/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 0.0694 - val_accuracy: 0.9775\n",
            "\n",
            "Epoch 00382: val_accuracy did not improve from 0.98633\n",
            "Epoch 383/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0650 - val_accuracy: 0.9795\n",
            "\n",
            "Epoch 00383: val_accuracy did not improve from 0.98633\n",
            "Epoch 384/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 7.9521e-04 - accuracy: 1.0000 - val_loss: 0.0752 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00384: val_accuracy did not improve from 0.98633\n",
            "Epoch 385/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 8.2844e-04 - accuracy: 1.0000 - val_loss: 0.0694 - val_accuracy: 0.9775\n",
            "\n",
            "Epoch 00385: val_accuracy did not improve from 0.98633\n",
            "Epoch 386/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0023 - accuracy: 0.9998 - val_loss: 0.0722 - val_accuracy: 0.9746\n",
            "\n",
            "Epoch 00386: val_accuracy did not improve from 0.98633\n",
            "Epoch 387/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.0621 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00387: val_accuracy did not improve from 0.98633\n",
            "Epoch 388/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.1236 - val_accuracy: 0.9619\n",
            "\n",
            "Epoch 00388: val_accuracy did not improve from 0.98633\n",
            "Epoch 389/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0619 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00389: val_accuracy did not improve from 0.98633\n",
            "Epoch 390/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 9.4052e-04 - accuracy: 1.0000 - val_loss: 0.0603 - val_accuracy: 0.9863\n",
            "\n",
            "Epoch 00390: val_accuracy did not improve from 0.98633\n",
            "Epoch 391/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 8.4720e-04 - accuracy: 1.0000 - val_loss: 0.0599 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00391: val_accuracy did not improve from 0.98633\n",
            "Epoch 392/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0656 - val_accuracy: 0.9785\n",
            "\n",
            "Epoch 00392: val_accuracy did not improve from 0.98633\n",
            "Epoch 393/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0682 - val_accuracy: 0.9795\n",
            "\n",
            "Epoch 00393: val_accuracy did not improve from 0.98633\n",
            "Epoch 394/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0664 - val_accuracy: 0.9795\n",
            "\n",
            "Epoch 00394: val_accuracy did not improve from 0.98633\n",
            "Epoch 395/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.0956 - val_accuracy: 0.9746\n",
            "\n",
            "Epoch 00395: val_accuracy did not improve from 0.98633\n",
            "Epoch 396/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.0641 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00396: val_accuracy did not improve from 0.98633\n",
            "Epoch 397/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0603 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00397: val_accuracy did not improve from 0.98633\n",
            "Epoch 398/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0800 - val_accuracy: 0.9785\n",
            "\n",
            "Epoch 00398: val_accuracy did not improve from 0.98633\n",
            "Epoch 399/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 9.3719e-04 - accuracy: 1.0000 - val_loss: 0.0632 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00399: val_accuracy did not improve from 0.98633\n",
            "Epoch 400/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 7.5299e-04 - accuracy: 1.0000 - val_loss: 0.0695 - val_accuracy: 0.9785\n",
            "\n",
            "Epoch 00400: val_accuracy did not improve from 0.98633\n",
            "Epoch 401/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 8.8523e-04 - accuracy: 1.0000 - val_loss: 0.0635 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00401: val_accuracy did not improve from 0.98633\n",
            "Epoch 402/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 7.2702e-04 - accuracy: 1.0000 - val_loss: 0.0554 - val_accuracy: 0.9873\n",
            "\n",
            "Epoch 00402: val_accuracy improved from 0.98633 to 0.98730, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 403/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 8.7380e-04 - accuracy: 1.0000 - val_loss: 0.0622 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00403: val_accuracy did not improve from 0.98730\n",
            "Epoch 404/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 8.9528e-04 - accuracy: 1.0000 - val_loss: 0.0604 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00404: val_accuracy improved from 0.98730 to 0.98828, saving model to /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/CargasEletricas/Vinicius/Dataset/modeled_current/assets\n",
            "Epoch 405/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 6.3824e-04 - accuracy: 1.0000 - val_loss: 0.0616 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00405: val_accuracy did not improve from 0.98828\n",
            "Epoch 406/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 8.0923e-04 - accuracy: 1.0000 - val_loss: 0.0597 - val_accuracy: 0.9863\n",
            "\n",
            "Epoch 00406: val_accuracy did not improve from 0.98828\n",
            "Epoch 407/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 7.3150e-04 - accuracy: 1.0000 - val_loss: 0.0625 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00407: val_accuracy did not improve from 0.98828\n",
            "Epoch 408/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 7.8369e-04 - accuracy: 1.0000 - val_loss: 0.0638 - val_accuracy: 0.9814\n",
            "\n",
            "Epoch 00408: val_accuracy did not improve from 0.98828\n",
            "Epoch 409/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 7.1814e-04 - accuracy: 1.0000 - val_loss: 0.0634 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00409: val_accuracy did not improve from 0.98828\n",
            "Epoch 410/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 6.9514e-04 - accuracy: 1.0000 - val_loss: 0.0665 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00410: val_accuracy did not improve from 0.98828\n",
            "Epoch 411/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 6.1515e-04 - accuracy: 1.0000 - val_loss: 0.0703 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00411: val_accuracy did not improve from 0.98828\n",
            "Epoch 412/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0042 - accuracy: 0.9990 - val_loss: 0.0662 - val_accuracy: 0.9775\n",
            "\n",
            "Epoch 00412: val_accuracy did not improve from 0.98828\n",
            "Epoch 413/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0543 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00413: val_accuracy did not improve from 0.98828\n",
            "Epoch 414/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0562 - val_accuracy: 0.9863\n",
            "\n",
            "Epoch 00414: val_accuracy did not improve from 0.98828\n",
            "Epoch 415/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0039 - accuracy: 0.9990 - val_loss: 0.6729 - val_accuracy: 0.9053\n",
            "\n",
            "Epoch 00415: val_accuracy did not improve from 0.98828\n",
            "Epoch 416/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0697 - val_accuracy: 0.9785\n",
            "\n",
            "Epoch 00416: val_accuracy did not improve from 0.98828\n",
            "Epoch 417/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 9.8135e-04 - accuracy: 0.9999 - val_loss: 0.0710 - val_accuracy: 0.9795\n",
            "\n",
            "Epoch 00417: val_accuracy did not improve from 0.98828\n",
            "Epoch 418/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0655 - val_accuracy: 0.9814\n",
            "\n",
            "Epoch 00418: val_accuracy did not improve from 0.98828\n",
            "Epoch 419/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 8.1172e-04 - accuracy: 1.0000 - val_loss: 0.0654 - val_accuracy: 0.9814\n",
            "\n",
            "Epoch 00419: val_accuracy did not improve from 0.98828\n",
            "Epoch 420/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 9.3936e-04 - accuracy: 1.0000 - val_loss: 0.0614 - val_accuracy: 0.9814\n",
            "\n",
            "Epoch 00420: val_accuracy did not improve from 0.98828\n",
            "Epoch 421/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 9.2112e-04 - accuracy: 1.0000 - val_loss: 0.0528 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00421: val_accuracy did not improve from 0.98828\n",
            "Epoch 422/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0635 - val_accuracy: 0.9785\n",
            "\n",
            "Epoch 00422: val_accuracy did not improve from 0.98828\n",
            "Epoch 423/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 9.2562e-04 - accuracy: 1.0000 - val_loss: 0.0645 - val_accuracy: 0.9814\n",
            "\n",
            "Epoch 00423: val_accuracy did not improve from 0.98828\n",
            "Epoch 424/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0567 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00424: val_accuracy did not improve from 0.98828\n",
            "Epoch 425/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 7.6096e-04 - accuracy: 1.0000 - val_loss: 0.0557 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00425: val_accuracy did not improve from 0.98828\n",
            "Epoch 426/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 8.5564e-04 - accuracy: 1.0000 - val_loss: 0.0638 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00426: val_accuracy did not improve from 0.98828\n",
            "Epoch 427/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 6.1503e-04 - accuracy: 1.0000 - val_loss: 0.0594 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00427: val_accuracy did not improve from 0.98828\n",
            "Epoch 428/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 9.8112e-04 - accuracy: 0.9999 - val_loss: 0.0615 - val_accuracy: 0.9863\n",
            "\n",
            "Epoch 00428: val_accuracy did not improve from 0.98828\n",
            "Epoch 429/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 6.0029e-04 - accuracy: 1.0000 - val_loss: 0.0570 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00429: val_accuracy did not improve from 0.98828\n",
            "Epoch 430/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 8.3963e-04 - accuracy: 1.0000 - val_loss: 0.0634 - val_accuracy: 0.9795\n",
            "\n",
            "Epoch 00430: val_accuracy did not improve from 0.98828\n",
            "Epoch 431/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 5.5083e-04 - accuracy: 1.0000 - val_loss: 0.0599 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00431: val_accuracy did not improve from 0.98828\n",
            "Epoch 432/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 5.8998e-04 - accuracy: 1.0000 - val_loss: 0.0558 - val_accuracy: 0.9873\n",
            "\n",
            "Epoch 00432: val_accuracy did not improve from 0.98828\n",
            "Epoch 433/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0042 - accuracy: 0.9994 - val_loss: 0.0848 - val_accuracy: 0.9707\n",
            "\n",
            "Epoch 00433: val_accuracy did not improve from 0.98828\n",
            "Epoch 434/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0619 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00434: val_accuracy did not improve from 0.98828\n",
            "Epoch 435/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 9.6932e-04 - accuracy: 1.0000 - val_loss: 0.0588 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00435: val_accuracy did not improve from 0.98828\n",
            "Epoch 436/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 7.1412e-04 - accuracy: 1.0000 - val_loss: 0.0634 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00436: val_accuracy did not improve from 0.98828\n",
            "Epoch 437/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 5.8263e-04 - accuracy: 1.0000 - val_loss: 0.0634 - val_accuracy: 0.9814\n",
            "\n",
            "Epoch 00437: val_accuracy did not improve from 0.98828\n",
            "Epoch 438/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0062 - accuracy: 0.9983 - val_loss: 0.0758 - val_accuracy: 0.9795\n",
            "\n",
            "Epoch 00438: val_accuracy did not improve from 0.98828\n",
            "Epoch 439/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 8.7305e-04 - accuracy: 1.0000 - val_loss: 0.0592 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00439: val_accuracy did not improve from 0.98828\n",
            "Epoch 440/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 7.1073e-04 - accuracy: 1.0000 - val_loss: 0.0577 - val_accuracy: 0.9863\n",
            "\n",
            "Epoch 00440: val_accuracy did not improve from 0.98828\n",
            "Epoch 441/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 6.8527e-04 - accuracy: 1.0000 - val_loss: 0.0586 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00441: val_accuracy did not improve from 0.98828\n",
            "Epoch 442/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 8.1471e-04 - accuracy: 1.0000 - val_loss: 0.0729 - val_accuracy: 0.9795\n",
            "\n",
            "Epoch 00442: val_accuracy did not improve from 0.98828\n",
            "Epoch 443/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 8.5242e-04 - accuracy: 1.0000 - val_loss: 0.0594 - val_accuracy: 0.9863\n",
            "\n",
            "Epoch 00443: val_accuracy did not improve from 0.98828\n",
            "Epoch 444/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 5.8030e-04 - accuracy: 1.0000 - val_loss: 0.0649 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00444: val_accuracy did not improve from 0.98828\n",
            "Epoch 445/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 9.1843e-04 - accuracy: 1.0000 - val_loss: 0.0540 - val_accuracy: 0.9863\n",
            "\n",
            "Epoch 00445: val_accuracy did not improve from 0.98828\n",
            "Epoch 446/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 6.0354e-04 - accuracy: 1.0000 - val_loss: 0.0597 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00446: val_accuracy did not improve from 0.98828\n",
            "Epoch 447/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 6.4412e-04 - accuracy: 1.0000 - val_loss: 0.0585 - val_accuracy: 0.9814\n",
            "\n",
            "Epoch 00447: val_accuracy did not improve from 0.98828\n",
            "Epoch 448/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 8.0732e-04 - accuracy: 1.0000 - val_loss: 0.0643 - val_accuracy: 0.9814\n",
            "\n",
            "Epoch 00448: val_accuracy did not improve from 0.98828\n",
            "Epoch 449/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 7.2974e-04 - accuracy: 1.0000 - val_loss: 0.0605 - val_accuracy: 0.9814\n",
            "\n",
            "Epoch 00449: val_accuracy did not improve from 0.98828\n",
            "Epoch 450/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 8.9313e-04 - accuracy: 1.0000 - val_loss: 0.0652 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00450: val_accuracy did not improve from 0.98828\n",
            "Epoch 451/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 5.8323e-04 - accuracy: 1.0000 - val_loss: 0.0593 - val_accuracy: 0.9814\n",
            "\n",
            "Epoch 00451: val_accuracy did not improve from 0.98828\n",
            "Epoch 452/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 6.2252e-04 - accuracy: 1.0000 - val_loss: 0.0607 - val_accuracy: 0.9814\n",
            "\n",
            "Epoch 00452: val_accuracy did not improve from 0.98828\n",
            "Epoch 453/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 6.3042e-04 - accuracy: 1.0000 - val_loss: 0.0581 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00453: val_accuracy did not improve from 0.98828\n",
            "Epoch 454/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 6.7607e-04 - accuracy: 1.0000 - val_loss: 0.0562 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00454: val_accuracy did not improve from 0.98828\n",
            "Epoch 455/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 5.0852e-04 - accuracy: 1.0000 - val_loss: 0.0605 - val_accuracy: 0.9863\n",
            "\n",
            "Epoch 00455: val_accuracy did not improve from 0.98828\n",
            "Epoch 456/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 5.3347e-04 - accuracy: 1.0000 - val_loss: 0.0564 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00456: val_accuracy did not improve from 0.98828\n",
            "Epoch 457/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 5.7127e-04 - accuracy: 1.0000 - val_loss: 0.0571 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00457: val_accuracy did not improve from 0.98828\n",
            "Epoch 458/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 6.6355e-04 - accuracy: 1.0000 - val_loss: 0.0580 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00458: val_accuracy did not improve from 0.98828\n",
            "Epoch 459/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 5.4443e-04 - accuracy: 1.0000 - val_loss: 0.0558 - val_accuracy: 0.9863\n",
            "\n",
            "Epoch 00459: val_accuracy did not improve from 0.98828\n",
            "Epoch 460/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 6.3596e-04 - accuracy: 1.0000 - val_loss: 0.0621 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00460: val_accuracy did not improve from 0.98828\n",
            "Epoch 461/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 5.6430e-04 - accuracy: 1.0000 - val_loss: 0.0588 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00461: val_accuracy did not improve from 0.98828\n",
            "Epoch 462/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 5.1991e-04 - accuracy: 1.0000 - val_loss: 0.0584 - val_accuracy: 0.9805\n",
            "\n",
            "Epoch 00462: val_accuracy did not improve from 0.98828\n",
            "Epoch 463/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 6.0711e-04 - accuracy: 1.0000 - val_loss: 0.0700 - val_accuracy: 0.9795\n",
            "\n",
            "Epoch 00463: val_accuracy did not improve from 0.98828\n",
            "Epoch 464/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 5.8391e-04 - accuracy: 1.0000 - val_loss: 0.0557 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00464: val_accuracy did not improve from 0.98828\n",
            "Epoch 465/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 5.1417e-04 - accuracy: 1.0000 - val_loss: 0.0599 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00465: val_accuracy did not improve from 0.98828\n",
            "Epoch 466/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 4.7956e-04 - accuracy: 1.0000 - val_loss: 0.0586 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00466: val_accuracy did not improve from 0.98828\n",
            "Epoch 467/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 4.7564e-04 - accuracy: 1.0000 - val_loss: 0.0569 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00467: val_accuracy did not improve from 0.98828\n",
            "Epoch 468/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 6.0743e-04 - accuracy: 1.0000 - val_loss: 0.0557 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00468: val_accuracy did not improve from 0.98828\n",
            "Epoch 469/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 4.5439e-04 - accuracy: 1.0000 - val_loss: 0.0597 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00469: val_accuracy did not improve from 0.98828\n",
            "Epoch 470/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 5.0458e-04 - accuracy: 1.0000 - val_loss: 0.0548 - val_accuracy: 0.9873\n",
            "\n",
            "Epoch 00470: val_accuracy did not improve from 0.98828\n",
            "Epoch 471/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 6.0763e-04 - accuracy: 1.0000 - val_loss: 0.0682 - val_accuracy: 0.9795\n",
            "\n",
            "Epoch 00471: val_accuracy did not improve from 0.98828\n",
            "Epoch 472/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 9.1601e-04 - accuracy: 1.0000 - val_loss: 0.0636 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00472: val_accuracy did not improve from 0.98828\n",
            "Epoch 473/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 8.5211e-04 - accuracy: 1.0000 - val_loss: 0.0535 - val_accuracy: 0.9873\n",
            "\n",
            "Epoch 00473: val_accuracy did not improve from 0.98828\n",
            "Epoch 474/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 4.2638e-04 - accuracy: 1.0000 - val_loss: 0.0589 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00474: val_accuracy did not improve from 0.98828\n",
            "Epoch 475/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 5.4494e-04 - accuracy: 1.0000 - val_loss: 0.0567 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00475: val_accuracy did not improve from 0.98828\n",
            "Epoch 476/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 4.8826e-04 - accuracy: 1.0000 - val_loss: 0.0582 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00476: val_accuracy did not improve from 0.98828\n",
            "Epoch 477/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 5.9805e-04 - accuracy: 1.0000 - val_loss: 0.0679 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00477: val_accuracy did not improve from 0.98828\n",
            "Epoch 478/500\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 5.1862e-04 - accuracy: 1.0000 - val_loss: 0.0652 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00478: val_accuracy did not improve from 0.98828\n",
            "Epoch 479/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 5.4204e-04 - accuracy: 1.0000 - val_loss: 0.0593 - val_accuracy: 0.9814\n",
            "\n",
            "Epoch 00479: val_accuracy did not improve from 0.98828\n",
            "Epoch 480/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 4.9799e-04 - accuracy: 1.0000 - val_loss: 0.0591 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00480: val_accuracy did not improve from 0.98828\n",
            "Epoch 481/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 4.3626e-04 - accuracy: 1.0000 - val_loss: 0.0571 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00481: val_accuracy did not improve from 0.98828\n",
            "Epoch 482/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 4.8795e-04 - accuracy: 1.0000 - val_loss: 0.0555 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00482: val_accuracy did not improve from 0.98828\n",
            "Epoch 483/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 5.3088e-04 - accuracy: 1.0000 - val_loss: 0.0549 - val_accuracy: 0.9873\n",
            "\n",
            "Epoch 00483: val_accuracy did not improve from 0.98828\n",
            "Epoch 484/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 5.0572e-04 - accuracy: 1.0000 - val_loss: 0.0570 - val_accuracy: 0.9873\n",
            "\n",
            "Epoch 00484: val_accuracy did not improve from 0.98828\n",
            "Epoch 485/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 3.6128e-04 - accuracy: 1.0000 - val_loss: 0.0523 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00485: val_accuracy did not improve from 0.98828\n",
            "Epoch 486/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 5.1447e-04 - accuracy: 1.0000 - val_loss: 0.0541 - val_accuracy: 0.9863\n",
            "\n",
            "Epoch 00486: val_accuracy did not improve from 0.98828\n",
            "Epoch 487/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 4.2412e-04 - accuracy: 1.0000 - val_loss: 0.0542 - val_accuracy: 0.9873\n",
            "\n",
            "Epoch 00487: val_accuracy did not improve from 0.98828\n",
            "Epoch 488/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 3.4920e-04 - accuracy: 1.0000 - val_loss: 0.0608 - val_accuracy: 0.9795\n",
            "\n",
            "Epoch 00488: val_accuracy did not improve from 0.98828\n",
            "Epoch 489/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 3.9238e-04 - accuracy: 1.0000 - val_loss: 0.0580 - val_accuracy: 0.9863\n",
            "\n",
            "Epoch 00489: val_accuracy did not improve from 0.98828\n",
            "Epoch 490/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 6.7777e-04 - accuracy: 1.0000 - val_loss: 0.0619 - val_accuracy: 0.9814\n",
            "\n",
            "Epoch 00490: val_accuracy did not improve from 0.98828\n",
            "Epoch 491/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 4.6848e-04 - accuracy: 1.0000 - val_loss: 0.0552 - val_accuracy: 0.9873\n",
            "\n",
            "Epoch 00491: val_accuracy did not improve from 0.98828\n",
            "Epoch 492/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 3.7595e-04 - accuracy: 1.0000 - val_loss: 0.0563 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00492: val_accuracy did not improve from 0.98828\n",
            "Epoch 493/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.0028 - accuracy: 0.9990 - val_loss: 0.0590 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00493: val_accuracy did not improve from 0.98828\n",
            "Epoch 494/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 4.9357e-04 - accuracy: 1.0000 - val_loss: 0.0625 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00494: val_accuracy did not improve from 0.98828\n",
            "Epoch 495/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 8.9833e-04 - accuracy: 1.0000 - val_loss: 0.0564 - val_accuracy: 0.9863\n",
            "\n",
            "Epoch 00495: val_accuracy did not improve from 0.98828\n",
            "Epoch 496/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 6.2017e-04 - accuracy: 1.0000 - val_loss: 0.0606 - val_accuracy: 0.9814\n",
            "\n",
            "Epoch 00496: val_accuracy did not improve from 0.98828\n",
            "Epoch 497/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 4.0980e-04 - accuracy: 1.0000 - val_loss: 0.0605 - val_accuracy: 0.9834\n",
            "\n",
            "Epoch 00497: val_accuracy did not improve from 0.98828\n",
            "Epoch 498/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 4.7725e-04 - accuracy: 1.0000 - val_loss: 0.0748 - val_accuracy: 0.9854\n",
            "\n",
            "Epoch 00498: val_accuracy did not improve from 0.98828\n",
            "Epoch 499/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 5.3047e-04 - accuracy: 1.0000 - val_loss: 0.0599 - val_accuracy: 0.9824\n",
            "\n",
            "Epoch 00499: val_accuracy did not improve from 0.98828\n",
            "Epoch 500/500\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 5.1720e-04 - accuracy: 1.0000 - val_loss: 0.0610 - val_accuracy: 0.9844\n",
            "\n",
            "Epoch 00500: val_accuracy did not improve from 0.98828\n",
            "Training/validation time was: 481.251 seconds\n",
            "\n",
            " \n",
            "EVALUATING THE MODEL:\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 0.0399 - accuracy: 0.9828\n",
            "Evaluate time was 0.148633 seconds\n",
            "0.9828125238418579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKzfgPUwns-u"
      },
      "source": [
        "Plotting the results from the final model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LqT_obUmKbV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c21f25df-9c2b-4015-d0a6-120cfd061cca"
      },
      "source": [
        "# Acurácia:\r\n",
        "plt.figure(figsize=(8,8))\r\n",
        "plt.plot(history3.history['accuracy'])\r\n",
        "plt.plot(history3.history['val_accuracy'])\r\n",
        "plt.title('Train_acc vs. val_acc', fontsize=20)\r\n",
        "plt.xlabel('Epoch',fontsize=18)\r\n",
        "plt.ylabel('Accuracy',fontsize=18)\r\n",
        "plt.legend(['Train', 'Validation'], loc='best', fontsize=18)\r\n",
        "plt.show()\r\n",
        "#Para salvar no Drive...\r\n",
        "#plt.savefig('/content/drive/My Drive/MESTRADO - UFES/trainacc_vs_valacc.png', transparent=True)\r\n",
        "\r\n",
        "# Perda (loss):\r\n",
        "plt.figure(figsize=(8,8))\r\n",
        "plt.plot(history3.history['loss'])\r\n",
        "plt.plot(history3.history['val_loss'])\r\n",
        "plt.title('Loss vs. epochs', fontsize=20)\r\n",
        "plt.ylabel('Loss',fontsize=18)\r\n",
        "plt.xlabel('Epoch',fontsize=18)\r\n",
        "plt.legend(['Train', 'Validation'], loc='best', fontsize=20)\r\n",
        "plt.show()\r\n",
        "#Para salvar no Drive...\r\n",
        "#plt.savefig('/content/drive/My Drive/MESTRADO - UFES/trainloss_vs_valloss.png', transparent=True)\r\n",
        "\r\n",
        "print('Max val_acc was:',acc_max3)\r\n",
        "print('Eval_acc was:',test_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Max val_acc was: 0.98828125\n",
            "Eval_acc was: 0.9828125238418579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5csKZiCg_AV"
      },
      "source": [
        "Function that creates the **confusion matrix**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A087XBWugrHd"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\r\n",
        "                          normalize=False,\r\n",
        "                          title='Confusion matrix',\r\n",
        "                          cmap=plt.cm.Blues):\r\n",
        "    \"\"\"\r\n",
        "    This function prints and plots the confusion matrix.\r\n",
        "    Normalization can be applied by setting `normalize=True`.\r\n",
        "    \"\"\"\r\n",
        "    if normalize:\r\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\r\n",
        "        print(\"Normalized confusion matrix\")\r\n",
        "    else:\r\n",
        "        print('Confusion matrix, without normalization')\r\n",
        "\r\n",
        "    print(cm)\r\n",
        "\r\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\r\n",
        "    plt.title(title, fontsize= 20)\r\n",
        "    plt.colorbar()\r\n",
        "    tick_marks = np.arange(len(classes))\r\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\r\n",
        "    plt.yticks(tick_marks, classes)\r\n",
        "\r\n",
        "    fmt = '.2f' if normalize else 'd'\r\n",
        "    thresh = cm.max() / 2.\r\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\r\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\r\n",
        "                 horizontalalignment=\"center\",\r\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\r\n",
        "\r\n",
        "    plt.tight_layout()\r\n",
        "    plt.ylabel('Classe Correta', fontsize=18)\r\n",
        "    plt.xlabel('Classe Predita', fontsize=18)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxGmq3tMeihJ"
      },
      "source": [
        "The **confusion matrix** of test data it's shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmwLk9GqzsYI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff514c35-1385-40ff-b76d-2eb80f10e4ee"
      },
      "source": [
        "y_test_pred = final_model.predict(X_test3)\r\n",
        "\r\n",
        "# \"Desbinarização\" dos labels:\r\n",
        "# (Labels estão binarizados; matriz de confusão não aceita)\r\n",
        "Y_test_non_category = [np.argmax(t) for t in Y_test3]\r\n",
        "y_test_pred_non_category = [np.argmax(t) for t in y_test_pred]\r\n",
        "\r\n",
        "cnf_matrix = confusion_matrix(Y_test_non_category, y_test_pred_non_category)\r\n",
        "class_names = np.arange(0,16)\r\n",
        "\r\n",
        "# Matriz de confusão não normalizada:\r\n",
        "plt.figure(figsize = (10,7))\r\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names,\r\n",
        "                      title='Non-normalized confusion matrix')\r\n",
        "\r\n",
        "# Matriz de confusão normalizada:\r\n",
        "plt.figure(figsize = (10,7))\r\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\r\n",
        "                      title='Normalized confusion matrix')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[96  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0 89  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0 78  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0 79  2  0  0  0  0  0  2  0  1  0  0  0]\n",
            " [ 0  1  0  0 84  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0 70  0  0  0  0  4  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0 73  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0 88  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0 86  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0 69  1  0  0  0  0  0]\n",
            " [ 0  0  1  0  0  0  4  0  0  6 68  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0 74  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0 76  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 77  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 73  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 78]]\n",
            "Normalized confusion matrix\n",
            "[[1.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         1.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         1.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.94047619 0.02380952 0.\n",
            "  0.         0.         0.         0.         0.02380952 0.\n",
            "  0.01190476 0.         0.         0.        ]\n",
            " [0.         0.01176471 0.         0.         0.98823529 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.94594595\n",
            "  0.         0.         0.         0.         0.05405405 0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  1.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         1.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         1.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.98571429 0.01428571 0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.01265823 0.         0.         0.\n",
            "  0.05063291 0.         0.         0.07594937 0.86075949 0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         1.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  1.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         1.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         1.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         1.        ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}